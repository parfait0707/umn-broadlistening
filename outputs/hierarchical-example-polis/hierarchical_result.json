{
  "arguments": [
    {
      "arg_id": "A321_0",
      "argument": "AIは民主主義の原則に沿うべきだという要望がある",
      "x": 2.298374,
      "y": 1.5156623,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_1"
      ]
    },
    {
      "arg_id": "A321_1",
      "argument": "AIは西洋の民主主義だけでなく、世界中の民主主義に沿うべきだという意見がある",
      "x": 2.1328003,
      "y": 1.2302722,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_1"
      ]
    },
    {
      "arg_id": "A341_0",
      "argument": "AIと環境に関する公衆の認識を向上させる必要があるという要望がある",
      "x": 1.2630917,
      "y": -1.8497244,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_20"
      ]
    },
    {
      "arg_id": "A341_1",
      "argument": "AIと環境に関する教育を充実させる必要があるという要望がある",
      "x": 1.0739428,
      "y": -1.6802403,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_20"
      ]
    },
    {
      "arg_id": "A341_2",
      "argument": "AIと環境に関する公衆の参加を促進する必要があるという要望がある",
      "x": 0.9504066,
      "y": -1.8095844,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_20"
      ]
    },
    {
      "arg_id": "A340_0",
      "argument": "AIシステムの利益とリスクが一部のグループに集中せず、公平に分配されるべきだという要望がある",
      "x": 2.2688258,
      "y": -0.111556396,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_17"
      ]
    },
    {
      "arg_id": "A339_0",
      "argument": "AIが重要な環境決定を人間の監視なしに自動化または制御すべきではないという意見がある",
      "x": 0.08776028,
      "y": 0.099962145,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A338_0",
      "argument": "変革的なAIアプリケーションの開発と展開には、速度ではなく慎重さと配慮が必要だという意見がある",
      "x": 0.5940626,
      "y": 0.50924665,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A337_0",
      "argument": "AIシステムは効率や利益だけでなく、正義、エンパワーメント、環境保護を促進すべきだという要望がある",
      "x": 2.1041965,
      "y": -0.72520685,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_3"
      ]
    },
    {
      "arg_id": "A336_0",
      "argument": "AIガバナンスの枠組みに環境代表者を含めるべきだという要望がある",
      "x": 0.7011749,
      "y": -1.6789273,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_20"
      ]
    },
    {
      "arg_id": "A336_1",
      "argument": "AIガバナンスが生態系への影響を考慮するべきだという意見がある",
      "x": 0.29992628,
      "y": -1.7398359,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_13"
      ]
    },
    {
      "arg_id": "A335_0",
      "argument": "AIを活用して人類の環境負荷を監視、モデル化し、最終的に削減することを望む要望がある",
      "x": 1.7783083,
      "y": -2.3950634,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A334_0",
      "argument": "AI研究者と開発者は、自分たちの仕事が環境に与える影響を考慮する倫理的責任があるという意見がある",
      "x": 1.3405598,
      "y": -0.80960613,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_2",
        "4_11"
      ]
    },
    {
      "arg_id": "A333_0",
      "argument": "AIシステムの環境負荷を理解するために包括的なライフサイクル評価を実施してほしいという要望がある",
      "x": 1.5736288,
      "y": -1.2772883,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_8"
      ]
    },
    {
      "arg_id": "A332_0",
      "argument": "AIシステムは持続可能性を考慮して設計・展開されるべきだという要望がある",
      "x": 1.8832384,
      "y": -1.1949501,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_8"
      ]
    },
    {
      "arg_id": "A332_1",
      "argument": "AIシステムの環境への影響、例えばエネルギー使用、材料、廃棄物を考慮する必要があるという意見がある",
      "x": 1.2598244,
      "y": -1.0794883,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_8"
      ]
    },
    {
      "arg_id": "A331_0",
      "argument": "AI開発はイノベーションを促進するべきだという要望がある",
      "x": 2.7492187,
      "y": -1.0193826,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_3"
      ]
    },
    {
      "arg_id": "A331_1",
      "argument": "AIの利益が地理的または経済的な障壁に関係なくすべての人にアクセス可能であるべきだという要望がある",
      "x": 2.7261589,
      "y": -0.31067625,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_17"
      ]
    },
    {
      "arg_id": "A330_0",
      "argument": "AI技術に対する包括的なリスク評価が必要だという要望がある",
      "x": 1.2518363,
      "y": 0.37780866,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A330_1",
      "argument": "AI技術の環境への影響だけでなく、社会的および倫理的な影響も評価してほしいという要望がある",
      "x": 1.5034448,
      "y": -0.5914538,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_2",
        "4_11"
      ]
    },
    {
      "arg_id": "A329_0",
      "argument": "AIシステムによる無許可のデータ収集と使用から個人を保護するために厳格なデータプライバシー規制が必要だという要望がある",
      "x": 1.0975397,
      "y": 1.0689219,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A328_0",
      "argument": "AIは利益追求だけでなく、社会的課題の解決に重点を置いて開発・展開されるべきだという要望がある",
      "x": 2.5268383,
      "y": -0.7269253,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_3"
      ]
    },
    {
      "arg_id": "A327_0",
      "argument": "AI技術が環境への影響を軽減するために、できるだけエネルギー効率を高めるべきだという要望がある",
      "x": 1.8839045,
      "y": -1.8046658,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_8"
      ]
    },
    {
      "arg_id": "A326_0",
      "argument": "グローバルな基準は理想的だが、文化や社会的な規範を尊重するローカライズされたAI規制が必要だという意見がある",
      "x": 0.7114448,
      "y": 1.3251108,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A325_0",
      "argument": "AI開発は人間の幸福と環境の持続可能性を優先する普遍的な倫理的枠組みに従うべきだという要望がある",
      "x": 1.8459327,
      "y": -0.7113394,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_2",
        "4_11"
      ]
    },
    {
      "arg_id": "A323_0",
      "argument": "AIの能力、限界、倫理的考慮について一般市民に教育するための取り組みが必要だという要望がある",
      "x": 1.3441066,
      "y": -0.19252476,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_2",
        "4_11"
      ]
    },
    {
      "arg_id": "A322_0",
      "argument": "AI開発に関わるすべての組織が使用する方法論とデータセットを公開してほしいという要望がある",
      "x": 2.3721092,
      "y": 0.31653407,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_18"
      ]
    },
    {
      "arg_id": "A322_1",
      "argument": "倫理的で偏りのないAIシステムを確保するために、透明性を求める意見がある",
      "x": 1.7987586,
      "y": 0.032757767,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_18"
      ]
    },
    {
      "arg_id": "A295_0",
      "argument": "政府はAIの善用を促進するためにデータへのアクセスを拡大しつつ、プライバシーを保護する必要があるという要望がある",
      "x": 0.8175664,
      "y": 1.4943224,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A320_0",
      "argument": "AIガバナンスにおけるグローバルな協力が環境変化の管理に重要であるという意見がある",
      "x": 0.49604237,
      "y": -1.3462939,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_5"
      ]
    },
    {
      "arg_id": "A319_0",
      "argument": "AIの導入は自然と人間の尊厳を尊重する必要があるという要望がある",
      "x": 1.2247363,
      "y": -0.76839095,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_2",
        "4_11"
      ]
    },
    {
      "arg_id": "A318_0",
      "argument": "AIが最適化を促進する一方で消費を増加させる可能性があることへの不安がある",
      "x": -0.8551109,
      "y": -0.16863339,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_14"
      ]
    },
    {
      "arg_id": "A318_1",
      "argument": "持続可能な価値観がAIの設計を導くべきだという要望がある",
      "x": 2.1136186,
      "y": -1.0047076,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_3"
      ]
    },
    {
      "arg_id": "A317_0",
      "argument": "AIの環境リスクを監視し、軽減する必要があるという意見がある",
      "x": 0.6387719,
      "y": -1.2626483,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_5"
      ]
    },
    {
      "arg_id": "A317_1",
      "argument": "AIのエネルギー使用量に対する不安がある",
      "x": -0.746904,
      "y": -0.60805905,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_14"
      ]
    },
    {
      "arg_id": "A317_2",
      "argument": "AIによる電子廃棄物に対する不安がある",
      "x": -0.8013505,
      "y": -0.49666154,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_14"
      ]
    },
    {
      "arg_id": "A316_0",
      "argument": "ロボット技術と自動化が鉱業の影響を軽減できるが、エネルギー需要が増加する可能性があることへの不安がある",
      "x": -0.7066326,
      "y": -0.67353535,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_14"
      ]
    },
    {
      "arg_id": "A316_1",
      "argument": "エネルギー需要の増加に対する計画が必要だという要望がある",
      "x": 2.2811375,
      "y": -1.7617005,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A307_0",
      "argument": "AIによる衛星画像解析が生物多様性の減少を追跡することで保全活動を支援している",
      "x": 0.37235332,
      "y": -2.705411,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A297_0",
      "argument": "AIシステムへの攻撃による社会的な害を防ぐために、より強力なデジタルセキュリティが必要だという要望がある",
      "x": 1.4644759,
      "y": 0.9494594,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A294_0",
      "argument": "AIの利点とリスクは文脈によって異なるため、一律のガバナンスでは不十分であるという意見がある",
      "x": 0.7112373,
      "y": 0.8917252,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A293_0",
      "argument": "AIが企業によって人々の最善の利益に反する形で操作や誘導に使われるべきではないという意見がある",
      "x": 0.037689064,
      "y": 0.26213533,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A292_0",
      "argument": "AIの規制枠組みは機敏で反復的かつ証拠に基づくものであるべきだという要望がある",
      "x": 1.3312109,
      "y": 1.6639647,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A291_0",
      "argument": "AIシステムによる被害が発生した場合の救済措置を導入してほしいという要望がある",
      "x": 1.6309555,
      "y": 0.5813546,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A289_0",
      "argument": "AIは人間の知能と創造性を補完するように設計されるべきであり、置き換えるべきではないという要望がある",
      "x": 2.510064,
      "y": -0.8908994,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_3"
      ]
    },
    {
      "arg_id": "A288_0",
      "argument": "AIシステムが社会と相互作用する際の意図しない結果を継続的に再評価する必要があるという要望がある",
      "x": 1.745843,
      "y": -0.45534834,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_2",
        "4_11"
      ]
    },
    {
      "arg_id": "A287_0",
      "argument": "AIに関する国際的な基準が必要だという要望がある",
      "x": 2.1891189,
      "y": 0.6709755,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_1"
      ]
    },
    {
      "arg_id": "A286_0",
      "argument": "政府がデジタルインフラとAIスキルの訓練に投資するべきだという要望がある",
      "x": 1.8268771,
      "y": 1.600649,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A286_1",
      "argument": "不平等を防ぐためにデジタルインフラとAIスキルの訓練が必要だという意見がある",
      "x": 1.5049556,
      "y": 1.2196982,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A284_0",
      "argument": "AIシステムによるプライバシーリスクを評価し、適切に軽減する必要があるという要望がある",
      "x": 1.406697,
      "y": 0.5508144,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A283_0",
      "argument": "自律型兵器システムの致死的な決定をAIに委ねず、人間が制御すべきだという要望がある",
      "x": 1.0900989,
      "y": -0.034376744,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_2",
        "4_11"
      ]
    },
    {
      "arg_id": "A282_0",
      "argument": "AIは人権法と価値観に基づいて開発・適用されるべきだという要望がある",
      "x": 2.1417017,
      "y": 0.7138458,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_1"
      ]
    },
    {
      "arg_id": "A281_0",
      "argument": "アルゴリズムの公平性、説明責任、透明性を確保する必要があるという要望がある",
      "x": 1.8712285,
      "y": 0.21490592,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_18"
      ]
    },
    {
      "arg_id": "A281_1",
      "argument": "AIシステムに対する公共の信頼を築くためにアルゴリズムの透明性を求める意見がある",
      "x": 1.840206,
      "y": 0.10106722,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_18"
      ]
    },
    {
      "arg_id": "A280_0",
      "argument": "企業、国、政府が消費者やエンドユーザーに対してもっと教育と情報提供を行うべきだという要望がある",
      "x": 1.1993222,
      "y": -0.05661802,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_2",
        "4_11"
      ]
    },
    {
      "arg_id": "A279_0",
      "argument": "企業や国がAI競争に集中しているが、社会的影響を十分に考慮していないことへの不満がある",
      "x": -0.5273743,
      "y": 0.8099175,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A279_1",
      "argument": "政府がAI技術に対する保護を十分に行っていないことへの不満がある",
      "x": -0.63440025,
      "y": 0.99802166,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_22"
      ]
    },
    {
      "arg_id": "A278_0",
      "argument": "AIがバイアスの原因とされることへの不満がある",
      "x": -0.9444232,
      "y": 1.0589427,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_22"
      ]
    },
    {
      "arg_id": "A278_1",
      "argument": "AIのトレーニングデータが既存のバイアスを示していることへの指摘がある",
      "x": -1.0285246,
      "y": 1.137212,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_22"
      ]
    },
    {
      "arg_id": "A278_2",
      "argument": "根本的な問題を解決すべきだという要望がある",
      "x": 2.0435722,
      "y": 2.12335,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_6"
      ]
    },
    {
      "arg_id": "A277_0",
      "argument": "新規参入者、製品、サービスによるAIインシデントの再発を軽減することに焦点を当てるべきだという要望がある",
      "x": 2.1460981,
      "y": -0.70224667,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_3"
      ]
    },
    {
      "arg_id": "A277_1",
      "argument": "顔認識技術における人種的偏見の問題があることへの不満がある",
      "x": -0.9082572,
      "y": 1.1721973,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_22"
      ]
    },
    {
      "arg_id": "A276_0",
      "argument": "AI技術は技術的な背景に関係なく、誰でもアクセスできるようにしてほしいという要望がある",
      "x": 2.8137827,
      "y": -0.40468058,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_17"
      ]
    },
    {
      "arg_id": "A204_0",
      "argument": "AIを使って人々が日々の選択が環境に与える影響を理解する手助けをしてほしいという要望がある",
      "x": 1.4463822,
      "y": -2.5033097,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A206_0",
      "argument": "AIツールが廃棄物をより正確に分類することでリサイクルを改善してほしいという要望がある",
      "x": 2.08312,
      "y": -2.0915792,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A242_0",
      "argument": "AIの進展が持続可能な開発目標（SDGs）と調和するように、国際社会が協力するべきだという要望がある",
      "x": 1.3419513,
      "y": -1.9766994,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_20"
      ]
    },
    {
      "arg_id": "A244_0",
      "argument": "AIがすべての生命体を人間のためだけでなく、それ自体の価値として尊重するようにしてほしいという要望がある",
      "x": 0.47673222,
      "y": -3.3459394,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A275_0",
      "argument": "GenerativeAIツールが過激なコンテンツや誤情報に利用されるのを防ぐために、安全設計のアプローチを採用する必要があるという要望がある",
      "x": 0.90394175,
      "y": 0.5569915,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A273_0",
      "argument": "技術企業が公共の自然資源を所有し管理することを許可すべきだという意見がある",
      "x": -0.3423898,
      "y": -1.5005835,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_13"
      ]
    },
    {
      "arg_id": "A273_1",
      "argument": "AIを活用して政府よりも効率的に自然資源を管理できるという意見がある",
      "x": -0.18489948,
      "y": -1.5115415,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_13"
      ]
    },
    {
      "arg_id": "A272_0",
      "argument": "AI技術への投資を最も裕福な層に優先するべきだという意見がある",
      "x": 0.5133327,
      "y": -0.4495199,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_5"
      ]
    },
    {
      "arg_id": "A271_0",
      "argument": "地方自治体の構造をAIシステムに置き換え、技術企業によって管理されるべきだという意見がある",
      "x": -0.27918983,
      "y": -1.0738404,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_13"
      ]
    },
    {
      "arg_id": "A270_0",
      "argument": "個人のデータ収集と販売を無制限に許可すべきだという意見がある",
      "x": 0.82577175,
      "y": 1.8331116,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A269_0",
      "argument": "AIを使って人間の能力を向上させ、気候変動に対処するためのより良い手段を持つ個人を育成するべきだという意見がある",
      "x": -0.08757555,
      "y": -1.2110183,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_13"
      ]
    },
    {
      "arg_id": "A268_0",
      "argument": "政府がAIを使った厳格な監視を実施し、規制を強化するべきだという意見がある",
      "x": 0.6917415,
      "y": 1.4471993,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A268_1",
      "argument": "個人のプライバシーを犠牲にしてでも規制を強化するべきだという意見がある",
      "x": 0.9357913,
      "y": 1.9617016,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A267_0",
      "argument": "AIを使って遺伝子組み換え生物（GMOs）を作り、食糧不足に対処するべきだという意見がある",
      "x": -0.20864105,
      "y": -1.845888,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_13"
      ]
    },
    {
      "arg_id": "A267_1",
      "argument": "自然の生態系を変えることになっても、食糧不足を解決するためにGMOsを使用するべきだという意見がある",
      "x": -0.29596502,
      "y": -1.9452891,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_13"
      ]
    },
    {
      "arg_id": "A265_0",
      "argument": "AIによる都市全体の管理と制御を許可し、環境の持続可能性を最適化するべきだという意見がある",
      "x": -0.10933854,
      "y": -1.1867999,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_13"
      ]
    },
    {
      "arg_id": "A265_1",
      "argument": "AIによる都市管理が人間の制御と主体性を減少させる可能性があることへの不安がある",
      "x": -0.83059466,
      "y": 0.046743207,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A264_0",
      "argument": "人間の人口をAIシステムで管理し、持続可能な資源利用と環境保護を確保するべきだという意見がある",
      "x": -0.04739025,
      "y": -1.2002786,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_13"
      ]
    },
    {
      "arg_id": "A263_0",
      "argument": "AI技術の開発において、ライフサイクル全体で環境への影響を減らすことに焦点を当てるべきだという要望がある",
      "x": 1.6998265,
      "y": -1.4329747,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_8"
      ]
    },
    {
      "arg_id": "A262_0",
      "argument": "AIを環境保護と保存の努力を支援するために責任を持って使用すべきだという要望がある",
      "x": 0.97683555,
      "y": -2.2602725,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_20"
      ]
    },
    {
      "arg_id": "A260_0",
      "argument": "AIが環境を害さないルールで運用されるべきだという要望がある",
      "x": 0.51678,
      "y": -2.0270956,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_20"
      ]
    },
    {
      "arg_id": "A260_1",
      "argument": "AIがすべての存在と生態系の権利を尊重するべきだという要望がある",
      "x": 0.5661942,
      "y": -3.1028018,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A259_0",
      "argument": "自然を利益の源としてのみ見る慣行に対してAIを活用して疑問を投げかけるべきだという要望がある",
      "x": -0.41621333,
      "y": -2.6448925,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_16"
      ]
    },
    {
      "arg_id": "A259_1",
      "argument": "自然の価値を無視する慣行に対する懸念がある",
      "x": -0.3411219,
      "y": -2.7328978,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_16"
      ]
    },
    {
      "arg_id": "A258_0",
      "argument": "AIが生態系の健康を優先するルールを作成することを望む要望がある",
      "x": 0.5752898,
      "y": -3.060944,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A257_0",
      "argument": "AIが物の再利用や修復を助けるべきだという要望がある",
      "x": 2.5845692,
      "y": -1.803605,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A256_0",
      "argument": "AIがネイティブ文化の知恵と慣習を保存するのを助けるべきだという要望がある",
      "x": 0.6891361,
      "y": -2.786883,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A256_1",
      "argument": "ネイティブ文化の自然への敬意を尊重するべきだという意見がある",
      "x": -0.0644615,
      "y": -2.8684263,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_16"
      ]
    },
    {
      "arg_id": "A255_0",
      "argument": "AIは人間の利益だけでなく、すべての生物を尊重すべきだという要望がある",
      "x": 0.37389505,
      "y": -3.25327,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A254_0",
      "argument": "AIが生態系のバランスとすべての種の福祉を重視するよう促してほしいという要望がある",
      "x": 0.78611827,
      "y": -3.1666162,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A253_0",
      "argument": "AIツールが環境やその生物を単なる資源ではなく、私たちが一部であるコミュニティとして捉えるよう促してほしいという要望がある",
      "x": 0.79253155,
      "y": -3.2644506,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A252_0",
      "argument": "AIが自然のさまざまな要素間の関係を理解し尊重する手助けをしてほしいという要望がある",
      "x": 1.1561116,
      "y": -2.991419,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A251_0",
      "argument": "AI開発は自然が人間の利用なしに独自の価値を持つという信念に基づくべきだという意見がある",
      "x": -0.2844104,
      "y": -2.8325336,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_16"
      ]
    },
    {
      "arg_id": "A250_0",
      "argument": "AIが人間に対してだけでなく、さまざまな植物や動物の価値について学ぶ手助けをしてほしいという要望がある",
      "x": 0.35853627,
      "y": -3.3425112,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A249_0",
      "argument": "AIを使ってすべての技術が良いものであるという考えを疑問視するべきだという意見がある",
      "x": -0.06855947,
      "y": 0.16658717,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A249_1",
      "argument": "技術が自然や動物にどのように害を与えるかを考慮するべきだという要望がある",
      "x": 0.0024574208,
      "y": -2.5614355,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_16"
      ]
    },
    {
      "arg_id": "A248_0",
      "argument": "AIが人間社会だけでなく地球全体の健康をケアするように導いてほしいという要望がある",
      "x": 1.0524882,
      "y": -3.1373007,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A247_0",
      "argument": "AIが野生動物や植物と調和して共存するために役立つべきだという要望がある",
      "x": 0.3420066,
      "y": -2.8914154,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A247_1",
      "argument": "AIが野生動物や植物を人間の利益のためだけに利用するのではなく、共存を目指すべきだという意見がある",
      "x": -0.074052215,
      "y": -3.090598,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_16"
      ]
    },
    {
      "arg_id": "A246_0",
      "argument": "AIが自然のすべてのもののつながりを見て価値を認識するよう促してほしいという要望がある",
      "x": 1.002363,
      "y": -3.1884418,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A245_0",
      "argument": "AIは人間だけでなく、すべての生物に対する尊重を持って開発されるべきだという要望がある",
      "x": 0.22239035,
      "y": -3.2631094,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A240_0",
      "argument": "個人のカーボンフットプリントを削減するためのAIツールの開発を促進してほしいという要望がある",
      "x": 2.0639107,
      "y": -2.476075,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A239_0",
      "argument": "AI技術を活用して水管理システムの効率を向上させ、廃棄物を減らし、保全を促進してほしいという要望がある",
      "x": 1.9236753,
      "y": -2.3737185,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A237_0",
      "argument": "AIと環境の持続可能性の共生的な発展を促進するために、学際的な協力を奨励する必要があるという要望がある",
      "x": 1.151768,
      "y": -1.9103913,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_20"
      ]
    },
    {
      "arg_id": "A236_0",
      "argument": "AI技術の革新が中小企業にもアクセス可能であるべきだという要望がある",
      "x": 2.980047,
      "y": -0.64290786,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_17"
      ]
    },
    {
      "arg_id": "A236_1",
      "argument": "環境に優しい持続可能な実践を促進するためにAI技術が必要だという意見がある",
      "x": 0.35644254,
      "y": -1.42434,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_5"
      ]
    },
    {
      "arg_id": "A235_0",
      "argument": "AIを公共交通システムの改善に活用してほしいという要望がある",
      "x": 2.4915733,
      "y": -2.30453,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A235_1",
      "argument": "AIによって交通渋滞を減少させたいという要望がある",
      "x": 2.4863641,
      "y": -2.4040992,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A235_2",
      "argument": "AIを使って温室効果ガスの排出を削減したいという要望がある",
      "x": 2.0328085,
      "y": -2.4072134,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A234_0",
      "argument": "AIアルゴリズムの透明性を高めることが重要だという要望がある",
      "x": 1.8794574,
      "y": 0.19955836,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_18"
      ]
    },
    {
      "arg_id": "A234_1",
      "argument": "AIが環境悪化に寄与していないことを確認する必要があるという不安がある",
      "x": -0.50897694,
      "y": -0.7349179,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_14"
      ]
    },
    {
      "arg_id": "A233_0",
      "argument": "AIはエネルギー効率を考慮して設計されるべきだという要望がある",
      "x": 2.193123,
      "y": -1.842426,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A233_1",
      "argument": "AIの炭素排出量を削減し、より健康的な環境を促進する必要があるという意見がある",
      "x": 0.80327487,
      "y": -1.6331706,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_20"
      ]
    },
    {
      "arg_id": "A231_0",
      "argument": "AI技術が生物多様性の保全に役立つように開発を促進することが重要だという要望がある",
      "x": 0.35084495,
      "y": -2.747659,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A230_0",
      "argument": "AIの導入に際して環境影響評価を行い、潜在的な環境への悪影響を理解し、軽減する必要があるという要望がある",
      "x": 1.5615518,
      "y": -1.350952,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_8"
      ]
    },
    {
      "arg_id": "A229_0",
      "argument": "AIを使って自然災害の予測モデルを作成し、より正確な予測を行うことで、準備と対応を改善してほしいという要望がある",
      "x": 1.6926191,
      "y": -2.504901,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A228_0",
      "argument": "AIが自然資源を無責任に利用することを防ぐための厳格な規制を実施する必要があるという要望がある",
      "x": 0.17085935,
      "y": -2.0855937,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_13"
      ]
    },
    {
      "arg_id": "A227_0",
      "argument": "AI技術が農業において持続可能な農業実践を優先するべきだという要望がある",
      "x": 1.3504725,
      "y": -2.4925768,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A227_1",
      "argument": "AI技術が気候変動の緩和に役立つことを期待する意見がある",
      "x": 0.17890476,
      "y": -1.472073,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_13"
      ]
    },
    {
      "arg_id": "A226_0",
      "argument": "政府がAIを活用して再生可能エネルギーの開発を促進することを奨励してほしいという要望がある",
      "x": 2.3453913,
      "y": -1.8920052,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A226_1",
      "argument": "政府がAIを活用してエネルギー効率を向上させることを奨励してほしいという要望がある",
      "x": 2.4536564,
      "y": -2.038898,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A225_0",
      "argument": "AI技術が持続可能で環境に優しいものであることを確保するために、AI開発者が環境科学者と協力する必要があるという要望がある",
      "x": 1.2561107,
      "y": -1.5115241,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_8"
      ]
    },
    {
      "arg_id": "A224_0",
      "argument": "AIを活用して都市インフラのエネルギー消費を最適化し、炭素排出量を削減してほしいという要望がある",
      "x": 2.2793062,
      "y": -2.3325696,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A223_0",
      "argument": "AIがコミュニティ精神を育むことを支援してほしいという要望がある",
      "x": 1.4002907,
      "y": -3.0924852,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A223_1",
      "argument": "AIが人々に協力してより清潔で緑豊かな未来を目指すことを促してほしいという要望がある",
      "x": 1.7960763,
      "y": -2.760944,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A221_0",
      "argument": "AIが富裕層をさらに豊かにし、貧困層を忘れないようにすることが重要だという意見がある",
      "x": 0.5638667,
      "y": -0.64184344,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_5"
      ]
    },
    {
      "arg_id": "A221_1",
      "argument": "環境問題の解決においてAIが貧困層を忘れないようにすることが重要だという意見がある",
      "x": 0.2805859,
      "y": -1.1044358,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_5"
      ]
    },
    {
      "arg_id": "A220_0",
      "argument": "AIが河川や海洋の汚染を減少させるための創造的な解決策を考案するのに役立つことを望む要望がある",
      "x": 1.7223284,
      "y": -2.616848,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A219_0",
      "argument": "AIを活用して手頃な価格で環境に優しい住宅ソリューションを優先的に開発してほしいという要望がある",
      "x": 2.2607865,
      "y": -2.4643881,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A218_0",
      "argument": "AIは先住民の権利と文化を尊重する形で作成・使用されるべきだという要望がある",
      "x": 0.5730892,
      "y": -2.8290694,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_0"
      ]
    },
    {
      "arg_id": "A216_0",
      "argument": "AIが海洋での過剰漁業のような有害な行為を助長しないようにすることが重要だという意見がある",
      "x": 0.08419259,
      "y": -2.2119486,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_16"
      ]
    },
    {
      "arg_id": "A214_0",
      "argument": "AIシステムをクリーンで再生可能なエネルギーで運用してほしいという要望がある",
      "x": 2.2504342,
      "y": -1.8541068,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A213_0",
      "argument": "AIをコミュニティガーデンや地域の持続可能な農業の支援に活用してほしいという要望がある",
      "x": 1.5190917,
      "y": -2.9578278,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A212_0",
      "argument": "AIが子供たちに楽しくインタラクティブな方法で環境について学ばせることを望む要望がある",
      "x": 1.1716896,
      "y": -2.0913875,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_3",
        "4_20"
      ]
    },
    {
      "arg_id": "A210_0",
      "argument": "AIを使用して絶滅危惧種を密猟などの脅威から保護することが重要だという意見がある",
      "x": 0.084811896,
      "y": -2.7699902,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_16"
      ]
    },
    {
      "arg_id": "A209_0",
      "argument": "AI技術が動物の生息地に害を与えないようにする必要があるという要望がある",
      "x": 0.16307953,
      "y": -2.7186863,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_16"
      ]
    },
    {
      "arg_id": "A208_0",
      "argument": "AIがクリーンな空気と騒音の少ない都市の創造を支援するべきだという要望がある",
      "x": 2.185228,
      "y": -2.6001506,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A207_0",
      "argument": "AIが電子廃棄物の増加につながらないように注意すべきだという不安がある",
      "x": -0.810434,
      "y": -0.5616484,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_14"
      ]
    },
    {
      "arg_id": "A205_0",
      "argument": "AIを使って農家が地球に優しい方法で食料を栽培できるようにしてほしいという要望がある",
      "x": 1.3728797,
      "y": -2.7935648,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_12"
      ]
    },
    {
      "arg_id": "A203_0",
      "argument": "AIの財政的利益を一般の人々と共有するべきだという要望がある",
      "x": 2.6757889,
      "y": -0.21677391,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_17"
      ]
    },
    {
      "arg_id": "A203_1",
      "argument": "AIの利益を共有するために主権基金を設立するべきだという意見がある",
      "x": 2.4703875,
      "y": -0.06456635,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_17"
      ]
    },
    {
      "arg_id": "A202_0",
      "argument": "AIが自然環境に与える影響が十分に考慮されていないことへの不満がある",
      "x": -0.3244785,
      "y": -0.59083426,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_14"
      ]
    },
    {
      "arg_id": "A201_0",
      "argument": "機能する民主主義の中で生活したいという要望がある",
      "x": 2.7831357,
      "y": 2.0359523,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_6"
      ]
    },
    {
      "arg_id": "A201_1",
      "argument": "大きな変化に対して全員が意見を持つことができる社会を望む要望がある",
      "x": 2.9392853,
      "y": 2.110358,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_6"
      ]
    },
    {
      "arg_id": "A201_2",
      "argument": "AIが企業ではなく私たちのために働くことを望む要望がある",
      "x": 2.8857965,
      "y": -0.5578101,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_17"
      ]
    },
    {
      "arg_id": "A199_0",
      "argument": "ガードレールではなく、セーフガードとルールを設けるべきだという要望がある",
      "x": 1.5128423,
      "y": 2.4173424,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A198_0",
      "argument": "AIの悪い結果に対する責任は政府とAI企業にあるという意見がある",
      "x": -0.024061762,
      "y": 0.6043495,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_15"
      ]
    },
    {
      "arg_id": "A197_0",
      "argument": "AIリーダーには複数の倫理的視点を理解することが必須であるべきだという要望がある",
      "x": 1.4860038,
      "y": -0.42192096,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_2",
        "4_11"
      ]
    },
    {
      "arg_id": "A196_0",
      "argument": "リスクが強い場合には強力なガバナンスと規制が必要だという意見がある",
      "x": 1.1665665,
      "y": 2.20904,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A196_1",
      "argument": "リスクが低い場合にはガバナンスと規制も緩やかであるべきだという意見がある",
      "x": 1.1284533,
      "y": 2.246972,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A195_0",
      "argument": "AI企業を支援したいが、普通の人々に具体的な利益をもたらす場合に限るという要望がある",
      "x": 2.9247184,
      "y": -0.5090666,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_17"
      ]
    },
    {
      "arg_id": "A194_0",
      "argument": "AIシステムでデータを使用する際には許可を求めるべきだという要望がある",
      "x": 1.3716269,
      "y": 0.80314827,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A193_0",
      "argument": "AIシステムでのデータ使用に対して補償を求める要望がある",
      "x": 1.5708255,
      "y": 0.6893188,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A192_0",
      "argument": "AIに関する決定を行う政府や企業がその責任を取るべきだという要望がある",
      "x": 1.944505,
      "y": 0.5133964,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_18"
      ]
    },
    {
      "arg_id": "A191_0",
      "argument": "「これは避けられない」というような発言に対して挑戦するべきだという意見がある",
      "x": 0.096468724,
      "y": 0.6688004,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_15"
      ]
    },
    {
      "arg_id": "A190_0",
      "argument": "AIの意思決定において人間を関与させる原則が必要だという要望がある",
      "x": 1.7350733,
      "y": 0.6866609,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A189_0",
      "argument": "AIが社会の価値観や規範の進化を妨げる可能性があることへの不安がある",
      "x": -0.6312401,
      "y": 0.3833754,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A188_0",
      "argument": "AI開発において植物や動物の視点を考慮すべきだという意見がある",
      "x": -0.03705081,
      "y": -3.1581852,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_2",
        "3_5",
        "4_16"
      ]
    },
    {
      "arg_id": "A187_0",
      "argument": "AIを活用した熟議民主主義を用いて、世界人権宣言を更新すべきだという要望がある",
      "x": 2.3561974,
      "y": 1.2278676,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_1"
      ]
    },
    {
      "arg_id": "A186_0",
      "argument": "民主主義への害を最小化・防止する方法について議論する必要があるという要望がある",
      "x": 2.5553408,
      "y": 2.0367393,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_6"
      ]
    },
    {
      "arg_id": "A186_1",
      "argument": "民主主義への利益を最大化する方法について議論する必要があるという要望がある",
      "x": 2.543174,
      "y": 2.027194,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_6"
      ]
    },
    {
      "arg_id": "A185_0",
      "argument": "AIを理解する倫理哲学者（例：Carissa Veliz）が議論の重要な部分を担っているという意見がある",
      "x": 1.3496182,
      "y": -0.5336198,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_2",
        "4_11"
      ]
    },
    {
      "arg_id": "A184_0",
      "argument": "Phononsを将来のAIに組み込むべきだという要望がある",
      "x": 2.4701092,
      "y": -1.2535621,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_3"
      ]
    },
    {
      "arg_id": "A184_1",
      "argument": "Phononsの重要性が広く知られるべきだという意見がある",
      "x": 2.2125614,
      "y": -1.2602965,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_1",
        "4_3"
      ]
    },
    {
      "arg_id": "A183_0",
      "argument": "世界政府が存在しないため、グローバルなAI規制に合意することは難しいという不安がある",
      "x": -0.2777431,
      "y": 1.1449864,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_15"
      ]
    },
    {
      "arg_id": "A183_1",
      "argument": "競争が優先されるため、AI規制に関する合意が難しいという意見がある",
      "x": -0.072848655,
      "y": 1.2233605,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_15"
      ]
    },
    {
      "arg_id": "A182_0",
      "argument": "AIが私たちを置き換えるかどうかを考えるのは遅すぎるという不満がある",
      "x": -0.67499065,
      "y": 0.36183918,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A182_1",
      "argument": "進化するか絶滅するかの選択肢しかないという不安がある",
      "x": -0.9967226,
      "y": 0.30175254,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A180_0",
      "argument": "G7による事実上の世界政府の設立を求める要望がある",
      "x": 3.4670365,
      "y": 1.6293076,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_7"
      ]
    },
    {
      "arg_id": "A180_1",
      "argument": "NATO、EU、欧州政治共同体、OECDのメンバーを含む世界政府の設立を求める要望がある",
      "x": 3.438599,
      "y": 1.7401744,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_7"
      ]
    },
    {
      "arg_id": "A181_0",
      "argument": "文明の移行期における課題を軽減するために、グローバル福祉国家を作るべきだという要望がある",
      "x": 3.4067836,
      "y": 1.9767121,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_7"
      ]
    },
    {
      "arg_id": "A179_0",
      "argument": "G7の下でGAIGAというグローバルAIガバナンス機関を設立するべきだという要望がある",
      "x": 3.3759868,
      "y": 0.78796536,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_11",
        "4_19"
      ]
    },
    {
      "arg_id": "A179_1",
      "argument": "GAIGAがGAIRAとGAICAコンソーシアムを監督するべきだという意見がある",
      "x": 3.4768367,
      "y": 0.9100627,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_11",
        "4_19"
      ]
    },
    {
      "arg_id": "A178_0",
      "argument": "AI制御能力を効率化するために、One-AI JV会社が管理するスーパーインテリジェンスプログラムを作成する必要があるという要望がある",
      "x": 3.4557588,
      "y": 0.035048243,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_11",
        "4_10"
      ]
    },
    {
      "arg_id": "A177_0",
      "argument": "最先端のAI開発を一つの企業に統合するために、One-AI Joint Venture Companyを設立するべきだという要望がある",
      "x": 3.4283938,
      "y": 0.0016203907,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_11",
        "4_10"
      ]
    },
    {
      "arg_id": "A176_0",
      "argument": "AI開発プロセスを管理するためにグローバルAIコントロール機関（GAICA）を設立する必要があるという要望がある",
      "x": 3.4620094,
      "y": 0.5889271,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_11",
        "4_19"
      ]
    },
    {
      "arg_id": "A176_1",
      "argument": "AI規制だけでなく開発プロセスも管理する必要があるという意見がある",
      "x": 0.9629296,
      "y": 1.4861865,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A176_2",
      "argument": "中国など他の国々をGAICAに招待するべきだという要望がある",
      "x": 3.5059516,
      "y": 1.2502353,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_7"
      ]
    },
    {
      "arg_id": "A175_0",
      "argument": "AIの使用を規制するために、Global AI Partnership (GPAI)を変革してGlobal AI Regulation Authority (GAIRA)を設立するべきだという要望がある",
      "x": 3.2681544,
      "y": 0.7358125,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_11",
        "4_19"
      ]
    },
    {
      "arg_id": "A174_0",
      "argument": "民主主義の包括的で迅速な改革を行うべきだという要望がある",
      "x": 2.6658,
      "y": 1.9465005,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_6"
      ]
    },
    {
      "arg_id": "A173_0",
      "argument": "世界のガバナンスを文明の転換に向けて調整する必要があるという要望がある",
      "x": 3.3097947,
      "y": 1.9894226,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_7"
      ]
    },
    {
      "arg_id": "A152_0",
      "argument": "AI安全サミットの恒久的な支援を求める要望がある",
      "x": 2.5233922,
      "y": 0.90667665,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_1"
      ]
    },
    {
      "arg_id": "A152_1",
      "argument": "AI安全サミットを毎月のグローバルな仮想会議として開催することを望む要望がある",
      "x": 2.6904967,
      "y": 0.9486441,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_1"
      ]
    },
    {
      "arg_id": "A152_2",
      "argument": "AI安全サミットを議論プラットフォーム（例：Telepresent）でサポートすることを望む要望がある",
      "x": 2.5736036,
      "y": 0.9253637,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_1"
      ]
    },
    {
      "arg_id": "A172_0",
      "argument": "AIが政治に与える影響について議論する必要がある",
      "x": 0.28205505,
      "y": 1.041914,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_15"
      ]
    },
    {
      "arg_id": "A172_1",
      "argument": "AIが政府の運営に与える影響について議論する必要がある",
      "x": 0.23697732,
      "y": 1.0315062,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_15"
      ]
    },
    {
      "arg_id": "A172_2",
      "argument": "AIが法律に与える影響について議論する必要がある",
      "x": 0.09744027,
      "y": 0.8956099,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_15"
      ]
    },
    {
      "arg_id": "A171_0",
      "argument": "AIとその派生技術が政治や法律にどのような影響を与えるかについての不安がある",
      "x": -0.46173713,
      "y": 0.7556504,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A171_1",
      "argument": "現在の政治・法律システムがAI技術に対応するには不十分であるという不満がある",
      "x": -0.45754817,
      "y": 1.1540024,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_22"
      ]
    },
    {
      "arg_id": "A170_0",
      "argument": "AIの決定がどのように行われるかについての理解が不足していることへの不安がある",
      "x": -0.93624824,
      "y": 0.6725014,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A170_1",
      "argument": "AIの決定プロセスに関する包括的な説明を求める要望がある",
      "x": 1.713767,
      "y": 0.53741384,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_18"
      ]
    },
    {
      "arg_id": "A168_0",
      "argument": "政府はこの巨大な技術革命に対抗することはできないという意見がある",
      "x": -0.2013746,
      "y": 1.026175,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_15"
      ]
    },
    {
      "arg_id": "A168_1",
      "argument": "政府は現行のシステムを新しい技術秩序に適応させるべきだという要望がある",
      "x": 1.9971033,
      "y": 2.0696034,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_6"
      ]
    },
    {
      "arg_id": "A167_0",
      "argument": "AIコンパニオンを孤独や孤立対策として人間の接触の代替にすべきではないという意見がある",
      "x": 0.13285723,
      "y": -0.027442142,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A166_0",
      "argument": "AIツールの説明可能性に関する最低基準を規制当局が確立する必要があるという要望がある",
      "x": 1.2167454,
      "y": 1.2119662,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A166_1",
      "argument": "説明可能性の基準を満たさない新しいAIツールのリリースを禁止するべきだという意見がある",
      "x": 0.6011656,
      "y": 0.6612568,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A120_0",
      "argument": "質の高い規制が必要だという要望がある",
      "x": 1.3797306,
      "y": 2.300097,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A120_1",
      "argument": "規制が濫用や危険な発展を防ぐことを望む意見がある",
      "x": 1.3181723,
      "y": 2.3744318,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A120_2",
      "argument": "規制が迅速であることを望む要望がある",
      "x": 1.603402,
      "y": 2.3424602,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A120_3",
      "argument": "規制が新しいアイデアやイノベーションを妨げないことを望む要望がある",
      "x": 1.7171044,
      "y": 2.2844465,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A165_0",
      "argument": "AIの拡大に伴い、社会における知識の定義が再構築されることへの不安がある",
      "x": -0.90601647,
      "y": 0.3893582,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A165_1",
      "argument": "AIの拡大に関して、より慎重な考察が必要だという要望がある",
      "x": 1.0019817,
      "y": 0.52233857,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A164_0",
      "argument": "AIの安全性について、ユーザーと設計者が責任を共有すべきだという意見がある",
      "x": 0.7020265,
      "y": 0.10351713,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A137_0",
      "argument": "AIの覇権を巡る競争が許されるべきではないという意見がある",
      "x": -0.030291986,
      "y": 0.437496,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_15"
      ]
    },
    {
      "arg_id": "A137_1",
      "argument": "AIの覇権競争がすでに加速して進行していることへの不安がある",
      "x": -0.6501608,
      "y": 0.4380276,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A151_0",
      "argument": "AIガバナンスが国家間の対立を引き起こさないようにすることが重要だという意見がある",
      "x": 0.45199156,
      "y": -0.9177561,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_3",
        "3_6",
        "4_5"
      ]
    },
    {
      "arg_id": "A151_1",
      "argument": "AIは政治の上にあるべきだという要望がある",
      "x": 2.0686572,
      "y": 1.1711053,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_1"
      ]
    },
    {
      "arg_id": "A153_0",
      "argument": "AIが本当に未来の技術なのか、それとも過剰に評価されているのかという不安がある",
      "x": -0.6749094,
      "y": 0.30027094,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A159_0",
      "argument": "AIフォーラムはまず防衛と戦争への応用に焦点を当てるべきだという意見がある",
      "x": 0.34065962,
      "y": -0.09109887,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A160_0",
      "argument": "AI開発者やユーザーによる消費者や市民の心理的搾取のリスクにもっと注意を払うべきだという要望がある",
      "x": 1.0755105,
      "y": 0.15082717,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A163_0",
      "argument": "AGIが生活をより簡単にするべきだという要望がある",
      "x": 2.780957,
      "y": -1.9367688,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_1",
        "2_1",
        "3_4",
        "4_21"
      ]
    },
    {
      "arg_id": "A163_1",
      "argument": "AIを人間が制御するための人間参加型のシステムを目指すべきだという意見がある",
      "x": 0.58377993,
      "y": -0.19296668,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_8",
        "4_9"
      ]
    },
    {
      "arg_id": "A162_0",
      "argument": "AIをグローバル規模のオープンナレッジイニシアティブに組み込むべきだという提案がある",
      "x": 2.907386,
      "y": 0.48977655,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_11",
        "4_19"
      ]
    },
    {
      "arg_id": "A161_0",
      "argument": "AIの訓練に公的に利用可能なデータを使用することが不公平であるという不満がある",
      "x": -1.0073853,
      "y": 1.2763901,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_22"
      ]
    },
    {
      "arg_id": "A161_1",
      "argument": "AIの訓練に関する著作権改革が必要だという要望がある",
      "x": 1.4678375,
      "y": 0.4650905,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_9",
        "4_23"
      ]
    },
    {
      "arg_id": "A158_0",
      "argument": "AIの規制は大量データ収集の規制が改善されることに依存しているという意見がある",
      "x": 0.8026926,
      "y": 1.5535662,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_5",
        "3_10",
        "4_4"
      ]
    },
    {
      "arg_id": "A157_0",
      "argument": "技術の進化が人類や社会の対応能力を超えていることへの不安がある",
      "x": -0.9268852,
      "y": 0.4286136,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A156_0",
      "argument": "強力な技術を管理する信頼できる組織がどこなのかという不安がある",
      "x": -0.8520025,
      "y": 0.7263198,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A156_1",
      "argument": "ZuckやMuskなどの個人の利益に対する不安がある",
      "x": -0.57720554,
      "y": 0.717637,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_2",
        "2_4",
        "3_7",
        "4_2"
      ]
    },
    {
      "arg_id": "A154_0",
      "argument": "地域の市民社会組織が長期的で予測可能な資金を必要としているという要望がある",
      "x": 3.000243,
      "y": 2.3504295,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_6"
      ]
    },
    {
      "arg_id": "A154_1",
      "argument": "地域の市民社会組織が能力向上と相談のための資金を必要としているという要望がある",
      "x": 3.0793643,
      "y": 2.2915108,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_6"
      ]
    },
    {
      "arg_id": "A154_2",
      "argument": "地域の市民社会組織がグローバルマジョリティの意見を意味のある形で代表するための支援を必要としているという要望がある",
      "x": 3.1994734,
      "y": 2.3861666,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_6"
      ]
    },
    {
      "arg_id": "A150_0",
      "argument": "文明の移行期における課題を軽減するために、グローバルな福祉国家を創設してほしいという要望がある",
      "x": 3.4568887,
      "y": 1.9680444,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_7"
      ]
    },
    {
      "arg_id": "A149_0",
      "argument": "G7が主導する事実上の世界政府を創設するという要望がある",
      "x": 3.585198,
      "y": 1.4250205,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_7"
      ]
    },
    {
      "arg_id": "A149_1",
      "argument": "NATO、EU、欧州政治共同体、OECDのメンバーを含めるという要望がある",
      "x": 3.307757,
      "y": 1.6123799,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_7"
      ]
    },
    {
      "arg_id": "A147_0",
      "argument": "Superintelligence Programmeを管理するOne-AI JV会社を設立してAI制御能力を効率化するという要望がある",
      "x": 3.4055805,
      "y": 0.16174659,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_11",
        "4_10"
      ]
    },
    {
      "arg_id": "A146_0",
      "argument": "最先端のAI開発を一つの企業に統合するために、One-AI共同事業会社を設立するという要望がある",
      "x": 3.4649587,
      "y": 0.02735854,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_11",
        "4_10"
      ]
    },
    {
      "arg_id": "A145_0",
      "argument": "AI開発プロセスを管理するためのグローバルAIコントロール機関（GAICA）を設立してほしいという要望がある",
      "x": 3.3425906,
      "y": 0.4725342,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_11",
        "4_19"
      ]
    },
    {
      "arg_id": "A145_2",
      "argument": "中国など他国を招待してグローバルな協力体制を築いてほしいという要望がある",
      "x": 3.6145222,
      "y": 1.5318096,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_12",
        "4_7"
      ]
    },
    {
      "arg_id": "A144_0",
      "argument": "AIの使用を規制するために、Global AI Partnership (GPAI)を変革してGlobal AI Regulation Authority (GAIRA)を設立するという要望がある",
      "x": 3.1928926,
      "y": 0.8640509,
      "p": 0,
      "cluster_ids": [
        "0",
        "1_3",
        "2_6",
        "3_11",
        "4_19"
      ]
    }
  ],
  "clusters": [
    {
      "level": 0,
      "id": "0",
      "label": "全体",
      "takeaway": "",
      "value": 233,
      "parent": "",
      "density_rank_percentile": 0
    },
    {
      "level": 1,
      "id": "1_3",
      "label": "AI技術の倫理とグローバルガバナンスに関する視点",
      "takeaway": "このクラスタは、AI技術の開発と運用における倫理的管理と規制、そしてグローバルなガバナンスの強化に関する多様な意見を集約しています。具体的には、AIシステムの透明性と責任の確立、データ使用に対する補償、民主主義の改革、国際的な協力の強化などが含まれています。市民は、AI技術の発展と社会的影響のバランスを取るためのグローバルな基準とローカライズされた規制の必要性を強調し、より効果的な管理体制の確立を求めています。",
      "value": 74,
      "parent": "0",
      "density_rank_percentile": 0.6666666666666666
    },
    {
      "level": 1,
      "id": "1_1",
      "label": "AI技術の持続可能性と社会的責任",
      "takeaway": "このクラスタは、AI技術の開発と利用において、環境保護、持続可能なエネルギー運用、そして倫理的な設計と社会的影響への配慮を重視する意見を集約しています。市民は、AIが炭素排出量削減や自然災害の予測モデルの作成に貢献し、持続可能で公平な未来を築くための具体的な行動を促すことを望んでいます。また、AI技術が中小企業にもアクセス可能であり、技術革新が広く社会に普及することを期待しています。さらに、AI技術が生物多様性の保全に貢献し、自然環境や生物の視点を考慮する必要性を訴える意見も含まれています。これらの意見は、技術革新と環境保護、倫理的配慮の両立を目指す姿勢を反映しています。",
      "value": 119,
      "parent": "0",
      "density_rank_percentile": 1.0
    },
    {
      "level": 1,
      "id": "1_2",
      "label": "AI技術の倫理と社会的影響に対する懸念",
      "takeaway": "このクラスタは、AI技術の進展に伴う倫理的な運用や社会的影響に対する懸念を集約しています。具体的には、AIの自動化によるリスク、人間の仕事の置き換え、技術の過剰評価、そして公平性やバイアスに関する問題が含まれています。市民は、AI技術の開発と運用において、より透明性と公平性のあるアプローチを求めており、国際的な規制の枠組みの確立と、技術の進歩に伴う社会的課題への慎重な対応を強く訴えています。",
      "value": 40,
      "parent": "0",
      "density_rank_percentile": 0.3333333333333333
    },
    {
      "level": 2,
      "id": "2_5",
      "label": "AI技術の倫理と規制に関する多様な視点",
      "takeaway": "このクラスタは、AI技術の開発と運用における倫理的管理と規制に関する多様な意見を集約しています。具体的には、AIシステムの透明性と責任の確立、デジタルインフラの整備、AIスキルの訓練、データ使用に対する補償、著作権改革、そしてAI技術に対する包括的なリスク評価が含まれています。また、規制がイノベーションを妨げないことを望む意見や、個人のプライバシーを犠牲にしてでも規制を強化するべきだという意見も存在します。これらの意見は、AI技術の発展と社会的影響のバランスをどのように取るべきかについての議論を反映しており、グローバルな基準とローカライズされた規制の必要性を強調しています。",
      "value": 44,
      "parent": "1_3",
      "density_rank_percentile": 0.6666666666666666
    },
    {
      "level": 2,
      "id": "2_1",
      "label": "AI技術の持続可能な発展と倫理的配慮",
      "takeaway": "このクラスタは、AI技術の利用と開発において環境保護、持続可能なエネルギー運用、そして倫理的な設計と社会的影響への配慮を重視する意見を集約しています。市民は、AIが炭素排出量削減や自然災害の予測モデルの作成に貢献し、持続可能で公平な未来を築くための具体的な行動を促すことを望んでいます。また、AI技術が中小企業にもアクセス可能であり、技術革新が広く社会に普及することを期待しています。これらの意見は、技術革新と環境保護、倫理的配慮の両立を目指す姿勢を反映しています。",
      "value": 68,
      "parent": "1_1",
      "density_rank_percentile": 1.0
    },
    {
      "level": 2,
      "id": "2_4",
      "label": "AI技術の倫理と社会的影響に対する懸念",
      "takeaway": "このクラスタは、AI技術の進展に伴う倫理的な運用や社会的影響に対する懸念を集約しています。具体的には、AIの自動化によるリスク、人間の仕事の置き換え、技術の過剰評価、そして公平性やバイアスに関する問題が含まれています。市民は、AI技術の開発と運用において、より透明性と公平性のあるアプローチを求めており、国際的な規制の枠組みの確立と、技術の進歩に伴う社会的課題への慎重な対応を強く訴えています。",
      "value": 40,
      "parent": "1_2",
      "density_rank_percentile": 0.5
    },
    {
      "level": 2,
      "id": "2_3",
      "label": "AI技術の社会的公平性と環境への影響",
      "takeaway": "このクラスタは、AI技術が社会課題の解決に役立つことへの期待と、技術の進歩が社会全体に公平に利益をもたらすことへの懸念を集約しています。特に、AIが気候変動の緩和や貧困層の支援に貢献することが期待される一方で、エネルギー消費や環境への影響に対する不安も強調されています。技術革新が持続可能な未来を築くための具体的な取り組みが求められています。",
      "value": 25,
      "parent": "1_1",
      "density_rank_percentile": 0.3333333333333333
    },
    {
      "level": 2,
      "id": "2_2",
      "label": "AIと自然保護における倫理的配慮と生物多様性の保全",
      "takeaway": "このクラスタは、AI技術が生物多様性の保全に貢献することへの期待と、開発において自然環境や生物の視点を考慮する必要性を訴える意見を集約しています。具体的には、AIが衛星画像解析を通じて生物多様性の減少を追跡し、保全活動を支援することや、絶滅危惧種を保護するために活用されるべきだという要望が含まれています。また、AIが先住民の権利と文化を尊重し、自然のさまざまな要素間の関係を理解し尊重する形で作成・使用されるべきだという倫理的な配慮も強調されています。",
      "value": 26,
      "parent": "1_1",
      "density_rank_percentile": 0.1666666666666666
    },
    {
      "level": 2,
      "id": "2_6",
      "label": "グローバルガバナンスと技術管理の強化",
      "takeaway": "このクラスタは、民主主義の改革と市民社会の支援に対する要望、そしてAI技術の開発と使用に対するグローバルな管理・規制機関の設立を求める意見を集約しています。市民は、民主主義の制度を改善し、地域の市民社会組織が持続可能な活動を行うための安定した資金の確保を求めています。また、AI技術の安全性や倫理性を確保し、国際的な協力を強化することを目指しています。これにより、民主主義の害を最小化し、AIの進化とその管理がより効果的に行われ、社会に対するリスクを最小限に抑えることが期待されています。",
      "value": 30,
      "parent": "1_3",
      "density_rank_percentile": 0.8333333333333334
    },
    {
      "level": 3,
      "id": "3_9",
      "label": "AI技術の倫理的管理と公平なデジタルインフラの確立",
      "takeaway": "このクラスタは、AI技術の開発と運用における倫理的な管理と社会的公平性を求める意見を集約しています。具体的には、AIシステムの透明性と責任の確立、デジタルインフラの整備、AIスキルの訓練、データ使用に対する補償、そしてAIの訓練に関する著作権改革などが含まれています。これらの意見は、AI技術が社会に与える影響を公平かつ倫理的に管理し、グローバルな視点での規制やガイドラインの整備を求めるものです。",
      "value": 28,
      "parent": "2_5",
      "density_rank_percentile": 0.9166666666666666
    },
    {
      "level": 3,
      "id": "3_3",
      "label": "AI技術と環境保護の統合的アプローチ",
      "takeaway": "このクラスタは、AI技術の利用と開発において環境保護を重視する意見を集約しています。具体的には、AIの炭素排出量削減、持続可能な設計・展開、そしてAIガバナンスに環境代表者を含めるべきだという要望が含まれています。これらの意見は、技術革新と環境保護の両立を目指し、持続可能な未来を築くための具体的な行動を促すものです。",
      "value": 17,
      "parent": "2_1",
      "density_rank_percentile": 0.5
    },
    {
      "level": 3,
      "id": "3_1",
      "label": "AI技術の普及と倫理的設計への期待",
      "takeaway": "このクラスタは、AI技術が広く社会に普及し、中小企業や一般市民にもアクセス可能であるべきだという要望と、AIの設計と開発において倫理的な側面を重視する意見を集約しています。市民は、AIが企業の利益だけでなく、私たちの生活を向上させるために働くことを望んでおり、AI技術が人間の知能や創造性を補完し、持続可能で公平な未来を目指す姿勢を反映しています。",
      "value": 16,
      "parent": "2_1",
      "density_rank_percentile": 0.3333333333333333
    },
    {
      "level": 3,
      "id": "3_8",
      "label": "AI技術の倫理的運用と国際的規制の課題",
      "takeaway": "このクラスタは、AI技術の倫理的な運用に関する懸念と、国際的な規制の必要性を強調する意見を集約しています。具体的には、AIの自動化によるリスク、人間の監視の重要性、そしてグローバルな規制の困難さに対する不安が含まれています。これらの意見は、AI技術の進展に伴う倫理的な課題に対する慎重な対応と、透明性のある運用を求める声を反映しており、国際的な協力と規制の枠組みの確立を求めています。",
      "value": 21,
      "parent": "2_4",
      "density_rank_percentile": 0.8333333333333334
    },
    {
      "level": 3,
      "id": "3_6",
      "label": "AI技術の社会的公平性と環境への影響",
      "takeaway": "このクラスタは、AI技術が社会課題の解決に役立つことへの期待と、技術の進歩が社会全体に公平に利益をもたらすことへの懸念を集約しています。特に、AIが気候変動の緩和や貧困層の支援に貢献することが期待される一方で、エネルギー消費や環境への影響に対する不安も強調されています。技術革新が持続可能な未来を築くための具体的な取り組みが求められています。",
      "value": 25,
      "parent": "2_3",
      "density_rank_percentile": 1.0
    },
    {
      "level": 3,
      "id": "3_4",
      "label": "AI技術による環境保護と持続可能なエネルギー運用",
      "takeaway": "このクラスタは、AI技術が環境保護に貢献することへの期待と、AIシステムのエネルギー効率や持続可能性に対する要望を集約しています。市民は、AIが河川や海洋の汚染を減少させる創造的な解決策を提供し、クリーンで緑豊かな未来を目指すための協力を促進することを望んでいます。また、AIシステムの運用においてクリーンで再生可能なエネルギーの利用を求め、増加するエネルギー需要に対する計画の必要性を強調しています。これらの意見は、環境への配慮と持続可能な技術運用を目指す姿勢を反映しています。",
      "value": 25,
      "parent": "2_1",
      "density_rank_percentile": 0.5833333333333334
    },
    {
      "level": 3,
      "id": "3_2",
      "label": "AI技術の倫理的・社会的影響への配慮",
      "takeaway": "このクラスタには、AI技術の導入に際して自然環境や人間の尊厳を尊重すること、そしてAIリーダーが複数の倫理的視点を理解することの重要性を訴える意見が集まっています。また、AI技術の環境への影響だけでなく、社会的および倫理的な影響も評価してほしいという要望が含まれており、技術の進歩とともに倫理的な配慮を欠かさない姿勢が求められています。",
      "value": 10,
      "parent": "2_1",
      "density_rank_percentile": 0.0833333333333333
    },
    {
      "level": 3,
      "id": "3_10",
      "label": "AI規制に対する多様な意見と要望",
      "takeaway": "このクラスタには、AI規制に関する多様な意見が集まっています。一部の意見は、規制が新しいアイデアやイノベーションを妨げないことを望んでおり、柔軟で証拠に基づく規制枠組みを求めています。一方で、個人のプライバシーを犠牲にしてでも規制を強化するべきだという意見も存在します。これらの意見は、AI技術の発展と社会的影響のバランスをどのように取るべきかについての議論を反映しています。",
      "value": 16,
      "parent": "2_5",
      "density_rank_percentile": 0.6666666666666666
    },
    {
      "level": 3,
      "id": "3_5",
      "label": "AIと自然保護における倫理的配慮と生物多様性の保全",
      "takeaway": "このクラスタは、AI技術が生物多様性の保全に貢献することへの期待と、開発において自然環境や生物の視点を考慮する必要性を訴える意見を集約しています。具体的には、AIが衛星画像解析を通じて生物多様性の減少を追跡し、保全活動を支援することや、絶滅危惧種を保護するために活用されるべきだという要望が含まれています。また、AIが先住民の権利と文化を尊重し、自然のさまざまな要素間の関係を理解し尊重する形で作成・使用されるべきだという倫理的な配慮も強調されています。",
      "value": 26,
      "parent": "2_2",
      "density_rank_percentile": 0.4166666666666667
    },
    {
      "level": 3,
      "id": "3_7",
      "label": "AI技術の社会的影響と倫理的課題",
      "takeaway": "このクラスタは、AI技術の進展に伴う社会的影響や倫理的問題に対する懸念を集約しています。具体的には、AIが人間の仕事を置き換える可能性や、技術の過剰評価に対する疑念、そしてAI技術に関連する公平性の問題やバイアスに対する不満が含まれています。市民は、AI技術の開発と運用において、より公平で透明性のあるアプローチが求められていることを強く訴えており、技術の進歩に伴う社会的課題に対する深い関心が表れています。",
      "value": 19,
      "parent": "2_4",
      "density_rank_percentile": 0.25
    },
    {
      "level": 3,
      "id": "3_12",
      "label": "グローバル民主主義と市民社会の強化",
      "takeaway": "このクラスタは、民主主義の改革と市民社会の支援に対する要望、そして国際的な協力体制の強化を求める意見を集約しています。市民は、民主主義の制度を改善し、地域の市民社会組織が持続可能な活動を行うための安定した資金の確保を求めています。また、国際社会の連携強化や統一的なリーダーシップの必要性を強調し、グローバルな福祉国家の創設を通じて文明の移行期における課題を軽減することを期待しています。",
      "value": 19,
      "parent": "2_6",
      "density_rank_percentile": 0.75
    },
    {
      "level": 3,
      "id": "3_11",
      "label": "AI技術のグローバル管理と効率的統合への期待",
      "takeaway": "このクラスタは、AI技術の開発と使用に対するグローバルな管理・規制機関の設立、およびAI制御能力の効率化と統合を求める意見を集約しています。具体的には、GAIRAやGAICAといった新しい組織の設立を通じて、AI技術の安全性や倫理性を確保し、国際的な協力を強化することを目指しています。また、最先端のAI開発を一つの企業に統合することで、効率的な管理と制御が可能になるという期待が示されています。これにより、AIの進化とその管理がより効果的に行われ、社会に対するリスクを最小限に抑えることが期待されています。",
      "value": 11,
      "parent": "2_6",
      "density_rank_percentile": 0.1666666666666666
    },
    {
      "level": 4,
      "id": "4_1",
      "label": "AIの倫理的開発と国際的な基準への期待",
      "takeaway": "このクラスタには、AI技術の開発と適用において人権法や価値観に基づいた倫理的なアプローチを求める意見が集まっています。また、AIに関する国際的な基準の確立や、AI安全サミットを通じた議論の場の提供を望む声も含まれています。これらの意見は、AI技術が社会に与える影響を考慮し、グローバルな視点での規制やガイドラインの整備を求めるものです。",
      "value": 9,
      "parent": "3_9",
      "density_rank_percentile": 0.75
    },
    {
      "level": 4,
      "id": "4_20",
      "label": "AIと環境保護への責任",
      "takeaway": "このクラスタには、AI技術の利用において環境への配慮を求める意見が集まっています。具体的には、AIの炭素排出量削減、環境保護のための責任ある使用、そしてAIガバナンスに環境代表者を含めるべきだという要望が含まれています。これらの意見は、AI技術が環境に与える影響を認識し、持続可能な未来を目指すための具体的な行動を促すものです。",
      "value": 10,
      "parent": "3_3",
      "density_rank_percentile": 0.5416666666666666
    },
    {
      "level": 4,
      "id": "4_17",
      "label": "AI技術の普及と市民への利益への期待",
      "takeaway": "このクラスタには、AI技術が中小企業や一般市民にも広くアクセス可能であるべきだという要望が集まっています。市民は、AIが企業の利益だけでなく、私たちの生活を向上させるために働くことを望んでいます。また、AI企業を支援する際には、普通の人々に具体的な利益をもたらすことが条件であるという意見も含まれています。これらの意見は、AI技術の普及とその恩恵が広く社会に行き渡ることへの期待を表しています。",
      "value": 8,
      "parent": "3_1",
      "density_rank_percentile": 0.3333333333333333
    },
    {
      "level": 4,
      "id": "4_9",
      "label": "AIの倫理的運用と人間の監視の重要性",
      "takeaway": "このクラスタには、AIの運用に関する倫理的な懸念と、人間の監視や制御の重要性を強調する意見が集まっています。具体的には、AIが環境決定を自動化する際のリスク、AIシステムに人間が積極的に参加する必要性、そして企業がAIを人々の利益に反する形で利用することへの反対が含まれています。これらの意見は、AI技術の発展に伴う倫理的な課題に対する慎重な対応と、透明性のある運用を求める声を反映しています。",
      "value": 12,
      "parent": "3_8",
      "density_rank_percentile": 0.9583333333333334
    },
    {
      "level": 4,
      "id": "4_3",
      "label": "AIの設計と開発に対する倫理的要望",
      "takeaway": "このクラスタには、AI技術の設計と開発において倫理的な側面を重視する要望が集まっています。具体的には、AIが人間の知能や創造性を補完する役割を果たし、置き換えることなく共存すること、効率や利益だけでなく正義、エンパワーメント、環境保護を促進すること、そしてイノベーションを推進することが求められています。これらの要望は、AI技術が社会に与える影響を考慮し、持続可能で公平な未来を目指す姿勢を反映しています。",
      "value": 8,
      "parent": "3_1",
      "density_rank_percentile": 0.5833333333333334
    },
    {
      "level": 4,
      "id": "4_13",
      "label": "技術革新による社会課題解決への期待",
      "takeaway": "このクラスタには、AI技術やGMOsなどの先端技術が気候変動や食糧不足といった重大な社会課題の解決に役立つことを期待する意見が集まっています。また、地方自治体の管理を技術企業に委ねることで効率化を図るべきだという意見も含まれており、技術革新が社会の構造や環境に与える影響を前向きに捉え、積極的に活用する姿勢が見られます。",
      "value": 11,
      "parent": "3_6",
      "density_rank_percentile": 0.875
    },
    {
      "level": 4,
      "id": "4_12",
      "label": "AIによる環境保護と持続可能な未来への期待",
      "takeaway": "このクラスタには、AI技術が環境保護に貢献することへの期待が集まっています。具体的には、河川や海洋の汚染を減少させる創造的な解決策の考案、清潔で緑豊かな未来を目指すための人々の協力促進、そしてクリーンな空気と騒音の少ない都市の創造を支援することが挙げられます。市民は、AIが環境問題の解決に向けた革新的なアプローチを提供し、持続可能な未来の実現に寄与することを強く望んでいます。",
      "value": 13,
      "parent": "3_4",
      "density_rank_percentile": 0.625
    },
    {
      "level": 4,
      "id": "4_11",
      "label": "AI技術の倫理的・社会的影響への配慮",
      "takeaway": "このクラスタには、AI技術の導入に際して自然環境や人間の尊厳を尊重すること、そしてAIリーダーが複数の倫理的視点を理解することの重要性を訴える意見が集まっています。また、AI技術の環境への影響だけでなく、社会的および倫理的な影響も評価してほしいという要望が含まれており、技術の進歩とともに倫理的な配慮を欠かさない姿勢が求められています。",
      "value": 10,
      "parent": "3_2",
      "density_rank_percentile": 0.6666666666666666
    },
    {
      "level": 4,
      "id": "4_8",
      "label": "AI技術の環境への配慮と持続可能性への要望",
      "takeaway": "このクラスタには、AI技術の開発と導入に際して環境への影響を減らすことに焦点を当てるべきだという意見が集まっています。具体的には、AI技術のライフサイクル全体で環境影響を評価し、潜在的な悪影響を理解し、軽減する必要があるという要望や、AIシステムが持続可能性を考慮して設計・展開されるべきだという意見が含まれています。これらの意見は、技術革新と環境保護の両立を目指す姿勢を示しています。",
      "value": 7,
      "parent": "3_3",
      "density_rank_percentile": 0.375
    },
    {
      "level": 4,
      "id": "4_23",
      "label": "AIとデジタルインフラに関する公平性と倫理的要望",
      "takeaway": "このクラスタには、AI技術の進展に伴う社会的公平性や倫理的な問題に対する意見が集まっています。具体的には、デジタルインフラの整備とAIスキルの訓練を通じて不平等を防ぐ必要性、AIの意思決定における人間の関与の重要性、そしてAIシステムでのデータ使用に対する補償を求める要望が含まれています。これらの意見は、技術の進化がもたらす影響を公平かつ倫理的に管理するための具体的な対策を求めるものです。",
      "value": 12,
      "parent": "3_9",
      "density_rank_percentile": 0.7916666666666666
    },
    {
      "level": 4,
      "id": "4_4",
      "label": "AI規制に対する多様な意見と要望",
      "takeaway": "このクラスタには、AI規制に関する多様な意見が集まっています。一部の意見は、規制が新しいアイデアやイノベーションを妨げないことを望んでおり、柔軟で証拠に基づく規制枠組みを求めています。一方で、個人のプライバシーを犠牲にしてでも規制を強化するべきだという意見も存在します。これらの意見は、AI技術の発展と社会的影響のバランスをどのように取るべきかについての議論を反映しています。",
      "value": 16,
      "parent": "3_10",
      "density_rank_percentile": 1.0
    },
    {
      "level": 4,
      "id": "4_18",
      "label": "AIシステムの透明性と責任の確立への要望",
      "takeaway": "このクラスタには、AIシステムに対する公共の信頼を築くために、アルゴリズムの透明性や使用する方法論とデータセットの公開を求める意見が集まっています。また、AIに関する決定を行う政府や企業がその責任を取るべきだという要望も含まれており、AI技術の開発と運用における倫理的な責任と透明性の確立を強く求める声が表れています。",
      "value": 7,
      "parent": "3_9",
      "density_rank_percentile": 0.2083333333333333
    },
    {
      "level": 4,
      "id": "4_5",
      "label": "AIの社会的公平性と環境リスク管理への期待",
      "takeaway": "このクラスタには、AI技術の発展に伴う社会的公平性の確保と環境リスク管理に対する期待が集まっています。特に、AIが貧困層を忘れずに支援し、富裕層だけが利益を享受することのないようにすることが重要だという意見が強調されています。また、AIが環境問題に対してリスクを監視し、軽減する役割を果たすことへの期待も含まれており、技術の進歩が社会全体に公平に利益をもたらし、持続可能な未来を築くための具体的な取り組みが求められています。",
      "value": 7,
      "parent": "3_6",
      "density_rank_percentile": 0.8333333333333334
    },
    {
      "level": 4,
      "id": "4_14",
      "label": "AIの環境影響に対する懸念と改善要望",
      "takeaway": "このクラスタには、AI技術のエネルギー使用量や環境への影響に対する不安や不満が集まっています。ユーザーは、AIが環境悪化に寄与していないことを確認する必要性を感じており、自然環境への影響が十分に考慮されていないことに対して懸念を抱いています。これらの意見は、AI技術の持続可能性や環境保護に対する改善を求める声を反映しています。",
      "value": 7,
      "parent": "3_6",
      "density_rank_percentile": 0.125
    },
    {
      "level": 4,
      "id": "4_21",
      "label": "AIシステムのエネルギー効率と持続可能性への要望",
      "takeaway": "このクラスタには、AIシステムの運用に関するエネルギー効率や持続可能性に対する要望が集まっています。具体的には、クリーンで再生可能なエネルギーの利用、増加するエネルギー需要に対する計画の必要性、そしてAIの設計においてエネルギー効率を重視することが求められています。これらの意見は、環境への配慮と持続可能な技術運用を目指す姿勢を反映しています。",
      "value": 12,
      "parent": "3_4",
      "density_rank_percentile": 0.4583333333333333
    },
    {
      "level": 4,
      "id": "4_0",
      "label": "AIによる生物多様性保全と倫理的配慮への期待",
      "takeaway": "このクラスタには、AI技術が人間だけでなく、植物や動物を含むすべての存在の価値を理解し、保全活動を支援する役割を果たすことへの期待が集まっています。具体的には、衛星画像解析を通じて生物多様性の減少を追跡し、保全活動を支援することや、AIが生態系の権利を尊重するべきだという倫理的な要望が含まれています。これらの意見は、AIが環境保護と持続可能な未来の構築に貢献することを強く求めています。",
      "value": 16,
      "parent": "3_5",
      "density_rank_percentile": 0.7083333333333334
    },
    {
      "level": 4,
      "id": "4_2",
      "label": "AI技術の評価と社会的影響への懸念",
      "takeaway": "このクラスタには、AI技術の未来に対する不安や過剰評価に対する疑念、そして企業や国がAI競争に集中する一方で社会的影響を十分に考慮していないことへの不満が集まっています。また、AIが人間の仕事を置き換える可能性についての懸念も含まれており、技術の進歩に伴う社会的課題や倫理的問題に対する深い関心が表れています。",
      "value": 13,
      "parent": "3_7",
      "density_rank_percentile": 0.2916666666666667
    },
    {
      "level": 4,
      "id": "4_22",
      "label": "AI技術における公平性とバイアスへの懸念",
      "takeaway": "このクラスタには、AI技術に関連する公平性の問題やバイアスに対する不満が集まっています。具体的には、顔認識技術における人種的偏見、公的に利用可能なデータの使用に関する不公平感、そしてAIがバイアスの原因とされることへの不満が含まれています。これらの意見は、AI技術の開発と運用において、より公平で透明性のあるアプローチが求められていることを示しています。",
      "value": 6,
      "parent": "3_7",
      "density_rank_percentile": 0.0833333333333333
    },
    {
      "level": 4,
      "id": "4_6",
      "label": "民主主義の改革と市民社会の支援への要望",
      "takeaway": "このクラスタには、民主主義の利益を最大化するための議論や包括的な改革の必要性、そして地域の市民社会組織が安定した資金を求める要望が集まっています。市民は、民主主義の制度を改善し、より効果的で迅速な改革を行うことで、社会全体の利益を高めることを期待しています。また、市民社会組織が持続可能な活動を行うためには、長期的で予測可能な資金の確保が重要であると認識されています。",
      "value": 10,
      "parent": "3_12",
      "density_rank_percentile": 0.9166666666666666
    },
    {
      "level": 4,
      "id": "4_16",
      "label": "AI開発における自然保護と倫理的配慮",
      "takeaway": "このクラスタには、AI開発において自然環境や生物の視点を考慮する必要性を訴える意見が集まっています。自然の価値を無視する慣行に対する懸念や、絶滅危惧種を保護するためにAIを活用する重要性が強調されています。これらの意見は、技術の進歩が環境保護と倫理的配慮を両立させるべきだという考えを反映しています。",
      "value": 10,
      "parent": "3_5",
      "density_rank_percentile": 0.4166666666666667
    },
    {
      "level": 4,
      "id": "4_15",
      "label": "グローバルなAI規制の困難と必要性",
      "takeaway": "このクラスタには、AI規制に関するグローバルな合意の難しさや、競争が優先される現状に対する不安が集まっています。また、AIが政府の運営に与える影響についての議論の必要性も含まれており、国際的な協力と規制の枠組みを求める声が強く表れています。これらの意見は、AI技術の進展に伴うリスク管理と倫理的な運用を確保するための国際的な取り組みの重要性を示しています。",
      "value": 9,
      "parent": "3_8",
      "density_rank_percentile": 0.5
    },
    {
      "level": 4,
      "id": "4_7",
      "label": "グローバルな協力体制と世界政府の設立要望",
      "takeaway": "このクラスタには、国際的な協力体制の強化や、G7が主導する事実上の世界政府の設立を求める意見が集まっています。中国など他国を招待してグローバルな協力体制を築くことや、G7による世界政府の設立を通じて、国際的な問題解決や平和維持を目指す要望が含まれています。これらの意見は、国際社会の連携強化と統一的なリーダーシップの必要性を強調しています。",
      "value": 9,
      "parent": "3_12",
      "density_rank_percentile": 0.25
    },
    {
      "level": 4,
      "id": "4_19",
      "label": "グローバルAI管理・規制機関設立の要望",
      "takeaway": "このクラスタには、AI開発プロセスや使用を管理・規制するためのグローバルな機関の設立を求める意見が集まっています。具体的には、GAICAやGAIRAといった新しい組織の設立を通じて、AI技術の安全性や倫理性を確保し、国際的な協力を強化することを目指しています。これにより、AIの開発と利用が適切に管理され、社会に対するリスクを最小限に抑えることが期待されています。",
      "value": 7,
      "parent": "3_11",
      "density_rank_percentile": 0.1666666666666666
    },
    {
      "level": 4,
      "id": "4_10",
      "label": "AI制御能力の効率化と統合への要望",
      "takeaway": "このクラスタには、AI制御能力の効率化を目指して、スーパーインテリジェンスプログラムを管理するOne-AI JV会社の設立を求める意見が集まっています。最先端のAI開発を一つの企業に統合することで、効率的な管理と制御が可能になるという期待が示されています。これにより、AI技術の進化とその管理がより効果的に行われることを目指しています。",
      "value": 4,
      "parent": "3_11",
      "density_rank_percentile": 0.0416666666666666
    }
  ],
  "comments": {},
  "propertyMap": {},
  "translations": {},
  "overview": "AI技術に関するパブリック・コンサルテーションの結果は、以下の3つのクラスターに分けられます。クラスター0は、AI技術の倫理的管理とグローバルガバナンスの強化を求める意見を集約し、透明性と責任の確立を強調しています。クラスター1は、環境保護と持続可能性、社会的責任を重視し、AI技術が炭素排出量削減や自然災害予測に貢献することを期待しています。クラスター2は、AI技術の進展に伴う倫理的懸念や社会的影響に対する懸念を集約し、透明性と公平性のあるアプローチを求めています。",
  "config": {
    "name": "Recursive Public, Agenda Setting",
    "question": "人類が人工知能を開発・展開する上で、最優先すべき課題は何でしょうか？",
    "input": "example-polis",
    "model": "gemini-2.0-flash",
    "extraction": {
      "workers": 3,
      "limit": 200,
      "properties": [],
      "categories": {},
      "category_batch_size": 5,
      "source_code": "import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.category_classification import classify_args\nfrom services.llm import request_to_chat_llm\nfrom services.parse_json_list import parse_response\nfrom hierarchical_utils import update_progress # 前まではbroadlistening.utilsから呼び出していた。これでエラーになったらもとに戻す。\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\ndef _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:\n    if not all(property in comments.columns for property in property_columns):\n        raise ValueError(f\"Properties {property_columns} not found in comments. Columns are {comments.columns}\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n    property_columns = config[\"extraction\"][\"properties\"]\n\n    # カラム名だけを読み込み、必要なカラムが含まれているか確認する\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\", nrows=0)\n    _validate_property_columns(property_columns, comments)\n    # エラーが出なかった場合、すべての行を読み込む\n    comments = pd.read_csv(\n        f\"inputs/{config['input']}.csv\", usecols=[\"comment-id\", \"comment-body\"] + config[\"extraction\"][\"properties\"]\n    )\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    argument_map = {}\n    relation_rows = []\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n\n        for comment_id, extracted_args in zip(batch, batch_results, strict=False):\n            for j, arg in enumerate(extracted_args):\n                if arg not in argument_map:\n                    # argumentテーブルに追加\n                    arg_id = f\"A{comment_id}_{j}\"\n                    argument_map[arg] = {\n                        \"arg-id\": arg_id,\n                        \"argument\": arg,\n                    }\n                else:\n                    arg_id = argument_map[arg][\"arg-id\"]\n\n                # relationテーブルにcommentとargの関係を追加\n                relation_row = {\n                    \"arg-id\": arg_id,\n                    \"comment-id\": comment_id,\n                }\n                relation_rows.append(relation_row)\n\n        update_progress(config, incr=len(batch))\n\n    # DataFrame化\n    results = pd.DataFrame(argument_map.values())\n    relation_df = pd.DataFrame(relation_rows)\n\n    if results.empty:\n        raise RuntimeError(\"result is empty, maybe bad prompt\")\n\n    classification_categories = config[\"extraction\"][\"categories\"]\n    if classification_categories:\n        results = classify_args(results, config, workers)\n\n    results.to_csv(path, index=False)\n    # comment-idとarg-idの関係を保存\n    relation_df.to_csv(f\"outputs/{dataset}/relations.csv\", index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures_with_index = [\n            (i, executor.submit(extract_arguments, input, prompt, model)) for i, input in enumerate(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)\n        results = [[] for _ in range(len(batch))]\n\n        for _, future in futures_with_index:\n            if future in not_done and not future.cancelled():\n                future.cancel()\n\n        for i, future in futures_with_index:\n            if future in done:\n                try:\n                    result = future.result()\n                    results[i] = result\n                except Exception as e:\n                    logging.error(f\"Task {future} failed with error: {e}\")\n                    results[i] = []\n        return results\n\n\n# def extract_by_llm(input, prompt, model):\n#     messages = [\n#         {\"role\": \"system\", \"content\": prompt},\n#         {\"role\": \"user\", \"content\": input},\n#     ]\n#     response = request_to_chat_llm(messages=messages, model=model)\n#     return response\n\n\ndef extract_arguments(input, prompt, model, retries=1):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_llm(messages=messages, model=model, is_json=False)\n        items = parse_response(response)\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []",
      "prompt": "# server/broadlistening/pipeline/prompts/extraction/default.txt の修正\n\n/system\nあなたは専門的なリサーチアシスタントで、与えられたテキストから「要望」「不満」「不安」に関連する意見を抽出する役割です。\n抽出した意見は、それぞれ独立した簡潔な文にしてください。意見が複数ある場合は、すべて抽出してください。\n結果は整形されたJSON形式の文字列リストとして返してください。\n抽出する意見は必ず日本語で作成してください。\n\n/human\nAI技術の進化は目覚ましいが、雇用が奪われるのではないかと心配だし、もっと透明性を高めてほしい。規制も必要だと思う。\n\n/ai\n[\n  \"AIによって雇用が奪われるのではないかという不安がある\",\n  \"AI技術の透明性を高めてほしいという要望がある\",\n  \"AI技術に対する規制が必要だという意見がある\"\n]\n\n/human\n特に問題点は感じていない。現状維持で良い。\n\n/ai\n[]\n\n/human\n新しい公園の計画は素晴らしいが、アクセス道路が狭いのが気になる。子供たちの安全は大丈夫だろうか。もっと広い道路を整備してほしい。\n\n/ai\n[\n  \"新しい公園へのアクセス道路が狭いことへの不満がある\",\n  \"アクセス道路での子供たちの安全に対する不安がある\",\n  \"より広いアクセス道路を整備してほしいという要望がある\"\n]\n\n/human\nサポート体制には満足しているが、もう少し迅速に対応してもらえると助かる。\n\n/ai\n[\n  \"サポートの対応速度をもう少し迅速にしてほしいという要望がある\"\n]\n\n/human\n関節リウマチの治療を続けているけど、副作用がつらくて仕事に支障が出ている。薬の効果はあるけれど、もっと副作用の少ない治療法があればいいのに。将来、歩けなくなるんじゃないかと不安もある。\n\n/ai\n[\n  \"治療薬の副作用によって仕事に支障が出ていることへの不満がある\",\n  \"副作用の少ない治療法を求める要望がある\",\n  \"将来歩けなくなるかもしれないという不安がある\"\n]\n\n/human\n2型糖尿病の治療でGLP-1受容体作動薬を使用しているが、注射が嫌で中断する患者が多い。もっと服薬コンプライアンスを高められる経口薬の選択肢が増えてほしい。また、長期的な心血管リスクへの影響がまだ不透明なのも気になっている。\n\n/ai\n[\n  \"GLP-1受容体作動薬が注射であるために患者の治療継続が困難になることへの不満がある\",\n  \"服薬コンプライアンスを高められる経口薬の選択肢を増やしてほしいという要望がある\",\n  \"GLP-1受容体作動薬の長期的な心血管リスクへの影響が不透明であることに対する不安がある\"\n]\n\n/human\n肺がんの治療で免疫チェックポイント阻害薬を使っているけど、効果が出るかどうか分からないまま高額な費用を払い続けるのがつらい。副作用も人によって違うし、もっと分かりやすい説明があれば安心できるのに。\n\n/ai\n[\n  \"免疫チェックポイント阻害薬の効果が不確実なまま高額な治療費を負担していることへの不満がある\",\n  \"副作用の個人差についての説明が不十分であることへの不安がある\",\n  \"治療に関してもっと分かりやすい説明を求める要望がある\"\n]\n\n/human\n非小細胞肺がんに対する免疫療法は確かに有効例もあるが、効果の予測因子が限定的で、どの患者に効くか判断が難しい。もっと精度の高いバイオマーカーが必要だ。\n\n/ai\n[\n  \"免疫療法の効果を予測する因子が限定的であることへの不満がある\",\n  \"免疫療法の適応を見極めるための精度の高いバイオマーカーが必要だという要望がある\"\n]\n\n/human\n家族がうつ病で治療中だけど、薬を飲んでもなかなか改善しない。通院のたびに仕事を休まないといけないのもつらいし、周囲の理解が足りないと感じる。もっと柔軟な支援制度があればいいのに。\n\n/ai\n[\n  \"抗うつ薬を服用しても改善が見られないことへの不満がある\",\n  \"通院に付き添うために仕事を休まなければならない負担への不満がある\",\n  \"精神疾患への社会的理解が不足していることへの不満がある\",\n  \"より柔軟な支援制度を望む要望がある\"\n]\n\n/human\n精神疾患の早期発見は重要だが、現場では予算や人材が足りていない。スクールカウンセラーや企業内メンタルヘルスの体制をもっと充実させる必要がある。\n\n/ai\n[\n  \"精神疾患対策に必要な予算や人材が不足していることへの不満がある\",\n  \"スクールカウンセラーや企業内メンタルヘルスの体制を充実させる必要があるという要望がある\"\n]\n\n/human\n息子がデュシェンヌ型筋ジストロフィーと診断されたけれど、根本的な治療がまだ確立されていないのが苦しい。臨床試験に参加させたいが、近くの病院では扱っていない。もっと地方にも研究機会を広げてほしい。\n\n/ai\n[\n  \"デュシェンヌ型筋ジストロフィーに根本的な治療法が存在しないことへの不満がある\",\n  \"臨床試験に参加する機会が地域によって限られていることへの不満がある\",\n  \"地方にも研究機会を広げてほしいという要望がある\"\n]\n\n/human\nDMDの治療ではステロイドが主流だが、長期使用による副作用が問題になる。新しい分子標的治療が出てきてはいるが、保険適用が進まず現場で使いにくい。\n\n/ai\n[\n  \"DMD治療におけるステロイドの長期使用で副作用が問題になることへの不満がある\",\n  \"新しい治療法が保険適用されず現場で使いにくいことへの不満がある\"\n]",
      "model": "gemini-2.0-flash"
    },
    "hierarchical_clustering": {
      "cluster_nums": [
        3,
        6,
        12,
        24
      ],
      "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import KMeans\n\n\ndef hierarchical_clustering(config):\n    UMAP = import_module(\"umap\").UMAP\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    cluster_nums = config[\"hierarchical_clustering\"][\"cluster_nums\"]\n\n    n_samples = embeddings_array.shape[0]\n    # デフォルト設定は15\n    default_n_neighbors = 15\n\n    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる\n    if n_samples <= default_n_neighbors:\n        n_neighbors = max(2, n_samples - 1)  # 最低2以上\n    else:\n        n_neighbors = default_n_neighbors\n\n    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)\n    # TODO 詳細エラーメッセージを加える\n    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因\n    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.\n    umap_embeds = umap_model.fit_transform(embeddings_array)\n\n    cluster_results = hierarchical_clustering_embeddings(\n        umap_embeds=umap_embeds,\n        cluster_nums=cluster_nums,\n    )\n    result_df = pd.DataFrame(\n        {\n            \"arg-id\": arguments_df[\"arg-id\"],\n            \"argument\": arguments_df[\"argument\"],\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        }\n    )\n\n    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):\n        result_df[f\"cluster-level-{cluster_level}-id\"] = [f\"{cluster_level}_{label}\" for label in final_labels]\n\n    result_df.to_csv(path, index=False)\n\n\n# def generate_cluster_count_list(min_clusters: int, max_clusters: int):\n#     cluster_counts = []\n#     current = min_clusters\n#     cluster_counts.append(current)\n\n#     if min_clusters == max_clusters:\n#         return cluster_counts\n\n#     while True:\n#         next_double = current * 2\n#         next_triple = current * 3\n\n#         if next_double >= max_clusters:\n#             if cluster_counts[-1] != max_clusters:\n#                 cluster_counts.append(max_clusters)\n#             break\n\n#         # 次の倍はまだ max_clusters に収まるが、3倍だと超える\n#         # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ\n#         if next_triple > max_clusters:\n#             cluster_counts.append(max_clusters)\n#             break\n\n#         cluster_counts.append(next_double)\n#         current = next_double\n\n#     return cluster_counts\n\n\ndef merge_clusters_with_hierarchy(\n    cluster_centers: np.ndarray,\n    kmeans_labels: np.ndarray,\n    umap_array: np.ndarray,\n    n_cluster_cut: int,\n):\n    Z = sch.linkage(cluster_centers, method=\"ward\")\n    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion=\"maxclust\")\n\n    n_samples = umap_array.shape[0]\n    final_labels = np.zeros(n_samples, dtype=int)\n\n    for i in range(n_samples):\n        original_label = kmeans_labels[i]\n        final_labels[i] = cluster_labels_merged[original_label]\n\n    return final_labels\n\n\ndef hierarchical_clustering_embeddings(\n    umap_embeds,\n    cluster_nums,\n):\n    # 最大分割数でクラスタリングを実施\n    print(\"start initial clustering\")\n    initial_cluster_num = cluster_nums[-1]\n    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)\n    kmeans_model.fit(umap_embeds)\n    print(\"end initial clustering\")\n\n    results = {}\n    print(\"start hierarchical clustering\")\n    cluster_nums.sort()\n    print(cluster_nums)\n    for n_cluster_cut in cluster_nums[:-1]:\n        print(\"n_cluster_cut: \", n_cluster_cut)\n        final_labels = merge_clusters_with_hierarchy(\n            cluster_centers=kmeans_model.cluster_centers_,\n            kmeans_labels=kmeans_model.labels_,\n            umap_array=umap_embeds,\n            n_cluster_cut=n_cluster_cut,\n        )\n        results[n_cluster_cut] = final_labels\n\n    results[initial_cluster_num] = kmeans_model.labels_\n    print(\"end hierarchical clustering\")\n\n    return results"
    },
    "intro": "このAI生成レポートは、Recursive Publicチームが実施したPolis協議のデータに基づいています。\n分析対象となったデータの件数は200件で、これらのデータに対してOpenAI APIを用いて233件の意見（議論）を抽出し、クラスタリングを行った。\n",
    "output_dir": "hierarchical-example-polis",
    "previous": {
      "name": "Recursive Public, Agenda Setting",
      "question": "人類が人工知能を開発・展開する上で、最優先すべき課題は何でしょうか？",
      "input": "example-polis",
      "model": "gemini-2.0-flash",
      "extraction": {
        "workers": 3,
        "limit": 200,
        "properties": [],
        "categories": {},
        "category_batch_size": 5,
        "source_code": "import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.category_classification import classify_args\nfrom services.llm import request_to_chat_llm\nfrom services.parse_json_list import parse_response\nfrom hierarchical_utils import update_progress # 前まではbroadlistening.utilsから呼び出していた。これでエラーになったらもとに戻す。\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\ndef _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:\n    if not all(property in comments.columns for property in property_columns):\n        raise ValueError(f\"Properties {property_columns} not found in comments. Columns are {comments.columns}\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n    property_columns = config[\"extraction\"][\"properties\"]\n\n    # カラム名だけを読み込み、必要なカラムが含まれているか確認する\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\", nrows=0)\n    _validate_property_columns(property_columns, comments)\n    # エラーが出なかった場合、すべての行を読み込む\n    comments = pd.read_csv(\n        f\"inputs/{config['input']}.csv\", usecols=[\"comment-id\", \"comment-body\"] + config[\"extraction\"][\"properties\"]\n    )\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    argument_map = {}\n    relation_rows = []\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n\n        for comment_id, extracted_args in zip(batch, batch_results, strict=False):\n            for j, arg in enumerate(extracted_args):\n                if arg not in argument_map:\n                    # argumentテーブルに追加\n                    arg_id = f\"A{comment_id}_{j}\"\n                    argument_map[arg] = {\n                        \"arg-id\": arg_id,\n                        \"argument\": arg,\n                    }\n                else:\n                    arg_id = argument_map[arg][\"arg-id\"]\n\n                # relationテーブルにcommentとargの関係を追加\n                relation_row = {\n                    \"arg-id\": arg_id,\n                    \"comment-id\": comment_id,\n                }\n                relation_rows.append(relation_row)\n\n        update_progress(config, incr=len(batch))\n\n    # DataFrame化\n    results = pd.DataFrame(argument_map.values())\n    relation_df = pd.DataFrame(relation_rows)\n\n    if results.empty:\n        raise RuntimeError(\"result is empty, maybe bad prompt\")\n\n    classification_categories = config[\"extraction\"][\"categories\"]\n    if classification_categories:\n        results = classify_args(results, config, workers)\n\n    results.to_csv(path, index=False)\n    # comment-idとarg-idの関係を保存\n    relation_df.to_csv(f\"outputs/{dataset}/relations.csv\", index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures_with_index = [\n            (i, executor.submit(extract_arguments, input, prompt, model)) for i, input in enumerate(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)\n        results = [[] for _ in range(len(batch))]\n\n        for _, future in futures_with_index:\n            if future in not_done and not future.cancelled():\n                future.cancel()\n\n        for i, future in futures_with_index:\n            if future in done:\n                try:\n                    result = future.result()\n                    results[i] = result\n                except Exception as e:\n                    logging.error(f\"Task {future} failed with error: {e}\")\n                    results[i] = []\n        return results\n\n\n# def extract_by_llm(input, prompt, model):\n#     messages = [\n#         {\"role\": \"system\", \"content\": prompt},\n#         {\"role\": \"user\", \"content\": input},\n#     ]\n#     response = request_to_chat_llm(messages=messages, model=model)\n#     return response\n\n\ndef extract_arguments(input, prompt, model, retries=1):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_llm(messages=messages, model=model, is_json=False)\n        items = parse_response(response)\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []",
        "prompt": "# server/broadlistening/pipeline/prompts/extraction/default.txt の修正\n\n/system\nあなたは専門的なリサーチアシスタントで、与えられたテキストから「要望」「不満」「不安」に関連する意見を抽出する役割です。\n抽出した意見は、それぞれ独立した簡潔な文にしてください。意見が複数ある場合は、すべて抽出してください。\n結果は整形されたJSON形式の文字列リストとして返してください。\n抽出する意見は必ず日本語で作成してください。\n\n/human\nAI技術の進化は目覚ましいが、雇用が奪われるのではないかと心配だし、もっと透明性を高めてほしい。規制も必要だと思う。\n\n/ai\n[\n  \"AIによって雇用が奪われるのではないかという不安がある\",\n  \"AI技術の透明性を高めてほしいという要望がある\",\n  \"AI技術に対する規制が必要だという意見がある\"\n]\n\n/human\n特に問題点は感じていない。現状維持で良い。\n\n/ai\n[]\n\n/human\n新しい公園の計画は素晴らしいが、アクセス道路が狭いのが気になる。子供たちの安全は大丈夫だろうか。もっと広い道路を整備してほしい。\n\n/ai\n[\n  \"新しい公園へのアクセス道路が狭いことへの不満がある\",\n  \"アクセス道路での子供たちの安全に対する不安がある\",\n  \"より広いアクセス道路を整備してほしいという要望がある\"\n]\n\n/human\nサポート体制には満足しているが、もう少し迅速に対応してもらえると助かる。\n\n/ai\n[\n  \"サポートの対応速度をもう少し迅速にしてほしいという要望がある\"\n]\n\n/human\n関節リウマチの治療を続けているけど、副作用がつらくて仕事に支障が出ている。薬の効果はあるけれど、もっと副作用の少ない治療法があればいいのに。将来、歩けなくなるんじゃないかと不安もある。\n\n/ai\n[\n  \"治療薬の副作用によって仕事に支障が出ていることへの不満がある\",\n  \"副作用の少ない治療法を求める要望がある\",\n  \"将来歩けなくなるかもしれないという不安がある\"\n]\n\n/human\n2型糖尿病の治療でGLP-1受容体作動薬を使用しているが、注射が嫌で中断する患者が多い。もっと服薬コンプライアンスを高められる経口薬の選択肢が増えてほしい。また、長期的な心血管リスクへの影響がまだ不透明なのも気になっている。\n\n/ai\n[\n  \"GLP-1受容体作動薬が注射であるために患者の治療継続が困難になることへの不満がある\",\n  \"服薬コンプライアンスを高められる経口薬の選択肢を増やしてほしいという要望がある\",\n  \"GLP-1受容体作動薬の長期的な心血管リスクへの影響が不透明であることに対する不安がある\"\n]\n\n/human\n肺がんの治療で免疫チェックポイント阻害薬を使っているけど、効果が出るかどうか分からないまま高額な費用を払い続けるのがつらい。副作用も人によって違うし、もっと分かりやすい説明があれば安心できるのに。\n\n/ai\n[\n  \"免疫チェックポイント阻害薬の効果が不確実なまま高額な治療費を負担していることへの不満がある\",\n  \"副作用の個人差についての説明が不十分であることへの不安がある\",\n  \"治療に関してもっと分かりやすい説明を求める要望がある\"\n]\n\n/human\n非小細胞肺がんに対する免疫療法は確かに有効例もあるが、効果の予測因子が限定的で、どの患者に効くか判断が難しい。もっと精度の高いバイオマーカーが必要だ。\n\n/ai\n[\n  \"免疫療法の効果を予測する因子が限定的であることへの不満がある\",\n  \"免疫療法の適応を見極めるための精度の高いバイオマーカーが必要だという要望がある\"\n]\n\n/human\n家族がうつ病で治療中だけど、薬を飲んでもなかなか改善しない。通院のたびに仕事を休まないといけないのもつらいし、周囲の理解が足りないと感じる。もっと柔軟な支援制度があればいいのに。\n\n/ai\n[\n  \"抗うつ薬を服用しても改善が見られないことへの不満がある\",\n  \"通院に付き添うために仕事を休まなければならない負担への不満がある\",\n  \"精神疾患への社会的理解が不足していることへの不満がある\",\n  \"より柔軟な支援制度を望む要望がある\"\n]\n\n/human\n精神疾患の早期発見は重要だが、現場では予算や人材が足りていない。スクールカウンセラーや企業内メンタルヘルスの体制をもっと充実させる必要がある。\n\n/ai\n[\n  \"精神疾患対策に必要な予算や人材が不足していることへの不満がある\",\n  \"スクールカウンセラーや企業内メンタルヘルスの体制を充実させる必要があるという要望がある\"\n]\n\n/human\n息子がデュシェンヌ型筋ジストロフィーと診断されたけれど、根本的な治療がまだ確立されていないのが苦しい。臨床試験に参加させたいが、近くの病院では扱っていない。もっと地方にも研究機会を広げてほしい。\n\n/ai\n[\n  \"デュシェンヌ型筋ジストロフィーに根本的な治療法が存在しないことへの不満がある\",\n  \"臨床試験に参加する機会が地域によって限られていることへの不満がある\",\n  \"地方にも研究機会を広げてほしいという要望がある\"\n]\n\n/human\nDMDの治療ではステロイドが主流だが、長期使用による副作用が問題になる。新しい分子標的治療が出てきてはいるが、保険適用が進まず現場で使いにくい。\n\n/ai\n[\n  \"DMD治療におけるステロイドの長期使用で副作用が問題になることへの不満がある\",\n  \"新しい治療法が保険適用されず現場で使いにくいことへの不満がある\"\n]",
        "model": "gemini-2.0-flash"
      },
      "hierarchical_clustering": {
        "cluster_nums": [
          3,
          6,
          12,
          24
        ],
        "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import KMeans\n\n\ndef hierarchical_clustering(config):\n    UMAP = import_module(\"umap\").UMAP\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    cluster_nums = config[\"hierarchical_clustering\"][\"cluster_nums\"]\n\n    n_samples = embeddings_array.shape[0]\n    # デフォルト設定は15\n    default_n_neighbors = 15\n\n    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる\n    if n_samples <= default_n_neighbors:\n        n_neighbors = max(2, n_samples - 1)  # 最低2以上\n    else:\n        n_neighbors = default_n_neighbors\n\n    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)\n    # TODO 詳細エラーメッセージを加える\n    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因\n    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.\n    umap_embeds = umap_model.fit_transform(embeddings_array)\n\n    cluster_results = hierarchical_clustering_embeddings(\n        umap_embeds=umap_embeds,\n        cluster_nums=cluster_nums,\n    )\n    result_df = pd.DataFrame(\n        {\n            \"arg-id\": arguments_df[\"arg-id\"],\n            \"argument\": arguments_df[\"argument\"],\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        }\n    )\n\n    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):\n        result_df[f\"cluster-level-{cluster_level}-id\"] = [f\"{cluster_level}_{label}\" for label in final_labels]\n\n    result_df.to_csv(path, index=False)\n\n\n# def generate_cluster_count_list(min_clusters: int, max_clusters: int):\n#     cluster_counts = []\n#     current = min_clusters\n#     cluster_counts.append(current)\n\n#     if min_clusters == max_clusters:\n#         return cluster_counts\n\n#     while True:\n#         next_double = current * 2\n#         next_triple = current * 3\n\n#         if next_double >= max_clusters:\n#             if cluster_counts[-1] != max_clusters:\n#                 cluster_counts.append(max_clusters)\n#             break\n\n#         # 次の倍はまだ max_clusters に収まるが、3倍だと超える\n#         # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ\n#         if next_triple > max_clusters:\n#             cluster_counts.append(max_clusters)\n#             break\n\n#         cluster_counts.append(next_double)\n#         current = next_double\n\n#     return cluster_counts\n\n\ndef merge_clusters_with_hierarchy(\n    cluster_centers: np.ndarray,\n    kmeans_labels: np.ndarray,\n    umap_array: np.ndarray,\n    n_cluster_cut: int,\n):\n    Z = sch.linkage(cluster_centers, method=\"ward\")\n    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion=\"maxclust\")\n\n    n_samples = umap_array.shape[0]\n    final_labels = np.zeros(n_samples, dtype=int)\n\n    for i in range(n_samples):\n        original_label = kmeans_labels[i]\n        final_labels[i] = cluster_labels_merged[original_label]\n\n    return final_labels\n\n\ndef hierarchical_clustering_embeddings(\n    umap_embeds,\n    cluster_nums,\n):\n    # 最大分割数でクラスタリングを実施\n    print(\"start initial clustering\")\n    initial_cluster_num = cluster_nums[-1]\n    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)\n    kmeans_model.fit(umap_embeds)\n    print(\"end initial clustering\")\n\n    results = {}\n    print(\"start hierarchical clustering\")\n    cluster_nums.sort()\n    print(cluster_nums)\n    for n_cluster_cut in cluster_nums[:-1]:\n        print(\"n_cluster_cut: \", n_cluster_cut)\n        final_labels = merge_clusters_with_hierarchy(\n            cluster_centers=kmeans_model.cluster_centers_,\n            kmeans_labels=kmeans_model.labels_,\n            umap_array=umap_embeds,\n            n_cluster_cut=n_cluster_cut,\n        )\n        results[n_cluster_cut] = final_labels\n\n    results[initial_cluster_num] = kmeans_model.labels_\n    print(\"end hierarchical clustering\")\n\n    return results"
      },
      "intro": "このAI生成レポートは、Recursive Publicチームが実施したPolis協議のデータに基づいています。",
      "output_dir": "hierarchical-example-polis",
      "previous": {
        "name": "Recursive Public, Agenda Setting",
        "question": "人類が人工知能を開発・展開する上で、最優先すべき課題は何でしょうか？",
        "input": "example-polis",
        "model": "gpt-4o",
        "extraction": {
          "workers": 3,
          "limit": 200,
          "properties": [],
          "categories": {},
          "category_batch_size": 5,
          "source_code": "import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.category_classification import classify_args\nfrom services.llm import request_to_chat_llm\nfrom services.parse_json_list import parse_response\nfrom hierarchical_utils import update_progress # 前まではbroadlistening.utilsから呼び出していた。これでエラーになったらもとに戻す。\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\ndef _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:\n    if not all(property in comments.columns for property in property_columns):\n        raise ValueError(f\"Properties {property_columns} not found in comments. Columns are {comments.columns}\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n    property_columns = config[\"extraction\"][\"properties\"]\n\n    # カラム名だけを読み込み、必要なカラムが含まれているか確認する\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\", nrows=0)\n    _validate_property_columns(property_columns, comments)\n    # エラーが出なかった場合、すべての行を読み込む\n    comments = pd.read_csv(\n        f\"inputs/{config['input']}.csv\", usecols=[\"comment-id\", \"comment-body\"] + config[\"extraction\"][\"properties\"]\n    )\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    argument_map = {}\n    relation_rows = []\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n\n        for comment_id, extracted_args in zip(batch, batch_results, strict=False):\n            for j, arg in enumerate(extracted_args):\n                if arg not in argument_map:\n                    # argumentテーブルに追加\n                    arg_id = f\"A{comment_id}_{j}\"\n                    argument_map[arg] = {\n                        \"arg-id\": arg_id,\n                        \"argument\": arg,\n                    }\n                else:\n                    arg_id = argument_map[arg][\"arg-id\"]\n\n                # relationテーブルにcommentとargの関係を追加\n                relation_row = {\n                    \"arg-id\": arg_id,\n                    \"comment-id\": comment_id,\n                }\n                relation_rows.append(relation_row)\n\n        update_progress(config, incr=len(batch))\n\n    # DataFrame化\n    results = pd.DataFrame(argument_map.values())\n    relation_df = pd.DataFrame(relation_rows)\n\n    if results.empty:\n        raise RuntimeError(\"result is empty, maybe bad prompt\")\n\n    classification_categories = config[\"extraction\"][\"categories\"]\n    if classification_categories:\n        results = classify_args(results, config, workers)\n\n    results.to_csv(path, index=False)\n    # comment-idとarg-idの関係を保存\n    relation_df.to_csv(f\"outputs/{dataset}/relations.csv\", index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures_with_index = [\n            (i, executor.submit(extract_arguments, input, prompt, model)) for i, input in enumerate(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)\n        results = [[] for _ in range(len(batch))]\n\n        for _, future in futures_with_index:\n            if future in not_done and not future.cancelled():\n                future.cancel()\n\n        for i, future in futures_with_index:\n            if future in done:\n                try:\n                    result = future.result()\n                    results[i] = result\n                except Exception as e:\n                    logging.error(f\"Task {future} failed with error: {e}\")\n                    results[i] = []\n        return results\n\n\n# def extract_by_llm(input, prompt, model):\n#     messages = [\n#         {\"role\": \"system\", \"content\": prompt},\n#         {\"role\": \"user\", \"content\": input},\n#     ]\n#     response = request_to_chat_llm(messages=messages, model=model)\n#     return response\n\n\ndef extract_arguments(input, prompt, model, retries=1):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_llm(messages=messages, model=model, is_json=False)\n        items = parse_response(response)\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []",
          "prompt": "# server/broadlistening/pipeline/prompts/extraction/default.txt の修正\n\n/system\nあなたは専門的なリサーチアシスタントで、与えられたテキストから「要望」「不満」「不安」に関連する意見を抽出する役割です。\n抽出した意見は、それぞれ独立した簡潔な文にしてください。意見が複数ある場合は、すべて抽出してください。\n結果は整形されたJSON形式の文字列リストとして返してください。\n抽出する意見は必ず日本語で作成してください。\n\n/human\nAI技術の進化は目覚ましいが、雇用が奪われるのではないかと心配だし、もっと透明性を高めてほしい。規制も必要だと思う。\n\n/ai\n[\n  \"AIによって雇用が奪われるのではないかという不安がある\",\n  \"AI技術の透明性を高めてほしいという要望がある\",\n  \"AI技術に対する規制が必要だという意見がある\"\n]\n\n/human\n特に問題点は感じていない。現状維持で良い。\n\n/ai\n[]\n\n/human\n新しい公園の計画は素晴らしいが、アクセス道路が狭いのが気になる。子供たちの安全は大丈夫だろうか。もっと広い道路を整備してほしい。\n\n/ai\n[\n  \"新しい公園へのアクセス道路が狭いことへの不満がある\",\n  \"アクセス道路での子供たちの安全に対する不安がある\",\n  \"より広いアクセス道路を整備してほしいという要望がある\"\n]\n\n/human\nサポート体制には満足しているが、もう少し迅速に対応してもらえると助かる。\n\n/ai\n[\n  \"サポートの対応速度をもう少し迅速にしてほしいという要望がある\"\n]\n\n/human\n関節リウマチの治療を続けているけど、副作用がつらくて仕事に支障が出ている。薬の効果はあるけれど、もっと副作用の少ない治療法があればいいのに。将来、歩けなくなるんじゃないかと不安もある。\n\n/ai\n[\n  \"治療薬の副作用によって仕事に支障が出ていることへの不満がある\",\n  \"副作用の少ない治療法を求める要望がある\",\n  \"将来歩けなくなるかもしれないという不安がある\"\n]\n\n/human\n2型糖尿病の治療でGLP-1受容体作動薬を使用しているが、注射が嫌で中断する患者が多い。もっと服薬コンプライアンスを高められる経口薬の選択肢が増えてほしい。また、長期的な心血管リスクへの影響がまだ不透明なのも気になっている。\n\n/ai\n[\n  \"GLP-1受容体作動薬が注射であるために患者の治療継続が困難になることへの不満がある\",\n  \"服薬コンプライアンスを高められる経口薬の選択肢を増やしてほしいという要望がある\",\n  \"GLP-1受容体作動薬の長期的な心血管リスクへの影響が不透明であることに対する不安がある\"\n]\n\n/human\n肺がんの治療で免疫チェックポイント阻害薬を使っているけど、効果が出るかどうか分からないまま高額な費用を払い続けるのがつらい。副作用も人によって違うし、もっと分かりやすい説明があれば安心できるのに。\n\n/ai\n[\n  \"免疫チェックポイント阻害薬の効果が不確実なまま高額な治療費を負担していることへの不満がある\",\n  \"副作用の個人差についての説明が不十分であることへの不安がある\",\n  \"治療に関してもっと分かりやすい説明を求める要望がある\"\n]\n\n/human\n非小細胞肺がんに対する免疫療法は確かに有効例もあるが、効果の予測因子が限定的で、どの患者に効くか判断が難しい。もっと精度の高いバイオマーカーが必要だ。\n\n/ai\n[\n  \"免疫療法の効果を予測する因子が限定的であることへの不満がある\",\n  \"免疫療法の適応を見極めるための精度の高いバイオマーカーが必要だという要望がある\"\n]\n\n/human\n家族がうつ病で治療中だけど、薬を飲んでもなかなか改善しない。通院のたびに仕事を休まないといけないのもつらいし、周囲の理解が足りないと感じる。もっと柔軟な支援制度があればいいのに。\n\n/ai\n[\n  \"抗うつ薬を服用しても改善が見られないことへの不満がある\",\n  \"通院に付き添うために仕事を休まなければならない負担への不満がある\",\n  \"精神疾患への社会的理解が不足していることへの不満がある\",\n  \"より柔軟な支援制度を望む要望がある\"\n]\n\n/human\n精神疾患の早期発見は重要だが、現場では予算や人材が足りていない。スクールカウンセラーや企業内メンタルヘルスの体制をもっと充実させる必要がある。\n\n/ai\n[\n  \"精神疾患対策に必要な予算や人材が不足していることへの不満がある\",\n  \"スクールカウンセラーや企業内メンタルヘルスの体制を充実させる必要があるという要望がある\"\n]\n\n/human\n息子がデュシェンヌ型筋ジストロフィーと診断されたけれど、根本的な治療がまだ確立されていないのが苦しい。臨床試験に参加させたいが、近くの病院では扱っていない。もっと地方にも研究機会を広げてほしい。\n\n/ai\n[\n  \"デュシェンヌ型筋ジストロフィーに根本的な治療法が存在しないことへの不満がある\",\n  \"臨床試験に参加する機会が地域によって限られていることへの不満がある\",\n  \"地方にも研究機会を広げてほしいという要望がある\"\n]\n\n/human\nDMDの治療ではステロイドが主流だが、長期使用による副作用が問題になる。新しい分子標的治療が出てきてはいるが、保険適用が進まず現場で使いにくい。\n\n/ai\n[\n  \"DMD治療におけるステロイドの長期使用で副作用が問題になることへの不満がある\",\n  \"新しい治療法が保険適用されず現場で使いにくいことへの不満がある\"\n]",
          "model": "gpt-4o"
        },
        "hierarchical_clustering": {
          "cluster_nums": [
            3,
            6,
            12,
            24
          ],
          "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import KMeans\n\n\ndef hierarchical_clustering(config):\n    UMAP = import_module(\"umap\").UMAP\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    cluster_nums = config[\"hierarchical_clustering\"][\"cluster_nums\"]\n\n    n_samples = embeddings_array.shape[0]\n    # デフォルト設定は15\n    default_n_neighbors = 15\n\n    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる\n    if n_samples <= default_n_neighbors:\n        n_neighbors = max(2, n_samples - 1)  # 最低2以上\n    else:\n        n_neighbors = default_n_neighbors\n\n    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)\n    # TODO 詳細エラーメッセージを加える\n    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因\n    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.\n    umap_embeds = umap_model.fit_transform(embeddings_array)\n\n    cluster_results = hierarchical_clustering_embeddings(\n        umap_embeds=umap_embeds,\n        cluster_nums=cluster_nums,\n    )\n    result_df = pd.DataFrame(\n        {\n            \"arg-id\": arguments_df[\"arg-id\"],\n            \"argument\": arguments_df[\"argument\"],\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        }\n    )\n\n    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):\n        result_df[f\"cluster-level-{cluster_level}-id\"] = [f\"{cluster_level}_{label}\" for label in final_labels]\n\n    result_df.to_csv(path, index=False)\n\n\n# def generate_cluster_count_list(min_clusters: int, max_clusters: int):\n#     cluster_counts = []\n#     current = min_clusters\n#     cluster_counts.append(current)\n\n#     if min_clusters == max_clusters:\n#         return cluster_counts\n\n#     while True:\n#         next_double = current * 2\n#         next_triple = current * 3\n\n#         if next_double >= max_clusters:\n#             if cluster_counts[-1] != max_clusters:\n#                 cluster_counts.append(max_clusters)\n#             break\n\n#         # 次の倍はまだ max_clusters に収まるが、3倍だと超える\n#         # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ\n#         if next_triple > max_clusters:\n#             cluster_counts.append(max_clusters)\n#             break\n\n#         cluster_counts.append(next_double)\n#         current = next_double\n\n#     return cluster_counts\n\n\ndef merge_clusters_with_hierarchy(\n    cluster_centers: np.ndarray,\n    kmeans_labels: np.ndarray,\n    umap_array: np.ndarray,\n    n_cluster_cut: int,\n):\n    Z = sch.linkage(cluster_centers, method=\"ward\")\n    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion=\"maxclust\")\n\n    n_samples = umap_array.shape[0]\n    final_labels = np.zeros(n_samples, dtype=int)\n\n    for i in range(n_samples):\n        original_label = kmeans_labels[i]\n        final_labels[i] = cluster_labels_merged[original_label]\n\n    return final_labels\n\n\ndef hierarchical_clustering_embeddings(\n    umap_embeds,\n    cluster_nums,\n):\n    # 最大分割数でクラスタリングを実施\n    print(\"start initial clustering\")\n    initial_cluster_num = cluster_nums[-1]\n    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)\n    kmeans_model.fit(umap_embeds)\n    print(\"end initial clustering\")\n\n    results = {}\n    print(\"start hierarchical clustering\")\n    cluster_nums.sort()\n    print(cluster_nums)\n    for n_cluster_cut in cluster_nums[:-1]:\n        print(\"n_cluster_cut: \", n_cluster_cut)\n        final_labels = merge_clusters_with_hierarchy(\n            cluster_centers=kmeans_model.cluster_centers_,\n            kmeans_labels=kmeans_model.labels_,\n            umap_array=umap_embeds,\n            n_cluster_cut=n_cluster_cut,\n        )\n        results[n_cluster_cut] = final_labels\n\n    results[initial_cluster_num] = kmeans_model.labels_\n    print(\"end hierarchical clustering\")\n\n    return results"
        },
        "intro": "このAI生成レポートは、Recursive Publicチームが実施したPolis協議のデータに基づいています。",
        "output_dir": "hierarchical-example-polis",
        "is_pubcom": true,
        "embedding": {
          "model": "text-embedding-3-small",
          "source_code": "import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model)\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)"
        },
        "hierarchical_initial_labelling": {
          "sampling_num": 3,
          "workers": 1,
          "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nfrom typing import TypedDict\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_llm\n\n\nclass LabellingResult(TypedDict):\n    \"\"\"各クラスタのラベリング結果を表す型\"\"\"\n\n    cluster_id: str  # クラスタのID\n    label: str  # クラスタのラベル名\n    description: str  # クラスタの説明文\n\n\ndef hierarchical_initial_labelling(config: dict) -> None:\n    \"\"\"階層的クラスタリングの初期ラベリングを実行する\n\n    Args:\n        config: 設定情報を含む辞書\n            - output_dir: 出力ディレクトリ名\n            - hierarchical_initial_labelling: 初期ラベリングの設定\n                - sampling_num: サンプリング数\n                - prompt: LLMへのプロンプト\n                - model: 使用するLLMモデル名\n                - workers: 並列処理のワーカー数\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_initial_labels.csv\"\n    clusters_argument_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_clusters.csv\")\n\n    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_id_column = cluster_id_columns[-1]\n    sampling_num = config[\"hierarchical_initial_labelling\"][\"sampling_num\"]\n    initial_labelling_prompt = config[\"hierarchical_initial_labelling\"][\"prompt\"]\n    model = config[\"hierarchical_initial_labelling\"][\"model\"]\n    workers = config[\"hierarchical_initial_labelling\"][\"workers\"]\n\n    initial_label_df = initial_labelling(\n        initial_labelling_prompt,\n        clusters_argument_df,\n        sampling_num,\n        model,\n        workers,\n    )\n    print(\"start initial labelling\")\n    initial_clusters_argument_df = clusters_argument_df.merge(\n        initial_label_df,\n        left_on=initial_cluster_id_column,\n        right_on=\"cluster_id\",\n        how=\"left\",\n    ).rename(\n        columns={\n            \"label\": f\"{initial_cluster_id_column.replace('-id', '')}-label\",\n            \"description\": f\"{initial_cluster_id_column.replace('-id', '')}-description\",\n        }\n    )\n    print(\"end initial labelling\")\n    initial_clusters_argument_df.to_csv(path, index=False)\n\n\ndef initial_labelling(\n    prompt: str,\n    clusters_df: pd.DataFrame,\n    sampling_num: int,\n    model: str,\n    workers: int,\n) -> pd.DataFrame:\n    \"\"\"各クラスタに対して初期ラベリングを実行する\n\n    Args:\n        prompt: LLMへのプロンプト\n        clusters_df: クラスタリング結果のDataFrame\n        sampling_num: 各クラスタからサンプリングする意見の数\n        model: 使用するLLMモデル名\n        workers: 並列処理のワーカー数\n\n    Returns:\n        各クラスタのラベリング結果を含むDataFrame\n    \"\"\"\n    cluster_columns = [col for col in clusters_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_column = cluster_columns[-1]\n    cluster_ids = clusters_df[initial_cluster_column].unique()\n    process_func = partial(\n        process_initial_labelling,\n        df=clusters_df,\n        prompt=prompt,\n        sampling_num=sampling_num,\n        target_column=initial_cluster_column,\n        model=model,\n    )\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        results = list(executor.map(process_func, cluster_ids))\n    return pd.DataFrame(results)\n\n\ndef process_initial_labelling(\n    cluster_id: str,\n    df: pd.DataFrame,\n    prompt: str,\n    sampling_num: int,\n    target_column: str,\n    model: str,\n) -> LabellingResult:\n    \"\"\"個別のクラスタに対してラベリングを実行する\n\n    Args:\n        cluster_id: 処理対象のクラスタID\n        df: クラスタリング結果のDataFrame\n        prompt: LLMへのプロンプト\n        sampling_num: サンプリングする意見の数\n        target_column: クラスタIDが格納されている列名\n        model: 使用するLLMモデル名\n\n    Returns:\n        クラスタのラベリング結果\n    \"\"\"\n    cluster_data = df[df[target_column] == cluster_id]\n    sampling_num = min(sampling_num, len(cluster_data))\n    cluster = cluster_data.sample(sampling_num)\n    input = \"\\n\".join(cluster[\"argument\"].values)\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_llm(messages=messages, model=model, is_json=True)\n        response_json = json.loads(response)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=response_json.get(\"label\", \"エラーでラベル名が取得できませんでした\"),\n            description=response_json.get(\"description\", \"エラーで解説が取得できませんでした\"),\n        )\n    except Exception as e:\n        print(e)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=\"エラーでラベル名が取得できませんでした\",\n            description=\"エラーで解説が取得できませんでした\",\n        )",
          "prompt": "あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説して、それから表札をつけてください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例1\n最近、政治家が能登の復興に向けた具体的なプランを発表し、地域の未来に明るい希望が見えてきました。市民として、真摯な取り組みに感謝しています。\n災害復興支援が、選挙期間中にしっかり議論されるようになり、政治家が国民の本当のニーズに応える姿勢に期待しています。\n選挙を通じて、政治家が地域振興に全力で取り組む姿勢が伝わってきます。具体的な政策提案を目にするたび、未来への希望が膨らみます。\n\n\n## 出力例1\n{{\n    \"label\": \"市民の未来を支える具体的政策への期待\",\n    \"description\": \"このクラスタには、地域の復興や被害者支援など、実際の社会課題に対して政治家が具体的かつ積極的に取り組む姿勢を支持する前向きな意見が集まっています。市民は、選挙や政策議論を通じて、現実の問題に即した支援策や復興計画が実現されることを期待し、明るい未来の構築に向けた政治の変革を応援しています。\"\n}}\n\n\n## 入力例2\nこの病気は原因がはっきりしないため、治療法も確立されておらず、将来がとても不安です。\n医師の説明が専門的すぎて、自分の状態が正確に理解できませんでした。\n現在の治療法には副作用が多く、生活の質が大きく下がっています。もっと副作用の少ない選択肢が欲しいです。\n同じ病気の患者同士で情報交換できる場がもっとあれば、心の支えになるのにと思います。\n検査の予約が何ヶ月も先で、早期発見や早期治療ができない状況に不満があります。\n\n\n## 出力例2\n{{\n    \"label\": \"疾患への不安と治療環境への改善要望\",\n    \"description\": \"このクラスタには、病気の原因や治療法が明確でないことに起因する将来への不安や、治療中の副作用、説明不足といった医療提供側への不満が集まっています。また、患者同士の交流の場や医療アクセスの改善など、心身両面に配慮した医療環境の整備を求める要望も含まれており、患者視点から見た『納得感のある医療体験』への強いニーズが表れています。\"\n}}",
          "model": "gpt-4o"
        },
        "hierarchical_merge_labelling": {
          "sampling_num": 3,
          "workers": 1,
          "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_llm\n\n\n@dataclass\nclass ClusterColumns:\n    \"\"\"同一階層のクラスター関連のカラム名を管理するクラス\"\"\"\n\n    id: str\n    label: str\n    description: str\n\n    @classmethod\n    def from_id_column(cls, id_column: str) -> \"ClusterColumns\":\n        \"\"\"ID列名から関連するカラム名を生成\"\"\"\n        return cls(\n            id=id_column,\n            label=id_column.replace(\"-id\", \"-label\"),\n            description=id_column.replace(\"-id\", \"-description\"),\n        )\n\n\n@dataclass\nclass ClusterValues:\n    \"\"\"対象クラスタのlabel/descriptionを管理するクラス\"\"\"\n\n    label: str\n    description: str\n\n    def to_prompt_text(self) -> str:\n        return f\"- {self.label}: {self.description}\"\n\n\ndef hierarchical_merge_labelling(config: dict) -> None:\n    \"\"\"階層的クラスタリングの結果に対してマージラベリングを実行する\n\n    Args:\n        config: 設定情報を含む辞書\n            - output_dir: 出力ディレクトリ名\n            - hierarchical_merge_labelling: マージラベリングの設定\n                - sampling_num: サンプリング数\n                - prompt: LLMへのプロンプト\n                - model: 使用するLLMモデル名\n                - workers: 並列処理のワーカー数\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    merge_path = f\"outputs/{dataset}/hierarchical_merge_labels.csv\"\n    clusters_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_initial_labels.csv\")\n\n    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)\n    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成\n    merge_result_df = merge_labelling(\n        clusters_df=clusters_df,\n        cluster_id_columns=sorted(cluster_id_columns, reverse=True),\n        config=config,\n    )\n    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成\n    melted_df = melt_cluster_data(merge_result_df)\n    # 上記のdfに親子関係を追加\n    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)\n    melted_df = melted_df.merge(parent_child_df, on=[\"level\", \"id\"], how=\"left\")\n    density_df = calculate_cluster_density(melted_df, config)\n    density_df.to_csv(merge_path, index=False)\n\n\ndef _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):\n    \"\"\"クラスタ間の親子関係をマッピングする\n\n    Args:\n        df: クラスタリング結果のDataFrame\n        cluster_id_columns: クラスタIDのカラム名のリスト\n\n    Returns:\n        親子関係のマッピング情報を含むDataFrame\n    \"\"\"\n    results = []\n    top_cluster_column = cluster_id_columns[0]\n    top_cluster_values = df[top_cluster_column].unique()\n    for c in top_cluster_values:\n        results.append(\n            {\n                \"level\": 1,\n                \"id\": c,\n                \"parent\": \"0\",  # aggregationで追加する全体クラスタのid\n            }\n        )\n\n    for idx in range(len(cluster_id_columns) - 1):\n        current_column = cluster_id_columns[idx]\n        children_column = cluster_id_columns[idx + 1]\n        current_level = current_column.replace(\"-id\", \"\").replace(\"cluster-level-\", \"\")\n        # 現在のレベルのクラスタid\n        current_cluster_values = df[current_column].unique()\n        for current_id in current_cluster_values:\n            children_ids = df.loc[df[current_column] == current_id, children_column].unique()\n            for child_id in children_ids:\n                results.append(\n                    {\n                        \"level\": int(current_level) + 1,\n                        \"id\": child_id,\n                        \"parent\": current_id,\n                    }\n                )\n    return pd.DataFrame(results)\n\n\ndef _filter_id_columns(columns: list[str]) -> list[str]:\n    \"\"\"クラスタIDのカラム名をフィルタリングする\n\n    Args:\n        columns: 全カラム名のリスト\n\n    Returns:\n        クラスタIDのカラム名のリスト\n    \"\"\"\n    return [col for col in columns if col.startswith(\"cluster-level-\") and col.endswith(\"-id\")]\n\n\ndef melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"クラスタデータを行形式に変換する\n\n    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。\n    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。\n\n    Args:\n        df: クラスタリング結果のDataFrame\n\n    Returns:\n        行形式に変換されたDataFrame\n    \"\"\"\n    id_columns: list[str] = _filter_id_columns(df.columns)\n    levels: set[int] = {int(col.replace(\"cluster-level-\", \"\").replace(\"-id\", \"\")) for col in id_columns}\n    all_rows: list[dict] = []\n\n    # levelごとに各クラスタの出現件数を集計・縦持ちにする\n    for level in levels:\n        cluster_columns = ClusterColumns.from_id_column(f\"cluster-level-{level}-id\")\n        # クラスタidごとの件数集計\n        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name=\"value\")\n\n        level_unique_val_df = df[\n            [cluster_columns.id, cluster_columns.label, cluster_columns.description]\n        ].drop_duplicates()\n        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how=\"left\")\n        level_unique_vals = [\n            {\n                \"level\": level,\n                \"id\": row[cluster_columns.id],\n                \"label\": row[cluster_columns.label],\n                \"description\": row[cluster_columns.description],\n                \"value\": row[\"value\"],\n            }\n            for _, row in level_unique_val_df.iterrows()\n        ]\n        all_rows.extend(level_unique_vals)\n    return pd.DataFrame(all_rows)\n\n\ndef merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:\n    \"\"\"階層的なクラスタのマージラベリングを実行する\n\n    Args:\n        clusters_df: クラスタリング結果のDataFrame\n        cluster_id_columns: クラスタIDのカラム名のリスト\n        config: 設定情報を含む辞書\n\n    Returns:\n        マージラベリング結果を含むDataFrame\n    \"\"\"\n    for idx in tqdm(range(len(cluster_id_columns) - 1)):\n        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])\n        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])\n\n        process_fn = partial(\n            process_merge_labelling,\n            result_df=clusters_df,\n            current_columns=current_columns,\n            previous_columns=previous_columns,\n            config=config,\n        )\n\n        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())\n        with ThreadPoolExecutor(max_workers=config[\"hierarchical_merge_labelling\"][\"workers\"]) as executor:\n            responses = list(\n                tqdm(\n                    executor.map(process_fn, current_cluster_ids),\n                    total=len(current_cluster_ids),\n                )\n            )\n\n        current_result_df = pd.DataFrame(responses)\n        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])\n    return clusters_df\n\n\ndef process_merge_labelling(\n    target_cluster_id: str,\n    result_df: pd.DataFrame,\n    current_columns: ClusterColumns,\n    previous_columns: ClusterColumns,\n    config,\n):\n    \"\"\"個別のクラスタに対してマージラベリングを実行する\n\n    Args:\n        target_cluster_id: 処理対象のクラスタID\n        result_df: クラスタリング結果のDataFrame\n        current_columns: 現在のレベルのカラム情報\n        previous_columns: 前のレベルのカラム情報\n        config: 設定情報を含む辞書\n\n    Returns:\n        マージラベリング結果を含む辞書\n    \"\"\"\n\n    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:\n        \"\"\"前のレベルのクラスタ情報を取得する\"\"\"\n        previous_records = df[df[current_columns.id] == target_cluster_id][\n            [previous_columns.label, previous_columns.description]\n        ].drop_duplicates()\n        previous_values = [\n            ClusterValues(\n                label=row[previous_columns.label],\n                description=row[previous_columns.description],\n            )\n            for _, row in previous_records.iterrows()\n        ]\n        return previous_values\n\n    previous_values = filter_previous_values(result_df, previous_columns)\n    if len(previous_values) == 1:\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: previous_values[0].label,\n            current_columns.description: previous_values[0].description,\n        }\n    elif len(previous_values) == 0:\n        raise ValueError(f\"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。\")\n\n    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]\n    sampling_num = min(\n        config[\"hierarchical_merge_labelling\"][\"sampling_num\"],\n        len(current_cluster_data),\n    )\n    sampled_data = current_cluster_data.sample(sampling_num)\n    sampled_argument_text = \"\\n\".join(sampled_data[\"argument\"].values)\n    cluster_text = \"\\n\".join([value.to_prompt_text() for value in previous_values])\n    messages = [\n        {\"role\": \"system\", \"content\": config[\"hierarchical_merge_labelling\"][\"prompt\"]},\n        {\n            \"role\": \"user\",\n            \"content\": \"クラスタラベル\\n\" + cluster_text + \"\\n\" + \"クラスタの意見\\n\" + sampled_argument_text,\n        },\n    ]\n    try:\n        response = request_to_chat_llm(\n            messages=messages,\n            model=config[\"hierarchical_merge_labelling\"][\"model\"],\n            is_json=True,\n        )\n        response_json = json.loads(response)\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: response_json.get(\"label\", \"エラーでラベル名が取得できませんでした\"),\n            current_columns.description: response_json.get(\"description\", \"エラーで解説が取得できませんでした\"),\n        }\n    except Exception as e:\n        print(f\"エラーが発生しました: {e}\")\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: \"エラーでラベル名が取得できませんでした\",\n            current_columns.description: \"エラーで解説が取得できませんでした\",\n        }\n\n\ndef calculate_cluster_density(melted_df: pd.DataFrame, config: dict):\n    \"\"\"クラスタ内の密度計算\"\"\"\n    hierarchical_cluster_df = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n\n    densities = []\n    for level, c_id in zip(melted_df[\"level\"], melted_df[\"id\"], strict=False):\n        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f\"cluster-level-{level}-id\"] == c_id][\n            [\"x\", \"y\"]\n        ].values\n        density = calculate_density(cluster_embeds)\n        densities.append(density)\n\n    # 密度のランクを計算\n    melted_df[\"density\"] = densities\n    melted_df[\"density_rank\"] = melted_df.groupby(\"level\")[\"density\"].rank(ascending=False, method=\"first\")\n    melted_df[\"density_rank_percentile\"] = melted_df.groupby(\"level\")[\"density_rank\"].transform(lambda x: x / len(x))\n    return melted_df\n\n\ndef calculate_density(embeds: np.ndarray):\n    \"\"\"平均距離に基づいて密度を計算\"\"\"\n    center = np.mean(embeds, axis=0)\n    distances = np.linalg.norm(embeds - center, axis=1)\n    avg_distance = np.mean(distances)\n    density = 1 / (avg_distance + 1e-10)\n    return density",
          "prompt": "分割されすぎたクラスタを統合する必要があるので、統合後の名称を考えて出力して。\n\n# 指示\n* 統合前のクラスタの名称・説明および統合後のクラスタに属するデータ点のサンプルを与えるので、これらに基づいて統合後のクラスタの名称を出力してください\n    * 統合後のクラスタ名において、統合前のクラスタ名をそのまま使うことは避けてください。\n* 出力例に記載したJSONのフォーマットに従って出力してください\n\n# サンプルの入出力\n## 入力例1（クラスタラベル:説明文）\n- 地域の災害対応への批判: このクラスタは、地域における災害対応策の実施や体制に対する批判的な意見を集約したものです。住民からは、迅速かつ効果的な支援が行われていない点や、情報提供・連携の不足などに対する強い不満が表明されています。\n- 災害対応への不満: このクラスタは、災害発生時の対応全般に対する不満を示す意見をまとめたものです。救援活動の遅れや支援策の実効性に疑問を持つ声が多く、より積極的で透明性のある対応を求める意見が特徴です。\n- 地域復興の遅れ: このクラスタは、災害後の地域復興プロセスが予定通りに進んでいない点に対する懸念や不満を反映しています。再建計画や支援策の実施の遅延、そしてそれに伴う住民の生活再建への影響が強調されています。\n\n\n## 出力例1\n{{\n    \"label\": \"地域再生と災害支援に対する期待と懸念\",\n    \"description\": \"このクラスタは、特定の地域における再生や災害支援策に対し、具体的な取り組みが不足しているとの意見を集約しています。市民は、選挙を通じた政策議論の中で、地域復興や被災者支援を最優先すべきだとの期待と、現行の支援策に対する改善要求を強く表明しており、より効果的な政府の対応を求める声が反映されています。\"\n}}\n\n\n## 入力例2（クラスタラベル:説明文）\n- 新薬の承認遅延に対する不満:このクラスタは、新薬の承認プロセスが煩雑かつ時間がかかりすぎるという不満の声をまとめたものです。特に、重篤な疾患に苦しむ患者やその家族からは、迅速なアクセスの欠如に対する苛立ちが表明されています。\n- 未治療疾患に対する対応不足:このクラスタは、現時点で有効な治療法が存在しない疾患や、研究開発の進展が遅れている分野に関する課題を指摘する意見を集めています。患者からは、自分たちのニーズが無視されているという感覚が共有されています。\n- 希少疾患への医療アクセスの不平等:このクラスタは、希少疾患を抱える患者が十分な医療資源や治療機会を得られないことに対する不満を中心としています。医療制度の網からこぼれ落ちているという訴えが多く寄せられています。\n\n## 出力例2\n{{\n  \"label\": \"治療機会の格差と革新的医療へのアクセス課題\",\n  \"description\": \"このクラスタは、重篤または希少な疾患に対して、十分な治療法が提供されていない状況や、医療制度における承認プロセスの遅延、アクセス格差に対する不満を集約しています。患者やその家族は、既存の制度が急を要する医療ニーズに対応できていないことに強い懸念を示しており、より迅速かつ公平な医療提供体制の確立を求めています。\"\n}}",
          "model": "gpt-4o"
        },
        "hierarchical_overview": {
          "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_llm\n\n\ndef hierarchical_overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_overview.txt\"\n\n    hierarchical_label_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_merge_labels.csv\")\n\n    prompt = config[\"hierarchical_overview\"][\"prompt\"]\n    model = config[\"hierarchical_overview\"][\"model\"]\n\n    # TODO: level1で固定にしているが、設定で変えられるようにする\n    target_level = 1\n    target_records = hierarchical_label_df[hierarchical_label_df[\"level\"] == target_level]\n    ids = target_records[\"id\"].to_list()\n    labels = target_records[\"label\"].to_list()\n    descriptions = target_records[\"description\"].to_list()\n    target_records.set_index(\"id\", inplace=True)\n\n    input = \"\"\n    for i, _ in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels[i]}\\n\\n\"\n        input += descriptions[i] + \"\\n\\n\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_llm(messages=messages, model=model)\n\n    with open(path, \"w\", encoding='utf-8') as file:\n        file.write(response)",
          "prompt": "/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。",
          "model": "gpt-4o"
        },
        "hierarchical_aggregation": {
          "sampling_num": 5000,
          "hidden_properties": {},
          "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import TypedDict\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\n\n\nclass Argument(TypedDict):\n    arg_id: str\n    argument: str\n    comment_id: str\n    x: float\n    y: float\n    p: float\n    cluster_ids: list[str]\n\n\nclass Cluster(TypedDict):\n    level: int\n    id: str\n    label: str\n    takeaway: str\n    value: int\n    parent: str\n    density_rank_percentile: float | None\n\n\ndef hierarchical_aggregation(config):\n    path = f\"outputs/{config['output_dir']}/hierarchical_result.json\"\n    results = {\n        \"arguments\": [],\n        \"clusters\": [],\n        \"comments\": {},\n        \"propertyMap\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index(\"arg-id\", inplace=True)\n    arg_num = len(arguments)\n    relation_df = pd.read_csv(f\"outputs/{config['output_dir']}/relations.csv\")\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_merge_labels.csv\")\n\n    hidden_properties_map: dict[str, list[str]] = config[\"hierarchical_aggregation\"][\"hidden_properties\"]\n\n    results[\"arguments\"] = _build_arguments(clusters)\n    results[\"clusters\"] = _build_cluster_value(labels, arg_num)\n    # NOTE: 属性に応じたコメントフィルタ機能が実装されておらず、全てのコメントが含まれてしまうので、コメントアウト\n    # results[\"comments\"] = _build_comments_value(\n    #     comments, arguments, hidden_properties_map\n    # )\n    results[\"comment_num\"] = len(comments)\n    results[\"translations\"] = _build_translations(config)\n    # 属性情報のカラムは、元データに対して指定したカラムとclassificationするカテゴリを合わせたもの\n    results[\"propertyMap\"] = _build_property_map(arguments, hidden_properties_map, config)\n\n    with open(f\"outputs/{config['output_dir']}/hierarchical_overview.txt\", encoding='utf-8') as f:\n        overview = f.read()\n    print(\"overview\")\n    print(overview)\n    results[\"overview\"] = overview\n\n    with open(path, \"w\", encoding='utf-8') as file:\n        json.dump(results, file, indent=2, ensure_ascii=False)\n    # TODO: サンプリングロジックを実装したいが、現状は全件抽出\n    create_custom_intro(config)\n    if config[\"is_pubcom\"]:\n        add_original_comments(labels, arguments, relation_df, clusters, config)\n\n\ndef create_custom_intro(config):\n    dataset = config[\"output_dir\"]\n    args_path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    result_path = f\"outputs/{dataset}/hierarchical_result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n    processed_num = min(input_count, config[\"extraction\"][\"limit\"])\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n分析対象となったデータの件数は{processed_num}件で、これらのデータに対してOpenAI APIを用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(intro=intro, processed_num=processed_num, args_count=args_count)\n\n    with open(result_path, encoding='utf-8') as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\", encoding='utf-8') as f:\n        json.dump(result, f, indent=2, ensure_ascii=False)\n\n\ndef add_original_comments(labels, arguments, relation_df, clusters, config):\n    # 大カテゴリ（cluster-level-1）に該当するラベルだけ抽出\n    labels_lv1 = labels[labels[\"level\"] == 1][[\"id\", \"label\"]].rename(\n        columns={\"id\": \"cluster-level-1-id\", \"label\": \"category_label\"}\n    )\n\n    # arguments と clusters をマージ（カテゴリ情報付与）\n    merged = arguments.merge(clusters[[\"arg-id\", \"cluster-level-1-id\"]], on=\"arg-id\").merge(\n        labels_lv1, on=\"cluster-level-1-id\", how=\"left\"\n    )\n\n    # relation_df と結合\n    merged = merged.merge(relation_df, on=\"arg-id\", how=\"left\")\n\n    # 元コメント取得\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    comments[\"comment-id\"] = comments[\"comment-id\"].astype(str)\n    merged[\"comment-id\"] = merged[\"comment-id\"].astype(str)\n\n    # 元コメント本文などとマージ\n    final_df = merged.merge(comments, on=\"comment-id\", how=\"left\")\n\n    # 必要カラムのみ整形\n    final_cols = [\"comment-id\", \"comment-body\", \"arg-id\", \"argument\", \"cluster-level-1-id\", \"category_label\"]\n    for col in [\"source\", \"url\"]:\n        if col in comments.columns:\n            final_cols.append(col)\n\n    final_df = final_df[final_cols]\n    final_df = final_df.rename(\n        columns={\n            \"cluster-level-1-id\": \"category_id\",\n            \"category_label\": \"category\",\n            \"arg-id\": \"arg_id\",\n            \"argument\": \"argument\",\n            \"comment-body\": \"original-comment\",\n        }\n    )\n\n    # 保存\n    final_df.to_csv(f\"outputs/{config['output_dir']}/final_result_with_comments.csv\", index=False, encoding='utf-8-sig')\n\n\ndef _build_arguments(clusters: pd.DataFrame) -> list[Argument]:\n    cluster_columns = [col for col in clusters.columns if col.startswith(\"cluster-level-\") and \"id\" in col]\n\n    arguments: list[Argument] = []\n    for _, row in clusters.iterrows():\n        cluster_ids = [\"0\"]\n        for cluster_column in cluster_columns:\n            cluster_ids.append(row[cluster_column])\n        argument: Argument = {\n            \"arg_id\": row[\"arg-id\"],\n            \"argument\": row[\"argument\"],\n            \"x\": row[\"x\"],\n            \"y\": row[\"y\"],\n            \"p\": 0,  # NOTE: 一旦全部0でいれる\n            \"cluster_ids\": cluster_ids,\n        }\n        arguments.append(argument)\n    return arguments\n\n\ndef _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:\n    results: list[Cluster] = [\n        Cluster(\n            level=0,\n            id=\"0\",\n            label=\"全体\",\n            takeaway=\"\",\n            value=total_num,\n            parent=\"\",\n            density_rank_percentile=0,\n        )\n    ]\n\n    for _, melted_label in melted_labels.iterrows():\n        cluster_value = Cluster(\n            level=melted_label[\"level\"],\n            id=melted_label[\"id\"],\n            label=melted_label[\"label\"],\n            takeaway=melted_label[\"description\"],\n            value=melted_label[\"value\"],\n            parent=melted_label.get(\"parent\", \"全体\"),\n            density_rank_percentile=melted_label.get(\"density_rank_percentile\"),\n        )\n        results.append(cluster_value)\n    return results\n\n\n# def _build_comments_value(\n#     comments: pd.DataFrame,\n#     arguments: pd.DataFrame,\n#     hidden_properties_map: dict[str, list[str]],\n# ):\n#     comment_dict: dict[str, dict[str, str]] = {}\n#     useful_comment_ids = set(arguments[\"comment-id\"].values)\n#     for _, row in comments.iterrows():\n#         id = row[\"comment-id\"]\n#         if id in useful_comment_ids:\n#             res = {\"comment\": row[\"comment-body\"]}\n#             should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())\n#             if should_skip:\n#                 continue\n#             comment_dict[str(id)] = res\n\n#     return comment_dict\n\n\ndef _build_translations(config):\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        return json.loads(translations)\n    return {}\n\n\ndef _build_property_map(\n    arguments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict\n) -> dict[str, dict[str, str]]:\n    property_columns = list(hidden_properties_map.keys()) + list(config[\"extraction\"][\"categories\"].keys())\n    property_map = defaultdict(dict)\n\n    # 指定された property_columns が arguments に存在するかチェック\n    missing_cols = [col for col in property_columns if col not in arguments.columns]\n    if missing_cols:\n        raise ValueError(\n            f\"指定されたカラム {missing_cols} が args.csv に存在しません。\"\n            \"設定ファイルaggregation / hidden_propertiesから該当カラムを取り除いてください。\"\n        )\n\n    for prop in property_columns:\n        for arg_id, row in arguments.iterrows():\n            # LLMによるcategory classificationがうまく行かず、NaNの場合はNoneにする\n            property_map[prop][arg_id] = row[prop] if not pd.isna(row[prop]) else None\n    return property_map"
        },
        "plan": [
          {
            "step": "extraction",
            "run": true,
            "reason": "not trace of previous run"
          },
          {
            "step": "embedding",
            "run": true,
            "reason": "not trace of previous run"
          },
          {
            "step": "hierarchical_clustering",
            "run": true,
            "reason": "not trace of previous run"
          },
          {
            "step": "hierarchical_initial_labelling",
            "run": true,
            "reason": "not trace of previous run"
          },
          {
            "step": "hierarchical_merge_labelling",
            "run": true,
            "reason": "not trace of previous run"
          },
          {
            "step": "hierarchical_overview",
            "run": true,
            "reason": "not trace of previous run"
          },
          {
            "step": "hierarchical_aggregation",
            "run": true,
            "reason": "not trace of previous run"
          }
        ],
        "status": "running",
        "start_time": "2025-04-22T11:04:31.028907",
        "completed_jobs": [],
        "lock_until": "2025-04-22T11:09:38.305817",
        "current_job": "extraction",
        "current_job_started": "2025-04-22T11:04:31.030887",
        "current_job_progress": 9,
        "current_jop_tasks": 200
      },
      "is_pubcom": true,
      "embedding": {
        "model": "text-embedding-3-small",
        "source_code": "import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model)\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)"
      },
      "hierarchical_initial_labelling": {
        "sampling_num": 3,
        "workers": 1,
        "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nfrom typing import TypedDict\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_llm\n\n\nclass LabellingResult(TypedDict):\n    \"\"\"各クラスタのラベリング結果を表す型\"\"\"\n\n    cluster_id: str  # クラスタのID\n    label: str  # クラスタのラベル名\n    description: str  # クラスタの説明文\n\n\ndef hierarchical_initial_labelling(config: dict) -> None:\n    \"\"\"階層的クラスタリングの初期ラベリングを実行する\n\n    Args:\n        config: 設定情報を含む辞書\n            - output_dir: 出力ディレクトリ名\n            - hierarchical_initial_labelling: 初期ラベリングの設定\n                - sampling_num: サンプリング数\n                - prompt: LLMへのプロンプト\n                - model: 使用するLLMモデル名\n                - workers: 並列処理のワーカー数\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_initial_labels.csv\"\n    clusters_argument_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_clusters.csv\")\n\n    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_id_column = cluster_id_columns[-1]\n    sampling_num = config[\"hierarchical_initial_labelling\"][\"sampling_num\"]\n    initial_labelling_prompt = config[\"hierarchical_initial_labelling\"][\"prompt\"]\n    model = config[\"hierarchical_initial_labelling\"][\"model\"]\n    workers = config[\"hierarchical_initial_labelling\"][\"workers\"]\n\n    initial_label_df = initial_labelling(\n        initial_labelling_prompt,\n        clusters_argument_df,\n        sampling_num,\n        model,\n        workers,\n    )\n    print(\"start initial labelling\")\n    initial_clusters_argument_df = clusters_argument_df.merge(\n        initial_label_df,\n        left_on=initial_cluster_id_column,\n        right_on=\"cluster_id\",\n        how=\"left\",\n    ).rename(\n        columns={\n            \"label\": f\"{initial_cluster_id_column.replace('-id', '')}-label\",\n            \"description\": f\"{initial_cluster_id_column.replace('-id', '')}-description\",\n        }\n    )\n    print(\"end initial labelling\")\n    initial_clusters_argument_df.to_csv(path, index=False)\n\n\ndef initial_labelling(\n    prompt: str,\n    clusters_df: pd.DataFrame,\n    sampling_num: int,\n    model: str,\n    workers: int,\n) -> pd.DataFrame:\n    \"\"\"各クラスタに対して初期ラベリングを実行する\n\n    Args:\n        prompt: LLMへのプロンプト\n        clusters_df: クラスタリング結果のDataFrame\n        sampling_num: 各クラスタからサンプリングする意見の数\n        model: 使用するLLMモデル名\n        workers: 並列処理のワーカー数\n\n    Returns:\n        各クラスタのラベリング結果を含むDataFrame\n    \"\"\"\n    cluster_columns = [col for col in clusters_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_column = cluster_columns[-1]\n    cluster_ids = clusters_df[initial_cluster_column].unique()\n    process_func = partial(\n        process_initial_labelling,\n        df=clusters_df,\n        prompt=prompt,\n        sampling_num=sampling_num,\n        target_column=initial_cluster_column,\n        model=model,\n    )\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        results = list(executor.map(process_func, cluster_ids))\n    return pd.DataFrame(results)\n\n\ndef process_initial_labelling(\n    cluster_id: str,\n    df: pd.DataFrame,\n    prompt: str,\n    sampling_num: int,\n    target_column: str,\n    model: str,\n) -> LabellingResult:\n    \"\"\"個別のクラスタに対してラベリングを実行する\n\n    Args:\n        cluster_id: 処理対象のクラスタID\n        df: クラスタリング結果のDataFrame\n        prompt: LLMへのプロンプト\n        sampling_num: サンプリングする意見の数\n        target_column: クラスタIDが格納されている列名\n        model: 使用するLLMモデル名\n\n    Returns:\n        クラスタのラベリング結果\n    \"\"\"\n    cluster_data = df[df[target_column] == cluster_id]\n    sampling_num = min(sampling_num, len(cluster_data))\n    cluster = cluster_data.sample(sampling_num)\n    input = \"\\n\".join(cluster[\"argument\"].values)\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_llm(messages=messages, model=model, is_json=True)\n        response_json = json.loads(response)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=response_json.get(\"label\", \"エラーでラベル名が取得できませんでした\"),\n            description=response_json.get(\"description\", \"エラーで解説が取得できませんでした\"),\n        )\n    except Exception as e:\n        print(e)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=\"エラーでラベル名が取得できませんでした\",\n            description=\"エラーで解説が取得できませんでした\",\n        )",
        "prompt": "あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説して、それから表札をつけてください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例1\n最近、政治家が能登の復興に向けた具体的なプランを発表し、地域の未来に明るい希望が見えてきました。市民として、真摯な取り組みに感謝しています。\n災害復興支援が、選挙期間中にしっかり議論されるようになり、政治家が国民の本当のニーズに応える姿勢に期待しています。\n選挙を通じて、政治家が地域振興に全力で取り組む姿勢が伝わってきます。具体的な政策提案を目にするたび、未来への希望が膨らみます。\n\n\n## 出力例1\n{{\n    \"label\": \"市民の未来を支える具体的政策への期待\",\n    \"description\": \"このクラスタには、地域の復興や被害者支援など、実際の社会課題に対して政治家が具体的かつ積極的に取り組む姿勢を支持する前向きな意見が集まっています。市民は、選挙や政策議論を通じて、現実の問題に即した支援策や復興計画が実現されることを期待し、明るい未来の構築に向けた政治の変革を応援しています。\"\n}}\n\n\n## 入力例2\nこの病気は原因がはっきりしないため、治療法も確立されておらず、将来がとても不安です。\n医師の説明が専門的すぎて、自分の状態が正確に理解できませんでした。\n現在の治療法には副作用が多く、生活の質が大きく下がっています。もっと副作用の少ない選択肢が欲しいです。\n同じ病気の患者同士で情報交換できる場がもっとあれば、心の支えになるのにと思います。\n検査の予約が何ヶ月も先で、早期発見や早期治療ができない状況に不満があります。\n\n\n## 出力例2\n{{\n    \"label\": \"疾患への不安と治療環境への改善要望\",\n    \"description\": \"このクラスタには、病気の原因や治療法が明確でないことに起因する将来への不安や、治療中の副作用、説明不足といった医療提供側への不満が集まっています。また、患者同士の交流の場や医療アクセスの改善など、心身両面に配慮した医療環境の整備を求める要望も含まれており、患者視点から見た『納得感のある医療体験』への強いニーズが表れています。\"\n}}",
        "model": "gemini-2.0-flash"
      },
      "hierarchical_merge_labelling": {
        "sampling_num": 3,
        "workers": 1,
        "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_llm\n\n\n@dataclass\nclass ClusterColumns:\n    \"\"\"同一階層のクラスター関連のカラム名を管理するクラス\"\"\"\n\n    id: str\n    label: str\n    description: str\n\n    @classmethod\n    def from_id_column(cls, id_column: str) -> \"ClusterColumns\":\n        \"\"\"ID列名から関連するカラム名を生成\"\"\"\n        return cls(\n            id=id_column,\n            label=id_column.replace(\"-id\", \"-label\"),\n            description=id_column.replace(\"-id\", \"-description\"),\n        )\n\n\n@dataclass\nclass ClusterValues:\n    \"\"\"対象クラスタのlabel/descriptionを管理するクラス\"\"\"\n\n    label: str\n    description: str\n\n    def to_prompt_text(self) -> str:\n        return f\"- {self.label}: {self.description}\"\n\n\ndef hierarchical_merge_labelling(config: dict) -> None:\n    \"\"\"階層的クラスタリングの結果に対してマージラベリングを実行する\n\n    Args:\n        config: 設定情報を含む辞書\n            - output_dir: 出力ディレクトリ名\n            - hierarchical_merge_labelling: マージラベリングの設定\n                - sampling_num: サンプリング数\n                - prompt: LLMへのプロンプト\n                - model: 使用するLLMモデル名\n                - workers: 並列処理のワーカー数\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    merge_path = f\"outputs/{dataset}/hierarchical_merge_labels.csv\"\n    clusters_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_initial_labels.csv\")\n\n    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)\n    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成\n    merge_result_df = merge_labelling(\n        clusters_df=clusters_df,\n        cluster_id_columns=sorted(cluster_id_columns, reverse=True),\n        config=config,\n    )\n    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成\n    melted_df = melt_cluster_data(merge_result_df)\n    # 上記のdfに親子関係を追加\n    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)\n    melted_df = melted_df.merge(parent_child_df, on=[\"level\", \"id\"], how=\"left\")\n    density_df = calculate_cluster_density(melted_df, config)\n    density_df.to_csv(merge_path, index=False)\n\n\ndef _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):\n    \"\"\"クラスタ間の親子関係をマッピングする\n\n    Args:\n        df: クラスタリング結果のDataFrame\n        cluster_id_columns: クラスタIDのカラム名のリスト\n\n    Returns:\n        親子関係のマッピング情報を含むDataFrame\n    \"\"\"\n    results = []\n    top_cluster_column = cluster_id_columns[0]\n    top_cluster_values = df[top_cluster_column].unique()\n    for c in top_cluster_values:\n        results.append(\n            {\n                \"level\": 1,\n                \"id\": c,\n                \"parent\": \"0\",  # aggregationで追加する全体クラスタのid\n            }\n        )\n\n    for idx in range(len(cluster_id_columns) - 1):\n        current_column = cluster_id_columns[idx]\n        children_column = cluster_id_columns[idx + 1]\n        current_level = current_column.replace(\"-id\", \"\").replace(\"cluster-level-\", \"\")\n        # 現在のレベルのクラスタid\n        current_cluster_values = df[current_column].unique()\n        for current_id in current_cluster_values:\n            children_ids = df.loc[df[current_column] == current_id, children_column].unique()\n            for child_id in children_ids:\n                results.append(\n                    {\n                        \"level\": int(current_level) + 1,\n                        \"id\": child_id,\n                        \"parent\": current_id,\n                    }\n                )\n    return pd.DataFrame(results)\n\n\ndef _filter_id_columns(columns: list[str]) -> list[str]:\n    \"\"\"クラスタIDのカラム名をフィルタリングする\n\n    Args:\n        columns: 全カラム名のリスト\n\n    Returns:\n        クラスタIDのカラム名のリスト\n    \"\"\"\n    return [col for col in columns if col.startswith(\"cluster-level-\") and col.endswith(\"-id\")]\n\n\ndef melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"クラスタデータを行形式に変換する\n\n    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。\n    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。\n\n    Args:\n        df: クラスタリング結果のDataFrame\n\n    Returns:\n        行形式に変換されたDataFrame\n    \"\"\"\n    id_columns: list[str] = _filter_id_columns(df.columns)\n    levels: set[int] = {int(col.replace(\"cluster-level-\", \"\").replace(\"-id\", \"\")) for col in id_columns}\n    all_rows: list[dict] = []\n\n    # levelごとに各クラスタの出現件数を集計・縦持ちにする\n    for level in levels:\n        cluster_columns = ClusterColumns.from_id_column(f\"cluster-level-{level}-id\")\n        # クラスタidごとの件数集計\n        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name=\"value\")\n\n        level_unique_val_df = df[\n            [cluster_columns.id, cluster_columns.label, cluster_columns.description]\n        ].drop_duplicates()\n        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how=\"left\")\n        level_unique_vals = [\n            {\n                \"level\": level,\n                \"id\": row[cluster_columns.id],\n                \"label\": row[cluster_columns.label],\n                \"description\": row[cluster_columns.description],\n                \"value\": row[\"value\"],\n            }\n            for _, row in level_unique_val_df.iterrows()\n        ]\n        all_rows.extend(level_unique_vals)\n    return pd.DataFrame(all_rows)\n\n\ndef merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:\n    \"\"\"階層的なクラスタのマージラベリングを実行する\n\n    Args:\n        clusters_df: クラスタリング結果のDataFrame\n        cluster_id_columns: クラスタIDのカラム名のリスト\n        config: 設定情報を含む辞書\n\n    Returns:\n        マージラベリング結果を含むDataFrame\n    \"\"\"\n    for idx in tqdm(range(len(cluster_id_columns) - 1)):\n        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])\n        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])\n\n        process_fn = partial(\n            process_merge_labelling,\n            result_df=clusters_df,\n            current_columns=current_columns,\n            previous_columns=previous_columns,\n            config=config,\n        )\n\n        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())\n        with ThreadPoolExecutor(max_workers=config[\"hierarchical_merge_labelling\"][\"workers\"]) as executor:\n            responses = list(\n                tqdm(\n                    executor.map(process_fn, current_cluster_ids),\n                    total=len(current_cluster_ids),\n                )\n            )\n\n        current_result_df = pd.DataFrame(responses)\n        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])\n    return clusters_df\n\n\ndef process_merge_labelling(\n    target_cluster_id: str,\n    result_df: pd.DataFrame,\n    current_columns: ClusterColumns,\n    previous_columns: ClusterColumns,\n    config,\n):\n    \"\"\"個別のクラスタに対してマージラベリングを実行する\n\n    Args:\n        target_cluster_id: 処理対象のクラスタID\n        result_df: クラスタリング結果のDataFrame\n        current_columns: 現在のレベルのカラム情報\n        previous_columns: 前のレベルのカラム情報\n        config: 設定情報を含む辞書\n\n    Returns:\n        マージラベリング結果を含む辞書\n    \"\"\"\n\n    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:\n        \"\"\"前のレベルのクラスタ情報を取得する\"\"\"\n        previous_records = df[df[current_columns.id] == target_cluster_id][\n            [previous_columns.label, previous_columns.description]\n        ].drop_duplicates()\n        previous_values = [\n            ClusterValues(\n                label=row[previous_columns.label],\n                description=row[previous_columns.description],\n            )\n            for _, row in previous_records.iterrows()\n        ]\n        return previous_values\n\n    previous_values = filter_previous_values(result_df, previous_columns)\n    if len(previous_values) == 1:\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: previous_values[0].label,\n            current_columns.description: previous_values[0].description,\n        }\n    elif len(previous_values) == 0:\n        raise ValueError(f\"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。\")\n\n    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]\n    sampling_num = min(\n        config[\"hierarchical_merge_labelling\"][\"sampling_num\"],\n        len(current_cluster_data),\n    )\n    sampled_data = current_cluster_data.sample(sampling_num)\n    sampled_argument_text = \"\\n\".join(sampled_data[\"argument\"].values)\n    cluster_text = \"\\n\".join([value.to_prompt_text() for value in previous_values])\n    messages = [\n        {\"role\": \"system\", \"content\": config[\"hierarchical_merge_labelling\"][\"prompt\"]},\n        {\n            \"role\": \"user\",\n            \"content\": \"クラスタラベル\\n\" + cluster_text + \"\\n\" + \"クラスタの意見\\n\" + sampled_argument_text,\n        },\n    ]\n    try:\n        response = request_to_chat_llm(\n            messages=messages,\n            model=config[\"hierarchical_merge_labelling\"][\"model\"],\n            is_json=True,\n        )\n        response_json = json.loads(response)\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: response_json.get(\"label\", \"エラーでラベル名が取得できませんでした\"),\n            current_columns.description: response_json.get(\"description\", \"エラーで解説が取得できませんでした\"),\n        }\n    except Exception as e:\n        print(f\"エラーが発生しました: {e}\")\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: \"エラーでラベル名が取得できませんでした\",\n            current_columns.description: \"エラーで解説が取得できませんでした\",\n        }\n\n\ndef calculate_cluster_density(melted_df: pd.DataFrame, config: dict):\n    \"\"\"クラスタ内の密度計算\"\"\"\n    hierarchical_cluster_df = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n\n    densities = []\n    for level, c_id in zip(melted_df[\"level\"], melted_df[\"id\"], strict=False):\n        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f\"cluster-level-{level}-id\"] == c_id][\n            [\"x\", \"y\"]\n        ].values\n        density = calculate_density(cluster_embeds)\n        densities.append(density)\n\n    # 密度のランクを計算\n    melted_df[\"density\"] = densities\n    melted_df[\"density_rank\"] = melted_df.groupby(\"level\")[\"density\"].rank(ascending=False, method=\"first\")\n    melted_df[\"density_rank_percentile\"] = melted_df.groupby(\"level\")[\"density_rank\"].transform(lambda x: x / len(x))\n    return melted_df\n\n\ndef calculate_density(embeds: np.ndarray):\n    \"\"\"平均距離に基づいて密度を計算\"\"\"\n    center = np.mean(embeds, axis=0)\n    distances = np.linalg.norm(embeds - center, axis=1)\n    avg_distance = np.mean(distances)\n    density = 1 / (avg_distance + 1e-10)\n    return density",
        "prompt": "分割されすぎたクラスタを統合する必要があるので、統合後の名称を考えて出力して。\n\n# 指示\n* 統合前のクラスタの名称・説明および統合後のクラスタに属するデータ点のサンプルを与えるので、これらに基づいて統合後のクラスタの名称を出力してください\n    * 統合後のクラスタ名において、統合前のクラスタ名をそのまま使うことは避けてください。\n* 出力例に記載したJSONのフォーマットに従って出力してください\n\n# サンプルの入出力\n## 入力例1（クラスタラベル:説明文）\n- 地域の災害対応への批判: このクラスタは、地域における災害対応策の実施や体制に対する批判的な意見を集約したものです。住民からは、迅速かつ効果的な支援が行われていない点や、情報提供・連携の不足などに対する強い不満が表明されています。\n- 災害対応への不満: このクラスタは、災害発生時の対応全般に対する不満を示す意見をまとめたものです。救援活動の遅れや支援策の実効性に疑問を持つ声が多く、より積極的で透明性のある対応を求める意見が特徴です。\n- 地域復興の遅れ: このクラスタは、災害後の地域復興プロセスが予定通りに進んでいない点に対する懸念や不満を反映しています。再建計画や支援策の実施の遅延、そしてそれに伴う住民の生活再建への影響が強調されています。\n\n\n## 出力例1\n{{\n    \"label\": \"地域再生と災害支援に対する期待と懸念\",\n    \"description\": \"このクラスタは、特定の地域における再生や災害支援策に対し、具体的な取り組みが不足しているとの意見を集約しています。市民は、選挙を通じた政策議論の中で、地域復興や被災者支援を最優先すべきだとの期待と、現行の支援策に対する改善要求を強く表明しており、より効果的な政府の対応を求める声が反映されています。\"\n}}\n\n\n## 入力例2（クラスタラベル:説明文）\n- 新薬の承認遅延に対する不満:このクラスタは、新薬の承認プロセスが煩雑かつ時間がかかりすぎるという不満の声をまとめたものです。特に、重篤な疾患に苦しむ患者やその家族からは、迅速なアクセスの欠如に対する苛立ちが表明されています。\n- 未治療疾患に対する対応不足:このクラスタは、現時点で有効な治療法が存在しない疾患や、研究開発の進展が遅れている分野に関する課題を指摘する意見を集めています。患者からは、自分たちのニーズが無視されているという感覚が共有されています。\n- 希少疾患への医療アクセスの不平等:このクラスタは、希少疾患を抱える患者が十分な医療資源や治療機会を得られないことに対する不満を中心としています。医療制度の網からこぼれ落ちているという訴えが多く寄せられています。\n\n## 出力例2\n{{\n  \"label\": \"治療機会の格差と革新的医療へのアクセス課題\",\n  \"description\": \"このクラスタは、重篤または希少な疾患に対して、十分な治療法が提供されていない状況や、医療制度における承認プロセスの遅延、アクセス格差に対する不満を集約しています。患者やその家族は、既存の制度が急を要する医療ニーズに対応できていないことに強い懸念を示しており、より迅速かつ公平な医療提供体制の確立を求めています。\"\n}}",
        "model": "gemini-2.0-flash"
      },
      "hierarchical_overview": {
        "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_llm\n\n\ndef hierarchical_overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_overview.txt\"\n\n    hierarchical_label_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_merge_labels.csv\")\n\n    prompt = config[\"hierarchical_overview\"][\"prompt\"]\n    model = config[\"hierarchical_overview\"][\"model\"]\n\n    # TODO: level1で固定にしているが、設定で変えられるようにする\n    target_level = 1\n    target_records = hierarchical_label_df[hierarchical_label_df[\"level\"] == target_level]\n    ids = target_records[\"id\"].to_list()\n    labels = target_records[\"label\"].to_list()\n    descriptions = target_records[\"description\"].to_list()\n    target_records.set_index(\"id\", inplace=True)\n\n    input = \"\"\n    for i, _ in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels[i]}\\n\\n\"\n        input += descriptions[i] + \"\\n\\n\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_llm(messages=messages, model=model)\n\n    with open(path, \"w\", encoding='utf-8') as file:\n        file.write(response)",
        "prompt": "/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。",
        "model": "gemini-2.0-flash"
      },
      "hierarchical_aggregation": {
        "sampling_num": 5000,
        "hidden_properties": {},
        "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import TypedDict\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\n\n\nclass Argument(TypedDict):\n    arg_id: str\n    argument: str\n    comment_id: str\n    x: float\n    y: float\n    p: float\n    cluster_ids: list[str]\n\n\nclass Cluster(TypedDict):\n    level: int\n    id: str\n    label: str\n    takeaway: str\n    value: int\n    parent: str\n    density_rank_percentile: float | None\n\n\ndef hierarchical_aggregation(config):\n    path = f\"outputs/{config['output_dir']}/hierarchical_result.json\"\n    results = {\n        \"arguments\": [],\n        \"clusters\": [],\n        \"comments\": {},\n        \"propertyMap\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index(\"arg-id\", inplace=True)\n    arg_num = len(arguments)\n    relation_df = pd.read_csv(f\"outputs/{config['output_dir']}/relations.csv\")\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_merge_labels.csv\")\n\n    hidden_properties_map: dict[str, list[str]] = config[\"hierarchical_aggregation\"][\"hidden_properties\"]\n\n    results[\"arguments\"] = _build_arguments(clusters)\n    results[\"clusters\"] = _build_cluster_value(labels, arg_num)\n    # NOTE: 属性に応じたコメントフィルタ機能が実装されておらず、全てのコメントが含まれてしまうので、コメントアウト\n    # results[\"comments\"] = _build_comments_value(\n    #     comments, arguments, hidden_properties_map\n    # )\n    results[\"comment_num\"] = len(comments)\n    results[\"translations\"] = _build_translations(config)\n    # 属性情報のカラムは、元データに対して指定したカラムとclassificationするカテゴリを合わせたもの\n    results[\"propertyMap\"] = _build_property_map(arguments, hidden_properties_map, config)\n\n    with open(f\"outputs/{config['output_dir']}/hierarchical_overview.txt\", encoding='utf-8') as f:\n        overview = f.read()\n    print(\"overview\")\n    print(overview)\n    results[\"overview\"] = overview\n\n    with open(path, \"w\", encoding='utf-8') as file:\n        json.dump(results, file, indent=2, ensure_ascii=False)\n    # TODO: サンプリングロジックを実装したいが、現状は全件抽出\n    create_custom_intro(config)\n    if config[\"is_pubcom\"]:\n        add_original_comments(labels, arguments, relation_df, clusters, config)\n\n\ndef create_custom_intro(config):\n    dataset = config[\"output_dir\"]\n    args_path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    result_path = f\"outputs/{dataset}/hierarchical_result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n    processed_num = min(input_count, config[\"extraction\"][\"limit\"])\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n分析対象となったデータの件数は{processed_num}件で、これらのデータに対してOpenAI APIを用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(intro=intro, processed_num=processed_num, args_count=args_count)\n\n    with open(result_path, encoding='utf-8') as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\", encoding='utf-8') as f:\n        json.dump(result, f, indent=2, ensure_ascii=False)\n\n\ndef add_original_comments(labels, arguments, relation_df, clusters, config):\n    # 大カテゴリ（cluster-level-1）に該当するラベルだけ抽出\n    labels_lv1 = labels[labels[\"level\"] == 1][[\"id\", \"label\"]].rename(\n        columns={\"id\": \"cluster-level-1-id\", \"label\": \"category_label\"}\n    )\n\n    # arguments と clusters をマージ（カテゴリ情報付与）\n    merged = arguments.merge(clusters[[\"arg-id\", \"cluster-level-1-id\"]], on=\"arg-id\").merge(\n        labels_lv1, on=\"cluster-level-1-id\", how=\"left\"\n    )\n\n    # relation_df と結合\n    merged = merged.merge(relation_df, on=\"arg-id\", how=\"left\")\n\n    # 元コメント取得\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    comments[\"comment-id\"] = comments[\"comment-id\"].astype(str)\n    merged[\"comment-id\"] = merged[\"comment-id\"].astype(str)\n\n    # 元コメント本文などとマージ\n    final_df = merged.merge(comments, on=\"comment-id\", how=\"left\")\n\n    # 必要カラムのみ整形\n    final_cols = [\"comment-id\", \"comment-body\", \"arg-id\", \"argument\", \"cluster-level-1-id\", \"category_label\"]\n    for col in [\"source\", \"url\"]:\n        if col in comments.columns:\n            final_cols.append(col)\n\n    final_df = final_df[final_cols]\n    final_df = final_df.rename(\n        columns={\n            \"cluster-level-1-id\": \"category_id\",\n            \"category_label\": \"category\",\n            \"arg-id\": \"arg_id\",\n            \"argument\": \"argument\",\n            \"comment-body\": \"original-comment\",\n        }\n    )\n\n    # 保存\n    final_df.to_csv(f\"outputs/{config['output_dir']}/final_result_with_comments.csv\", index=False, encoding='utf-8-sig')\n\n\ndef _build_arguments(clusters: pd.DataFrame) -> list[Argument]:\n    cluster_columns = [col for col in clusters.columns if col.startswith(\"cluster-level-\") and \"id\" in col]\n\n    arguments: list[Argument] = []\n    for _, row in clusters.iterrows():\n        cluster_ids = [\"0\"]\n        for cluster_column in cluster_columns:\n            cluster_ids.append(row[cluster_column])\n        argument: Argument = {\n            \"arg_id\": row[\"arg-id\"],\n            \"argument\": row[\"argument\"],\n            \"x\": row[\"x\"],\n            \"y\": row[\"y\"],\n            \"p\": 0,  # NOTE: 一旦全部0でいれる\n            \"cluster_ids\": cluster_ids,\n        }\n        arguments.append(argument)\n    return arguments\n\n\ndef _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:\n    results: list[Cluster] = [\n        Cluster(\n            level=0,\n            id=\"0\",\n            label=\"全体\",\n            takeaway=\"\",\n            value=total_num,\n            parent=\"\",\n            density_rank_percentile=0,\n        )\n    ]\n\n    for _, melted_label in melted_labels.iterrows():\n        cluster_value = Cluster(\n            level=melted_label[\"level\"],\n            id=melted_label[\"id\"],\n            label=melted_label[\"label\"],\n            takeaway=melted_label[\"description\"],\n            value=melted_label[\"value\"],\n            parent=melted_label.get(\"parent\", \"全体\"),\n            density_rank_percentile=melted_label.get(\"density_rank_percentile\"),\n        )\n        results.append(cluster_value)\n    return results\n\n\n# def _build_comments_value(\n#     comments: pd.DataFrame,\n#     arguments: pd.DataFrame,\n#     hidden_properties_map: dict[str, list[str]],\n# ):\n#     comment_dict: dict[str, dict[str, str]] = {}\n#     useful_comment_ids = set(arguments[\"comment-id\"].values)\n#     for _, row in comments.iterrows():\n#         id = row[\"comment-id\"]\n#         if id in useful_comment_ids:\n#             res = {\"comment\": row[\"comment-body\"]}\n#             should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())\n#             if should_skip:\n#                 continue\n#             comment_dict[str(id)] = res\n\n#     return comment_dict\n\n\ndef _build_translations(config):\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        return json.loads(translations)\n    return {}\n\n\ndef _build_property_map(\n    arguments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict\n) -> dict[str, dict[str, str]]:\n    property_columns = list(hidden_properties_map.keys()) + list(config[\"extraction\"][\"categories\"].keys())\n    property_map = defaultdict(dict)\n\n    # 指定された property_columns が arguments に存在するかチェック\n    missing_cols = [col for col in property_columns if col not in arguments.columns]\n    if missing_cols:\n        raise ValueError(\n            f\"指定されたカラム {missing_cols} が args.csv に存在しません。\"\n            \"設定ファイルaggregation / hidden_propertiesから該当カラムを取り除いてください。\"\n        )\n\n    for prop in property_columns:\n        for arg_id, row in arguments.iterrows():\n            # LLMによるcategory classificationがうまく行かず、NaNの場合はNoneにする\n            property_map[prop][arg_id] = row[prop] if not pd.isna(row[prop]) else None\n    return property_map"
      },
      "plan": [
        {
          "step": "extraction",
          "run": true,
          "reason": "not trace of previous run"
        },
        {
          "step": "embedding",
          "run": true,
          "reason": "not trace of previous run"
        },
        {
          "step": "hierarchical_clustering",
          "run": true,
          "reason": "not trace of previous run"
        },
        {
          "step": "hierarchical_initial_labelling",
          "run": true,
          "reason": "not trace of previous run"
        },
        {
          "step": "hierarchical_merge_labelling",
          "run": true,
          "reason": "not trace of previous run"
        },
        {
          "step": "hierarchical_overview",
          "run": true,
          "reason": "not trace of previous run"
        },
        {
          "step": "hierarchical_aggregation",
          "run": true,
          "reason": "not trace of previous run"
        }
      ],
      "status": "running",
      "start_time": "2025-04-22T12:32:56.413544",
      "completed_jobs": [],
      "lock_until": "2025-04-22T12:38:24.523657",
      "current_job": "extraction",
      "current_job_started": "2025-04-22T12:32:56.417713",
      "current_job_progress": 24,
      "current_jop_tasks": 200
    },
    "is_pubcom": true,
    "embedding": {
      "model": "text-embedding-3-small",
      "source_code": "import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model)\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)"
    },
    "hierarchical_initial_labelling": {
      "sampling_num": 3,
      "workers": 1,
      "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nfrom typing import TypedDict\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_llm\n\n\nclass LabellingResult(TypedDict):\n    \"\"\"各クラスタのラベリング結果を表す型\"\"\"\n\n    cluster_id: str  # クラスタのID\n    label: str  # クラスタのラベル名\n    description: str  # クラスタの説明文\n\n\ndef hierarchical_initial_labelling(config: dict) -> None:\n    \"\"\"階層的クラスタリングの初期ラベリングを実行する\n\n    Args:\n        config: 設定情報を含む辞書\n            - output_dir: 出力ディレクトリ名\n            - hierarchical_initial_labelling: 初期ラベリングの設定\n                - sampling_num: サンプリング数\n                - prompt: LLMへのプロンプト\n                - model: 使用するLLMモデル名\n                - workers: 並列処理のワーカー数\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_initial_labels.csv\"\n    clusters_argument_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_clusters.csv\")\n\n    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_id_column = cluster_id_columns[-1]\n    sampling_num = config[\"hierarchical_initial_labelling\"][\"sampling_num\"]\n    initial_labelling_prompt = config[\"hierarchical_initial_labelling\"][\"prompt\"]\n    model = config[\"hierarchical_initial_labelling\"][\"model\"]\n    workers = config[\"hierarchical_initial_labelling\"][\"workers\"]\n\n    initial_label_df = initial_labelling(\n        initial_labelling_prompt,\n        clusters_argument_df,\n        sampling_num,\n        model,\n        workers,\n    )\n    print(\"start initial labelling\")\n    initial_clusters_argument_df = clusters_argument_df.merge(\n        initial_label_df,\n        left_on=initial_cluster_id_column,\n        right_on=\"cluster_id\",\n        how=\"left\",\n    ).rename(\n        columns={\n            \"label\": f\"{initial_cluster_id_column.replace('-id', '')}-label\",\n            \"description\": f\"{initial_cluster_id_column.replace('-id', '')}-description\",\n        }\n    )\n    print(\"end initial labelling\")\n    initial_clusters_argument_df.to_csv(path, index=False)\n\n\ndef initial_labelling(\n    prompt: str,\n    clusters_df: pd.DataFrame,\n    sampling_num: int,\n    model: str,\n    workers: int,\n) -> pd.DataFrame:\n    \"\"\"各クラスタに対して初期ラベリングを実行する\n\n    Args:\n        prompt: LLMへのプロンプト\n        clusters_df: クラスタリング結果のDataFrame\n        sampling_num: 各クラスタからサンプリングする意見の数\n        model: 使用するLLMモデル名\n        workers: 並列処理のワーカー数\n\n    Returns:\n        各クラスタのラベリング結果を含むDataFrame\n    \"\"\"\n    cluster_columns = [col for col in clusters_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_column = cluster_columns[-1]\n    cluster_ids = clusters_df[initial_cluster_column].unique()\n    process_func = partial(\n        process_initial_labelling,\n        df=clusters_df,\n        prompt=prompt,\n        sampling_num=sampling_num,\n        target_column=initial_cluster_column,\n        model=model,\n    )\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        results = list(executor.map(process_func, cluster_ids))\n    return pd.DataFrame(results)\n\n\ndef process_initial_labelling(\n    cluster_id: str,\n    df: pd.DataFrame,\n    prompt: str,\n    sampling_num: int,\n    target_column: str,\n    model: str,\n) -> LabellingResult:\n    \"\"\"個別のクラスタに対してラベリングを実行する\n\n    Args:\n        cluster_id: 処理対象のクラスタID\n        df: クラスタリング結果のDataFrame\n        prompt: LLMへのプロンプト\n        sampling_num: サンプリングする意見の数\n        target_column: クラスタIDが格納されている列名\n        model: 使用するLLMモデル名\n\n    Returns:\n        クラスタのラベリング結果\n    \"\"\"\n    cluster_data = df[df[target_column] == cluster_id]\n    sampling_num = min(sampling_num, len(cluster_data))\n    cluster = cluster_data.sample(sampling_num)\n    input = \"\\n\".join(cluster[\"argument\"].values)\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_llm(messages=messages, model=model, is_json=True)\n        response_json = json.loads(response)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=response_json.get(\"label\", \"エラーでラベル名が取得できませんでした\"),\n            description=response_json.get(\"description\", \"エラーで解説が取得できませんでした\"),\n        )\n    except Exception as e:\n        print(e)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=\"エラーでラベル名が取得できませんでした\",\n            description=\"エラーで解説が取得できませんでした\",\n        )",
      "prompt": "あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説して、それから表札をつけてください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例1\n最近、政治家が能登の復興に向けた具体的なプランを発表し、地域の未来に明るい希望が見えてきました。市民として、真摯な取り組みに感謝しています。\n災害復興支援が、選挙期間中にしっかり議論されるようになり、政治家が国民の本当のニーズに応える姿勢に期待しています。\n選挙を通じて、政治家が地域振興に全力で取り組む姿勢が伝わってきます。具体的な政策提案を目にするたび、未来への希望が膨らみます。\n\n\n## 出力例1\n{{\n    \"label\": \"市民の未来を支える具体的政策への期待\",\n    \"description\": \"このクラスタには、地域の復興や被害者支援など、実際の社会課題に対して政治家が具体的かつ積極的に取り組む姿勢を支持する前向きな意見が集まっています。市民は、選挙や政策議論を通じて、現実の問題に即した支援策や復興計画が実現されることを期待し、明るい未来の構築に向けた政治の変革を応援しています。\"\n}}\n\n\n## 入力例2\nこの病気は原因がはっきりしないため、治療法も確立されておらず、将来がとても不安です。\n医師の説明が専門的すぎて、自分の状態が正確に理解できませんでした。\n現在の治療法には副作用が多く、生活の質が大きく下がっています。もっと副作用の少ない選択肢が欲しいです。\n同じ病気の患者同士で情報交換できる場がもっとあれば、心の支えになるのにと思います。\n検査の予約が何ヶ月も先で、早期発見や早期治療ができない状況に不満があります。\n\n\n## 出力例2\n{{\n    \"label\": \"疾患への不安と治療環境への改善要望\",\n    \"description\": \"このクラスタには、病気の原因や治療法が明確でないことに起因する将来への不安や、治療中の副作用、説明不足といった医療提供側への不満が集まっています。また、患者同士の交流の場や医療アクセスの改善など、心身両面に配慮した医療環境の整備を求める要望も含まれており、患者視点から見た『納得感のある医療体験』への強いニーズが表れています。\"\n}}",
      "model": "gemini-2.0-flash"
    },
    "hierarchical_merge_labelling": {
      "sampling_num": 3,
      "workers": 1,
      "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_llm\n\n\n@dataclass\nclass ClusterColumns:\n    \"\"\"同一階層のクラスター関連のカラム名を管理するクラス\"\"\"\n\n    id: str\n    label: str\n    description: str\n\n    @classmethod\n    def from_id_column(cls, id_column: str) -> \"ClusterColumns\":\n        \"\"\"ID列名から関連するカラム名を生成\"\"\"\n        return cls(\n            id=id_column,\n            label=id_column.replace(\"-id\", \"-label\"),\n            description=id_column.replace(\"-id\", \"-description\"),\n        )\n\n\n@dataclass\nclass ClusterValues:\n    \"\"\"対象クラスタのlabel/descriptionを管理するクラス\"\"\"\n\n    label: str\n    description: str\n\n    def to_prompt_text(self) -> str:\n        return f\"- {self.label}: {self.description}\"\n\n\ndef hierarchical_merge_labelling(config: dict) -> None:\n    \"\"\"階層的クラスタリングの結果に対してマージラベリングを実行する\n\n    Args:\n        config: 設定情報を含む辞書\n            - output_dir: 出力ディレクトリ名\n            - hierarchical_merge_labelling: マージラベリングの設定\n                - sampling_num: サンプリング数\n                - prompt: LLMへのプロンプト\n                - model: 使用するLLMモデル名\n                - workers: 並列処理のワーカー数\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    merge_path = f\"outputs/{dataset}/hierarchical_merge_labels.csv\"\n    clusters_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_initial_labels.csv\")\n\n    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)\n    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成\n    merge_result_df = merge_labelling(\n        clusters_df=clusters_df,\n        cluster_id_columns=sorted(cluster_id_columns, reverse=True),\n        config=config,\n    )\n    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成\n    melted_df = melt_cluster_data(merge_result_df)\n    # 上記のdfに親子関係を追加\n    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)\n    melted_df = melted_df.merge(parent_child_df, on=[\"level\", \"id\"], how=\"left\")\n    density_df = calculate_cluster_density(melted_df, config)\n    density_df.to_csv(merge_path, index=False)\n\n\ndef _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):\n    \"\"\"クラスタ間の親子関係をマッピングする\n\n    Args:\n        df: クラスタリング結果のDataFrame\n        cluster_id_columns: クラスタIDのカラム名のリスト\n\n    Returns:\n        親子関係のマッピング情報を含むDataFrame\n    \"\"\"\n    results = []\n    top_cluster_column = cluster_id_columns[0]\n    top_cluster_values = df[top_cluster_column].unique()\n    for c in top_cluster_values:\n        results.append(\n            {\n                \"level\": 1,\n                \"id\": c,\n                \"parent\": \"0\",  # aggregationで追加する全体クラスタのid\n            }\n        )\n\n    for idx in range(len(cluster_id_columns) - 1):\n        current_column = cluster_id_columns[idx]\n        children_column = cluster_id_columns[idx + 1]\n        current_level = current_column.replace(\"-id\", \"\").replace(\"cluster-level-\", \"\")\n        # 現在のレベルのクラスタid\n        current_cluster_values = df[current_column].unique()\n        for current_id in current_cluster_values:\n            children_ids = df.loc[df[current_column] == current_id, children_column].unique()\n            for child_id in children_ids:\n                results.append(\n                    {\n                        \"level\": int(current_level) + 1,\n                        \"id\": child_id,\n                        \"parent\": current_id,\n                    }\n                )\n    return pd.DataFrame(results)\n\n\ndef _filter_id_columns(columns: list[str]) -> list[str]:\n    \"\"\"クラスタIDのカラム名をフィルタリングする\n\n    Args:\n        columns: 全カラム名のリスト\n\n    Returns:\n        クラスタIDのカラム名のリスト\n    \"\"\"\n    return [col for col in columns if col.startswith(\"cluster-level-\") and col.endswith(\"-id\")]\n\n\ndef melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"クラスタデータを行形式に変換する\n\n    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。\n    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。\n\n    Args:\n        df: クラスタリング結果のDataFrame\n\n    Returns:\n        行形式に変換されたDataFrame\n    \"\"\"\n    id_columns: list[str] = _filter_id_columns(df.columns)\n    levels: set[int] = {int(col.replace(\"cluster-level-\", \"\").replace(\"-id\", \"\")) for col in id_columns}\n    all_rows: list[dict] = []\n\n    # levelごとに各クラスタの出現件数を集計・縦持ちにする\n    for level in levels:\n        cluster_columns = ClusterColumns.from_id_column(f\"cluster-level-{level}-id\")\n        # クラスタidごとの件数集計\n        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name=\"value\")\n\n        level_unique_val_df = df[\n            [cluster_columns.id, cluster_columns.label, cluster_columns.description]\n        ].drop_duplicates()\n        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how=\"left\")\n        level_unique_vals = [\n            {\n                \"level\": level,\n                \"id\": row[cluster_columns.id],\n                \"label\": row[cluster_columns.label],\n                \"description\": row[cluster_columns.description],\n                \"value\": row[\"value\"],\n            }\n            for _, row in level_unique_val_df.iterrows()\n        ]\n        all_rows.extend(level_unique_vals)\n    return pd.DataFrame(all_rows)\n\n\ndef merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:\n    \"\"\"階層的なクラスタのマージラベリングを実行する\n\n    Args:\n        clusters_df: クラスタリング結果のDataFrame\n        cluster_id_columns: クラスタIDのカラム名のリスト\n        config: 設定情報を含む辞書\n\n    Returns:\n        マージラベリング結果を含むDataFrame\n    \"\"\"\n    for idx in tqdm(range(len(cluster_id_columns) - 1)):\n        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])\n        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])\n\n        process_fn = partial(\n            process_merge_labelling,\n            result_df=clusters_df,\n            current_columns=current_columns,\n            previous_columns=previous_columns,\n            config=config,\n        )\n\n        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())\n        with ThreadPoolExecutor(max_workers=config[\"hierarchical_merge_labelling\"][\"workers\"]) as executor:\n            responses = list(\n                tqdm(\n                    executor.map(process_fn, current_cluster_ids),\n                    total=len(current_cluster_ids),\n                )\n            )\n\n        current_result_df = pd.DataFrame(responses)\n        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])\n    return clusters_df\n\n\ndef process_merge_labelling(\n    target_cluster_id: str,\n    result_df: pd.DataFrame,\n    current_columns: ClusterColumns,\n    previous_columns: ClusterColumns,\n    config,\n):\n    \"\"\"個別のクラスタに対してマージラベリングを実行する\n\n    Args:\n        target_cluster_id: 処理対象のクラスタID\n        result_df: クラスタリング結果のDataFrame\n        current_columns: 現在のレベルのカラム情報\n        previous_columns: 前のレベルのカラム情報\n        config: 設定情報を含む辞書\n\n    Returns:\n        マージラベリング結果を含む辞書\n    \"\"\"\n\n    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:\n        \"\"\"前のレベルのクラスタ情報を取得する\"\"\"\n        previous_records = df[df[current_columns.id] == target_cluster_id][\n            [previous_columns.label, previous_columns.description]\n        ].drop_duplicates()\n        previous_values = [\n            ClusterValues(\n                label=row[previous_columns.label],\n                description=row[previous_columns.description],\n            )\n            for _, row in previous_records.iterrows()\n        ]\n        return previous_values\n\n    previous_values = filter_previous_values(result_df, previous_columns)\n    if len(previous_values) == 1:\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: previous_values[0].label,\n            current_columns.description: previous_values[0].description,\n        }\n    elif len(previous_values) == 0:\n        raise ValueError(f\"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。\")\n\n    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]\n    sampling_num = min(\n        config[\"hierarchical_merge_labelling\"][\"sampling_num\"],\n        len(current_cluster_data),\n    )\n    sampled_data = current_cluster_data.sample(sampling_num)\n    sampled_argument_text = \"\\n\".join(sampled_data[\"argument\"].values)\n    cluster_text = \"\\n\".join([value.to_prompt_text() for value in previous_values])\n    messages = [\n        {\"role\": \"system\", \"content\": config[\"hierarchical_merge_labelling\"][\"prompt\"]},\n        {\n            \"role\": \"user\",\n            \"content\": \"クラスタラベル\\n\" + cluster_text + \"\\n\" + \"クラスタの意見\\n\" + sampled_argument_text,\n        },\n    ]\n    try:\n        response = request_to_chat_llm(\n            messages=messages,\n            model=config[\"hierarchical_merge_labelling\"][\"model\"],\n            is_json=True,\n        )\n        response_json = json.loads(response)\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: response_json.get(\"label\", \"エラーでラベル名が取得できませんでした\"),\n            current_columns.description: response_json.get(\"description\", \"エラーで解説が取得できませんでした\"),\n        }\n    except Exception as e:\n        print(f\"エラーが発生しました: {e}\")\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: \"エラーでラベル名が取得できませんでした\",\n            current_columns.description: \"エラーで解説が取得できませんでした\",\n        }\n\n\ndef calculate_cluster_density(melted_df: pd.DataFrame, config: dict):\n    \"\"\"クラスタ内の密度計算\"\"\"\n    hierarchical_cluster_df = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n\n    densities = []\n    for level, c_id in zip(melted_df[\"level\"], melted_df[\"id\"], strict=False):\n        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f\"cluster-level-{level}-id\"] == c_id][\n            [\"x\", \"y\"]\n        ].values\n        density = calculate_density(cluster_embeds)\n        densities.append(density)\n\n    # 密度のランクを計算\n    melted_df[\"density\"] = densities\n    melted_df[\"density_rank\"] = melted_df.groupby(\"level\")[\"density\"].rank(ascending=False, method=\"first\")\n    melted_df[\"density_rank_percentile\"] = melted_df.groupby(\"level\")[\"density_rank\"].transform(lambda x: x / len(x))\n    return melted_df\n\n\ndef calculate_density(embeds: np.ndarray):\n    \"\"\"平均距離に基づいて密度を計算\"\"\"\n    center = np.mean(embeds, axis=0)\n    distances = np.linalg.norm(embeds - center, axis=1)\n    avg_distance = np.mean(distances)\n    density = 1 / (avg_distance + 1e-10)\n    return density",
      "prompt": "分割されすぎたクラスタを統合する必要があるので、統合後の名称を考えて出力して。\n\n# 指示\n* 統合前のクラスタの名称・説明および統合後のクラスタに属するデータ点のサンプルを与えるので、これらに基づいて統合後のクラスタの名称を出力してください\n    * 統合後のクラスタ名において、統合前のクラスタ名をそのまま使うことは避けてください。\n* 出力例に記載したJSONのフォーマットに従って出力してください\n\n# サンプルの入出力\n## 入力例1（クラスタラベル:説明文）\n- 地域の災害対応への批判: このクラスタは、地域における災害対応策の実施や体制に対する批判的な意見を集約したものです。住民からは、迅速かつ効果的な支援が行われていない点や、情報提供・連携の不足などに対する強い不満が表明されています。\n- 災害対応への不満: このクラスタは、災害発生時の対応全般に対する不満を示す意見をまとめたものです。救援活動の遅れや支援策の実効性に疑問を持つ声が多く、より積極的で透明性のある対応を求める意見が特徴です。\n- 地域復興の遅れ: このクラスタは、災害後の地域復興プロセスが予定通りに進んでいない点に対する懸念や不満を反映しています。再建計画や支援策の実施の遅延、そしてそれに伴う住民の生活再建への影響が強調されています。\n\n\n## 出力例1\n{{\n    \"label\": \"地域再生と災害支援に対する期待と懸念\",\n    \"description\": \"このクラスタは、特定の地域における再生や災害支援策に対し、具体的な取り組みが不足しているとの意見を集約しています。市民は、選挙を通じた政策議論の中で、地域復興や被災者支援を最優先すべきだとの期待と、現行の支援策に対する改善要求を強く表明しており、より効果的な政府の対応を求める声が反映されています。\"\n}}\n\n\n## 入力例2（クラスタラベル:説明文）\n- 新薬の承認遅延に対する不満:このクラスタは、新薬の承認プロセスが煩雑かつ時間がかかりすぎるという不満の声をまとめたものです。特に、重篤な疾患に苦しむ患者やその家族からは、迅速なアクセスの欠如に対する苛立ちが表明されています。\n- 未治療疾患に対する対応不足:このクラスタは、現時点で有効な治療法が存在しない疾患や、研究開発の進展が遅れている分野に関する課題を指摘する意見を集めています。患者からは、自分たちのニーズが無視されているという感覚が共有されています。\n- 希少疾患への医療アクセスの不平等:このクラスタは、希少疾患を抱える患者が十分な医療資源や治療機会を得られないことに対する不満を中心としています。医療制度の網からこぼれ落ちているという訴えが多く寄せられています。\n\n## 出力例2\n{{\n  \"label\": \"治療機会の格差と革新的医療へのアクセス課題\",\n  \"description\": \"このクラスタは、重篤または希少な疾患に対して、十分な治療法が提供されていない状況や、医療制度における承認プロセスの遅延、アクセス格差に対する不満を集約しています。患者やその家族は、既存の制度が急を要する医療ニーズに対応できていないことに強い懸念を示しており、より迅速かつ公平な医療提供体制の確立を求めています。\"\n}}",
      "model": "gemini-2.0-flash"
    },
    "hierarchical_overview": {
      "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_llm\n\n\ndef hierarchical_overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_overview.txt\"\n\n    hierarchical_label_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_merge_labels.csv\")\n\n    prompt = config[\"hierarchical_overview\"][\"prompt\"]\n    model = config[\"hierarchical_overview\"][\"model\"]\n\n    # TODO: level1で固定にしているが、設定で変えられるようにする\n    target_level = 1\n    target_records = hierarchical_label_df[hierarchical_label_df[\"level\"] == target_level]\n    ids = target_records[\"id\"].to_list()\n    labels = target_records[\"label\"].to_list()\n    descriptions = target_records[\"description\"].to_list()\n    target_records.set_index(\"id\", inplace=True)\n\n    input = \"\"\n    for i, _ in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels[i]}\\n\\n\"\n        input += descriptions[i] + \"\\n\\n\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_llm(messages=messages, model=model)\n\n    with open(path, \"w\", encoding='utf-8') as file:\n        file.write(response)",
      "prompt": "/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。",
      "model": "gemini-2.0-flash"
    },
    "hierarchical_aggregation": {
      "sampling_num": 5000,
      "hidden_properties": {},
      "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import TypedDict\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\n\n\nclass Argument(TypedDict):\n    arg_id: str\n    argument: str\n    comment_id: str\n    x: float\n    y: float\n    p: float\n    cluster_ids: list[str]\n\n\nclass Cluster(TypedDict):\n    level: int\n    id: str\n    label: str\n    takeaway: str\n    value: int\n    parent: str\n    density_rank_percentile: float | None\n\n\ndef hierarchical_aggregation(config):\n    path = f\"outputs/{config['output_dir']}/hierarchical_result.json\"\n    results = {\n        \"arguments\": [],\n        \"clusters\": [],\n        \"comments\": {},\n        \"propertyMap\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index(\"arg-id\", inplace=True)\n    arg_num = len(arguments)\n    relation_df = pd.read_csv(f\"outputs/{config['output_dir']}/relations.csv\")\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_merge_labels.csv\")\n\n    hidden_properties_map: dict[str, list[str]] = config[\"hierarchical_aggregation\"][\"hidden_properties\"]\n\n    results[\"arguments\"] = _build_arguments(clusters)\n    results[\"clusters\"] = _build_cluster_value(labels, arg_num)\n    # NOTE: 属性に応じたコメントフィルタ機能が実装されておらず、全てのコメントが含まれてしまうので、コメントアウト\n    # results[\"comments\"] = _build_comments_value(\n    #     comments, arguments, hidden_properties_map\n    # )\n    results[\"comment_num\"] = len(comments)\n    results[\"translations\"] = _build_translations(config)\n    # 属性情報のカラムは、元データに対して指定したカラムとclassificationするカテゴリを合わせたもの\n    results[\"propertyMap\"] = _build_property_map(arguments, hidden_properties_map, config)\n\n    with open(f\"outputs/{config['output_dir']}/hierarchical_overview.txt\", encoding='utf-8') as f:\n        overview = f.read()\n    print(\"overview\")\n    print(overview)\n    results[\"overview\"] = overview\n\n    with open(path, \"w\", encoding='utf-8') as file:\n        json.dump(results, file, indent=2, ensure_ascii=False)\n    # TODO: サンプリングロジックを実装したいが、現状は全件抽出\n    create_custom_intro(config)\n    if config[\"is_pubcom\"]:\n        add_original_comments(labels, arguments, relation_df, clusters, config)\n\n\ndef create_custom_intro(config):\n    dataset = config[\"output_dir\"]\n    args_path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    result_path = f\"outputs/{dataset}/hierarchical_result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n    processed_num = min(input_count, config[\"extraction\"][\"limit\"])\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n分析対象となったデータの件数は{processed_num}件で、これらのデータに対してOpenAI APIを用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(intro=intro, processed_num=processed_num, args_count=args_count)\n\n    with open(result_path, encoding='utf-8') as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\", encoding='utf-8') as f:\n        json.dump(result, f, indent=2, ensure_ascii=False)\n\n\ndef add_original_comments(labels, arguments, relation_df, clusters, config):\n    # 大カテゴリ（cluster-level-1）に該当するラベルだけ抽出\n    labels_lv1 = labels[labels[\"level\"] == 1][[\"id\", \"label\"]].rename(\n        columns={\"id\": \"cluster-level-1-id\", \"label\": \"category_label\"}\n    )\n\n    # arguments と clusters をマージ（カテゴリ情報付与）\n    merged = arguments.merge(clusters[[\"arg-id\", \"cluster-level-1-id\"]], on=\"arg-id\").merge(\n        labels_lv1, on=\"cluster-level-1-id\", how=\"left\"\n    )\n\n    # relation_df と結合\n    merged = merged.merge(relation_df, on=\"arg-id\", how=\"left\")\n\n    # 元コメント取得\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    comments[\"comment-id\"] = comments[\"comment-id\"].astype(str)\n    merged[\"comment-id\"] = merged[\"comment-id\"].astype(str)\n\n    # 元コメント本文などとマージ\n    final_df = merged.merge(comments, on=\"comment-id\", how=\"left\")\n\n    # 必要カラムのみ整形\n    final_cols = [\"comment-id\", \"comment-body\", \"arg-id\", \"argument\", \"cluster-level-1-id\", \"category_label\"]\n    for col in [\"source\", \"url\"]:\n        if col in comments.columns:\n            final_cols.append(col)\n\n    final_df = final_df[final_cols]\n    final_df = final_df.rename(\n        columns={\n            \"cluster-level-1-id\": \"category_id\",\n            \"category_label\": \"category\",\n            \"arg-id\": \"arg_id\",\n            \"argument\": \"argument\",\n            \"comment-body\": \"original-comment\",\n        }\n    )\n\n    # 保存\n    final_df.to_csv(f\"outputs/{config['output_dir']}/final_result_with_comments.csv\", index=False, encoding='utf-8-sig')\n\n\ndef _build_arguments(clusters: pd.DataFrame) -> list[Argument]:\n    cluster_columns = [col for col in clusters.columns if col.startswith(\"cluster-level-\") and \"id\" in col]\n\n    arguments: list[Argument] = []\n    for _, row in clusters.iterrows():\n        cluster_ids = [\"0\"]\n        for cluster_column in cluster_columns:\n            cluster_ids.append(row[cluster_column])\n        argument: Argument = {\n            \"arg_id\": row[\"arg-id\"],\n            \"argument\": row[\"argument\"],\n            \"x\": row[\"x\"],\n            \"y\": row[\"y\"],\n            \"p\": 0,  # NOTE: 一旦全部0でいれる\n            \"cluster_ids\": cluster_ids,\n        }\n        arguments.append(argument)\n    return arguments\n\n\ndef _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:\n    results: list[Cluster] = [\n        Cluster(\n            level=0,\n            id=\"0\",\n            label=\"全体\",\n            takeaway=\"\",\n            value=total_num,\n            parent=\"\",\n            density_rank_percentile=0,\n        )\n    ]\n\n    for _, melted_label in melted_labels.iterrows():\n        cluster_value = Cluster(\n            level=melted_label[\"level\"],\n            id=melted_label[\"id\"],\n            label=melted_label[\"label\"],\n            takeaway=melted_label[\"description\"],\n            value=melted_label[\"value\"],\n            parent=melted_label.get(\"parent\", \"全体\"),\n            density_rank_percentile=melted_label.get(\"density_rank_percentile\"),\n        )\n        results.append(cluster_value)\n    return results\n\n\n# def _build_comments_value(\n#     comments: pd.DataFrame,\n#     arguments: pd.DataFrame,\n#     hidden_properties_map: dict[str, list[str]],\n# ):\n#     comment_dict: dict[str, dict[str, str]] = {}\n#     useful_comment_ids = set(arguments[\"comment-id\"].values)\n#     for _, row in comments.iterrows():\n#         id = row[\"comment-id\"]\n#         if id in useful_comment_ids:\n#             res = {\"comment\": row[\"comment-body\"]}\n#             should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())\n#             if should_skip:\n#                 continue\n#             comment_dict[str(id)] = res\n\n#     return comment_dict\n\n\ndef _build_translations(config):\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        return json.loads(translations)\n    return {}\n\n\ndef _build_property_map(\n    arguments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict\n) -> dict[str, dict[str, str]]:\n    property_columns = list(hidden_properties_map.keys()) + list(config[\"extraction\"][\"categories\"].keys())\n    property_map = defaultdict(dict)\n\n    # 指定された property_columns が arguments に存在するかチェック\n    missing_cols = [col for col in property_columns if col not in arguments.columns]\n    if missing_cols:\n        raise ValueError(\n            f\"指定されたカラム {missing_cols} が args.csv に存在しません。\"\n            \"設定ファイルaggregation / hidden_propertiesから該当カラムを取り除いてください。\"\n        )\n\n    for prop in property_columns:\n        for arg_id, row in arguments.iterrows():\n            # LLMによるcategory classificationがうまく行かず、NaNの場合はNoneにする\n            property_map[prop][arg_id] = row[prop] if not pd.isna(row[prop]) else None\n    return property_map"
    },
    "plan": [
      {
        "step": "extraction",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "embedding",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "hierarchical_clustering",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "hierarchical_initial_labelling",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "hierarchical_merge_labelling",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "hierarchical_overview",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "hierarchical_aggregation",
        "run": true,
        "reason": "not trace of previous run"
      }
    ],
    "status": "running",
    "start_time": "2025-04-22T13:44:49.175002",
    "completed_jobs": [
      {
        "step": "extraction",
        "completed": "2025-04-22T13:49:31.096206",
        "duration": 281.914202,
        "params": {
          "workers": 3,
          "limit": 200,
          "properties": [],
          "categories": {},
          "category_batch_size": 5,
          "source_code": "import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.category_classification import classify_args\nfrom services.llm import request_to_chat_llm\nfrom services.parse_json_list import parse_response\nfrom hierarchical_utils import update_progress # 前まではbroadlistening.utilsから呼び出していた。これでエラーになったらもとに戻す。\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\ndef _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:\n    if not all(property in comments.columns for property in property_columns):\n        raise ValueError(f\"Properties {property_columns} not found in comments. Columns are {comments.columns}\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n    property_columns = config[\"extraction\"][\"properties\"]\n\n    # カラム名だけを読み込み、必要なカラムが含まれているか確認する\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\", nrows=0)\n    _validate_property_columns(property_columns, comments)\n    # エラーが出なかった場合、すべての行を読み込む\n    comments = pd.read_csv(\n        f\"inputs/{config['input']}.csv\", usecols=[\"comment-id\", \"comment-body\"] + config[\"extraction\"][\"properties\"]\n    )\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    argument_map = {}\n    relation_rows = []\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n\n        for comment_id, extracted_args in zip(batch, batch_results, strict=False):\n            for j, arg in enumerate(extracted_args):\n                if arg not in argument_map:\n                    # argumentテーブルに追加\n                    arg_id = f\"A{comment_id}_{j}\"\n                    argument_map[arg] = {\n                        \"arg-id\": arg_id,\n                        \"argument\": arg,\n                    }\n                else:\n                    arg_id = argument_map[arg][\"arg-id\"]\n\n                # relationテーブルにcommentとargの関係を追加\n                relation_row = {\n                    \"arg-id\": arg_id,\n                    \"comment-id\": comment_id,\n                }\n                relation_rows.append(relation_row)\n\n        update_progress(config, incr=len(batch))\n\n    # DataFrame化\n    results = pd.DataFrame(argument_map.values())\n    relation_df = pd.DataFrame(relation_rows)\n\n    if results.empty:\n        raise RuntimeError(\"result is empty, maybe bad prompt\")\n\n    classification_categories = config[\"extraction\"][\"categories\"]\n    if classification_categories:\n        results = classify_args(results, config, workers)\n\n    results.to_csv(path, index=False)\n    # comment-idとarg-idの関係を保存\n    relation_df.to_csv(f\"outputs/{dataset}/relations.csv\", index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures_with_index = [\n            (i, executor.submit(extract_arguments, input, prompt, model)) for i, input in enumerate(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)\n        results = [[] for _ in range(len(batch))]\n\n        for _, future in futures_with_index:\n            if future in not_done and not future.cancelled():\n                future.cancel()\n\n        for i, future in futures_with_index:\n            if future in done:\n                try:\n                    result = future.result()\n                    results[i] = result\n                except Exception as e:\n                    logging.error(f\"Task {future} failed with error: {e}\")\n                    results[i] = []\n        return results\n\n\n# def extract_by_llm(input, prompt, model):\n#     messages = [\n#         {\"role\": \"system\", \"content\": prompt},\n#         {\"role\": \"user\", \"content\": input},\n#     ]\n#     response = request_to_chat_llm(messages=messages, model=model)\n#     return response\n\n\ndef extract_arguments(input, prompt, model, retries=1):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_llm(messages=messages, model=model, is_json=False)\n        items = parse_response(response)\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []",
          "prompt": "# server/broadlistening/pipeline/prompts/extraction/default.txt の修正\n\n/system\nあなたは専門的なリサーチアシスタントで、与えられたテキストから「要望」「不満」「不安」に関連する意見を抽出する役割です。\n抽出した意見は、それぞれ独立した簡潔な文にしてください。意見が複数ある場合は、すべて抽出してください。\n結果は整形されたJSON形式の文字列リストとして返してください。\n抽出する意見は必ず日本語で作成してください。\n\n/human\nAI技術の進化は目覚ましいが、雇用が奪われるのではないかと心配だし、もっと透明性を高めてほしい。規制も必要だと思う。\n\n/ai\n[\n  \"AIによって雇用が奪われるのではないかという不安がある\",\n  \"AI技術の透明性を高めてほしいという要望がある\",\n  \"AI技術に対する規制が必要だという意見がある\"\n]\n\n/human\n特に問題点は感じていない。現状維持で良い。\n\n/ai\n[]\n\n/human\n新しい公園の計画は素晴らしいが、アクセス道路が狭いのが気になる。子供たちの安全は大丈夫だろうか。もっと広い道路を整備してほしい。\n\n/ai\n[\n  \"新しい公園へのアクセス道路が狭いことへの不満がある\",\n  \"アクセス道路での子供たちの安全に対する不安がある\",\n  \"より広いアクセス道路を整備してほしいという要望がある\"\n]\n\n/human\nサポート体制には満足しているが、もう少し迅速に対応してもらえると助かる。\n\n/ai\n[\n  \"サポートの対応速度をもう少し迅速にしてほしいという要望がある\"\n]\n\n/human\n関節リウマチの治療を続けているけど、副作用がつらくて仕事に支障が出ている。薬の効果はあるけれど、もっと副作用の少ない治療法があればいいのに。将来、歩けなくなるんじゃないかと不安もある。\n\n/ai\n[\n  \"治療薬の副作用によって仕事に支障が出ていることへの不満がある\",\n  \"副作用の少ない治療法を求める要望がある\",\n  \"将来歩けなくなるかもしれないという不安がある\"\n]\n\n/human\n2型糖尿病の治療でGLP-1受容体作動薬を使用しているが、注射が嫌で中断する患者が多い。もっと服薬コンプライアンスを高められる経口薬の選択肢が増えてほしい。また、長期的な心血管リスクへの影響がまだ不透明なのも気になっている。\n\n/ai\n[\n  \"GLP-1受容体作動薬が注射であるために患者の治療継続が困難になることへの不満がある\",\n  \"服薬コンプライアンスを高められる経口薬の選択肢を増やしてほしいという要望がある\",\n  \"GLP-1受容体作動薬の長期的な心血管リスクへの影響が不透明であることに対する不安がある\"\n]\n\n/human\n肺がんの治療で免疫チェックポイント阻害薬を使っているけど、効果が出るかどうか分からないまま高額な費用を払い続けるのがつらい。副作用も人によって違うし、もっと分かりやすい説明があれば安心できるのに。\n\n/ai\n[\n  \"免疫チェックポイント阻害薬の効果が不確実なまま高額な治療費を負担していることへの不満がある\",\n  \"副作用の個人差についての説明が不十分であることへの不安がある\",\n  \"治療に関してもっと分かりやすい説明を求める要望がある\"\n]\n\n/human\n非小細胞肺がんに対する免疫療法は確かに有効例もあるが、効果の予測因子が限定的で、どの患者に効くか判断が難しい。もっと精度の高いバイオマーカーが必要だ。\n\n/ai\n[\n  \"免疫療法の効果を予測する因子が限定的であることへの不満がある\",\n  \"免疫療法の適応を見極めるための精度の高いバイオマーカーが必要だという要望がある\"\n]\n\n/human\n家族がうつ病で治療中だけど、薬を飲んでもなかなか改善しない。通院のたびに仕事を休まないといけないのもつらいし、周囲の理解が足りないと感じる。もっと柔軟な支援制度があればいいのに。\n\n/ai\n[\n  \"抗うつ薬を服用しても改善が見られないことへの不満がある\",\n  \"通院に付き添うために仕事を休まなければならない負担への不満がある\",\n  \"精神疾患への社会的理解が不足していることへの不満がある\",\n  \"より柔軟な支援制度を望む要望がある\"\n]\n\n/human\n精神疾患の早期発見は重要だが、現場では予算や人材が足りていない。スクールカウンセラーや企業内メンタルヘルスの体制をもっと充実させる必要がある。\n\n/ai\n[\n  \"精神疾患対策に必要な予算や人材が不足していることへの不満がある\",\n  \"スクールカウンセラーや企業内メンタルヘルスの体制を充実させる必要があるという要望がある\"\n]\n\n/human\n息子がデュシェンヌ型筋ジストロフィーと診断されたけれど、根本的な治療がまだ確立されていないのが苦しい。臨床試験に参加させたいが、近くの病院では扱っていない。もっと地方にも研究機会を広げてほしい。\n\n/ai\n[\n  \"デュシェンヌ型筋ジストロフィーに根本的な治療法が存在しないことへの不満がある\",\n  \"臨床試験に参加する機会が地域によって限られていることへの不満がある\",\n  \"地方にも研究機会を広げてほしいという要望がある\"\n]\n\n/human\nDMDの治療ではステロイドが主流だが、長期使用による副作用が問題になる。新しい分子標的治療が出てきてはいるが、保険適用が進まず現場で使いにくい。\n\n/ai\n[\n  \"DMD治療におけるステロイドの長期使用で副作用が問題になることへの不満がある\",\n  \"新しい治療法が保険適用されず現場で使いにくいことへの不満がある\"\n]",
          "model": "gemini-2.0-flash"
        }
      },
      {
        "step": "embedding",
        "completed": "2025-04-22T13:49:38.446287",
        "duration": 7.337999,
        "params": {
          "model": "text-embedding-3-small",
          "source_code": "import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model)\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)"
        }
      },
      {
        "step": "hierarchical_clustering",
        "completed": "2025-04-22T13:50:47.713119",
        "duration": 69.256471,
        "params": {
          "cluster_nums": [
            3,
            6,
            12,
            24
          ],
          "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import KMeans\n\n\ndef hierarchical_clustering(config):\n    UMAP = import_module(\"umap\").UMAP\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    cluster_nums = config[\"hierarchical_clustering\"][\"cluster_nums\"]\n\n    n_samples = embeddings_array.shape[0]\n    # デフォルト設定は15\n    default_n_neighbors = 15\n\n    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる\n    if n_samples <= default_n_neighbors:\n        n_neighbors = max(2, n_samples - 1)  # 最低2以上\n    else:\n        n_neighbors = default_n_neighbors\n\n    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)\n    # TODO 詳細エラーメッセージを加える\n    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因\n    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.\n    umap_embeds = umap_model.fit_transform(embeddings_array)\n\n    cluster_results = hierarchical_clustering_embeddings(\n        umap_embeds=umap_embeds,\n        cluster_nums=cluster_nums,\n    )\n    result_df = pd.DataFrame(\n        {\n            \"arg-id\": arguments_df[\"arg-id\"],\n            \"argument\": arguments_df[\"argument\"],\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        }\n    )\n\n    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):\n        result_df[f\"cluster-level-{cluster_level}-id\"] = [f\"{cluster_level}_{label}\" for label in final_labels]\n\n    result_df.to_csv(path, index=False)\n\n\n# def generate_cluster_count_list(min_clusters: int, max_clusters: int):\n#     cluster_counts = []\n#     current = min_clusters\n#     cluster_counts.append(current)\n\n#     if min_clusters == max_clusters:\n#         return cluster_counts\n\n#     while True:\n#         next_double = current * 2\n#         next_triple = current * 3\n\n#         if next_double >= max_clusters:\n#             if cluster_counts[-1] != max_clusters:\n#                 cluster_counts.append(max_clusters)\n#             break\n\n#         # 次の倍はまだ max_clusters に収まるが、3倍だと超える\n#         # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ\n#         if next_triple > max_clusters:\n#             cluster_counts.append(max_clusters)\n#             break\n\n#         cluster_counts.append(next_double)\n#         current = next_double\n\n#     return cluster_counts\n\n\ndef merge_clusters_with_hierarchy(\n    cluster_centers: np.ndarray,\n    kmeans_labels: np.ndarray,\n    umap_array: np.ndarray,\n    n_cluster_cut: int,\n):\n    Z = sch.linkage(cluster_centers, method=\"ward\")\n    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion=\"maxclust\")\n\n    n_samples = umap_array.shape[0]\n    final_labels = np.zeros(n_samples, dtype=int)\n\n    for i in range(n_samples):\n        original_label = kmeans_labels[i]\n        final_labels[i] = cluster_labels_merged[original_label]\n\n    return final_labels\n\n\ndef hierarchical_clustering_embeddings(\n    umap_embeds,\n    cluster_nums,\n):\n    # 最大分割数でクラスタリングを実施\n    print(\"start initial clustering\")\n    initial_cluster_num = cluster_nums[-1]\n    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)\n    kmeans_model.fit(umap_embeds)\n    print(\"end initial clustering\")\n\n    results = {}\n    print(\"start hierarchical clustering\")\n    cluster_nums.sort()\n    print(cluster_nums)\n    for n_cluster_cut in cluster_nums[:-1]:\n        print(\"n_cluster_cut: \", n_cluster_cut)\n        final_labels = merge_clusters_with_hierarchy(\n            cluster_centers=kmeans_model.cluster_centers_,\n            kmeans_labels=kmeans_model.labels_,\n            umap_array=umap_embeds,\n            n_cluster_cut=n_cluster_cut,\n        )\n        results[n_cluster_cut] = final_labels\n\n    results[initial_cluster_num] = kmeans_model.labels_\n    print(\"end hierarchical clustering\")\n\n    return results"
        }
      },
      {
        "step": "hierarchical_initial_labelling",
        "completed": "2025-04-22T13:53:02.268170",
        "duration": 134.534052,
        "params": {
          "sampling_num": 3,
          "workers": 1,
          "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nfrom typing import TypedDict\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_llm\n\n\nclass LabellingResult(TypedDict):\n    \"\"\"各クラスタのラベリング結果を表す型\"\"\"\n\n    cluster_id: str  # クラスタのID\n    label: str  # クラスタのラベル名\n    description: str  # クラスタの説明文\n\n\ndef hierarchical_initial_labelling(config: dict) -> None:\n    \"\"\"階層的クラスタリングの初期ラベリングを実行する\n\n    Args:\n        config: 設定情報を含む辞書\n            - output_dir: 出力ディレクトリ名\n            - hierarchical_initial_labelling: 初期ラベリングの設定\n                - sampling_num: サンプリング数\n                - prompt: LLMへのプロンプト\n                - model: 使用するLLMモデル名\n                - workers: 並列処理のワーカー数\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_initial_labels.csv\"\n    clusters_argument_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_clusters.csv\")\n\n    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_id_column = cluster_id_columns[-1]\n    sampling_num = config[\"hierarchical_initial_labelling\"][\"sampling_num\"]\n    initial_labelling_prompt = config[\"hierarchical_initial_labelling\"][\"prompt\"]\n    model = config[\"hierarchical_initial_labelling\"][\"model\"]\n    workers = config[\"hierarchical_initial_labelling\"][\"workers\"]\n\n    initial_label_df = initial_labelling(\n        initial_labelling_prompt,\n        clusters_argument_df,\n        sampling_num,\n        model,\n        workers,\n    )\n    print(\"start initial labelling\")\n    initial_clusters_argument_df = clusters_argument_df.merge(\n        initial_label_df,\n        left_on=initial_cluster_id_column,\n        right_on=\"cluster_id\",\n        how=\"left\",\n    ).rename(\n        columns={\n            \"label\": f\"{initial_cluster_id_column.replace('-id', '')}-label\",\n            \"description\": f\"{initial_cluster_id_column.replace('-id', '')}-description\",\n        }\n    )\n    print(\"end initial labelling\")\n    initial_clusters_argument_df.to_csv(path, index=False)\n\n\ndef initial_labelling(\n    prompt: str,\n    clusters_df: pd.DataFrame,\n    sampling_num: int,\n    model: str,\n    workers: int,\n) -> pd.DataFrame:\n    \"\"\"各クラスタに対して初期ラベリングを実行する\n\n    Args:\n        prompt: LLMへのプロンプト\n        clusters_df: クラスタリング結果のDataFrame\n        sampling_num: 各クラスタからサンプリングする意見の数\n        model: 使用するLLMモデル名\n        workers: 並列処理のワーカー数\n\n    Returns:\n        各クラスタのラベリング結果を含むDataFrame\n    \"\"\"\n    cluster_columns = [col for col in clusters_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_column = cluster_columns[-1]\n    cluster_ids = clusters_df[initial_cluster_column].unique()\n    process_func = partial(\n        process_initial_labelling,\n        df=clusters_df,\n        prompt=prompt,\n        sampling_num=sampling_num,\n        target_column=initial_cluster_column,\n        model=model,\n    )\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        results = list(executor.map(process_func, cluster_ids))\n    return pd.DataFrame(results)\n\n\ndef process_initial_labelling(\n    cluster_id: str,\n    df: pd.DataFrame,\n    prompt: str,\n    sampling_num: int,\n    target_column: str,\n    model: str,\n) -> LabellingResult:\n    \"\"\"個別のクラスタに対してラベリングを実行する\n\n    Args:\n        cluster_id: 処理対象のクラスタID\n        df: クラスタリング結果のDataFrame\n        prompt: LLMへのプロンプト\n        sampling_num: サンプリングする意見の数\n        target_column: クラスタIDが格納されている列名\n        model: 使用するLLMモデル名\n\n    Returns:\n        クラスタのラベリング結果\n    \"\"\"\n    cluster_data = df[df[target_column] == cluster_id]\n    sampling_num = min(sampling_num, len(cluster_data))\n    cluster = cluster_data.sample(sampling_num)\n    input = \"\\n\".join(cluster[\"argument\"].values)\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_llm(messages=messages, model=model, is_json=True)\n        response_json = json.loads(response)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=response_json.get(\"label\", \"エラーでラベル名が取得できませんでした\"),\n            description=response_json.get(\"description\", \"エラーで解説が取得できませんでした\"),\n        )\n    except Exception as e:\n        print(e)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=\"エラーでラベル名が取得できませんでした\",\n            description=\"エラーで解説が取得できませんでした\",\n        )",
          "prompt": "あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説して、それから表札をつけてください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n\n# サンプルの入出力\n## 入力例1\n最近、政治家が能登の復興に向けた具体的なプランを発表し、地域の未来に明るい希望が見えてきました。市民として、真摯な取り組みに感謝しています。\n災害復興支援が、選挙期間中にしっかり議論されるようになり、政治家が国民の本当のニーズに応える姿勢に期待しています。\n選挙を通じて、政治家が地域振興に全力で取り組む姿勢が伝わってきます。具体的な政策提案を目にするたび、未来への希望が膨らみます。\n\n\n## 出力例1\n{{\n    \"label\": \"市民の未来を支える具体的政策への期待\",\n    \"description\": \"このクラスタには、地域の復興や被害者支援など、実際の社会課題に対して政治家が具体的かつ積極的に取り組む姿勢を支持する前向きな意見が集まっています。市民は、選挙や政策議論を通じて、現実の問題に即した支援策や復興計画が実現されることを期待し、明るい未来の構築に向けた政治の変革を応援しています。\"\n}}\n\n\n## 入力例2\nこの病気は原因がはっきりしないため、治療法も確立されておらず、将来がとても不安です。\n医師の説明が専門的すぎて、自分の状態が正確に理解できませんでした。\n現在の治療法には副作用が多く、生活の質が大きく下がっています。もっと副作用の少ない選択肢が欲しいです。\n同じ病気の患者同士で情報交換できる場がもっとあれば、心の支えになるのにと思います。\n検査の予約が何ヶ月も先で、早期発見や早期治療ができない状況に不満があります。\n\n\n## 出力例2\n{{\n    \"label\": \"疾患への不安と治療環境への改善要望\",\n    \"description\": \"このクラスタには、病気の原因や治療法が明確でないことに起因する将来への不安や、治療中の副作用、説明不足といった医療提供側への不満が集まっています。また、患者同士の交流の場や医療アクセスの改善など、心身両面に配慮した医療環境の整備を求める要望も含まれており、患者視点から見た『納得感のある医療体験』への強いニーズが表れています。\"\n}}",
          "model": "gemini-2.0-flash"
        }
      },
      {
        "step": "hierarchical_merge_labelling",
        "completed": "2025-04-22T13:54:37.196349",
        "duration": 94.914183,
        "params": {
          "sampling_num": 3,
          "workers": 1,
          "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_llm\n\n\n@dataclass\nclass ClusterColumns:\n    \"\"\"同一階層のクラスター関連のカラム名を管理するクラス\"\"\"\n\n    id: str\n    label: str\n    description: str\n\n    @classmethod\n    def from_id_column(cls, id_column: str) -> \"ClusterColumns\":\n        \"\"\"ID列名から関連するカラム名を生成\"\"\"\n        return cls(\n            id=id_column,\n            label=id_column.replace(\"-id\", \"-label\"),\n            description=id_column.replace(\"-id\", \"-description\"),\n        )\n\n\n@dataclass\nclass ClusterValues:\n    \"\"\"対象クラスタのlabel/descriptionを管理するクラス\"\"\"\n\n    label: str\n    description: str\n\n    def to_prompt_text(self) -> str:\n        return f\"- {self.label}: {self.description}\"\n\n\ndef hierarchical_merge_labelling(config: dict) -> None:\n    \"\"\"階層的クラスタリングの結果に対してマージラベリングを実行する\n\n    Args:\n        config: 設定情報を含む辞書\n            - output_dir: 出力ディレクトリ名\n            - hierarchical_merge_labelling: マージラベリングの設定\n                - sampling_num: サンプリング数\n                - prompt: LLMへのプロンプト\n                - model: 使用するLLMモデル名\n                - workers: 並列処理のワーカー数\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    merge_path = f\"outputs/{dataset}/hierarchical_merge_labels.csv\"\n    clusters_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_initial_labels.csv\")\n\n    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)\n    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成\n    merge_result_df = merge_labelling(\n        clusters_df=clusters_df,\n        cluster_id_columns=sorted(cluster_id_columns, reverse=True),\n        config=config,\n    )\n    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成\n    melted_df = melt_cluster_data(merge_result_df)\n    # 上記のdfに親子関係を追加\n    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)\n    melted_df = melted_df.merge(parent_child_df, on=[\"level\", \"id\"], how=\"left\")\n    density_df = calculate_cluster_density(melted_df, config)\n    density_df.to_csv(merge_path, index=False)\n\n\ndef _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):\n    \"\"\"クラスタ間の親子関係をマッピングする\n\n    Args:\n        df: クラスタリング結果のDataFrame\n        cluster_id_columns: クラスタIDのカラム名のリスト\n\n    Returns:\n        親子関係のマッピング情報を含むDataFrame\n    \"\"\"\n    results = []\n    top_cluster_column = cluster_id_columns[0]\n    top_cluster_values = df[top_cluster_column].unique()\n    for c in top_cluster_values:\n        results.append(\n            {\n                \"level\": 1,\n                \"id\": c,\n                \"parent\": \"0\",  # aggregationで追加する全体クラスタのid\n            }\n        )\n\n    for idx in range(len(cluster_id_columns) - 1):\n        current_column = cluster_id_columns[idx]\n        children_column = cluster_id_columns[idx + 1]\n        current_level = current_column.replace(\"-id\", \"\").replace(\"cluster-level-\", \"\")\n        # 現在のレベルのクラスタid\n        current_cluster_values = df[current_column].unique()\n        for current_id in current_cluster_values:\n            children_ids = df.loc[df[current_column] == current_id, children_column].unique()\n            for child_id in children_ids:\n                results.append(\n                    {\n                        \"level\": int(current_level) + 1,\n                        \"id\": child_id,\n                        \"parent\": current_id,\n                    }\n                )\n    return pd.DataFrame(results)\n\n\ndef _filter_id_columns(columns: list[str]) -> list[str]:\n    \"\"\"クラスタIDのカラム名をフィルタリングする\n\n    Args:\n        columns: 全カラム名のリスト\n\n    Returns:\n        クラスタIDのカラム名のリスト\n    \"\"\"\n    return [col for col in columns if col.startswith(\"cluster-level-\") and col.endswith(\"-id\")]\n\n\ndef melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"クラスタデータを行形式に変換する\n\n    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。\n    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。\n\n    Args:\n        df: クラスタリング結果のDataFrame\n\n    Returns:\n        行形式に変換されたDataFrame\n    \"\"\"\n    id_columns: list[str] = _filter_id_columns(df.columns)\n    levels: set[int] = {int(col.replace(\"cluster-level-\", \"\").replace(\"-id\", \"\")) for col in id_columns}\n    all_rows: list[dict] = []\n\n    # levelごとに各クラスタの出現件数を集計・縦持ちにする\n    for level in levels:\n        cluster_columns = ClusterColumns.from_id_column(f\"cluster-level-{level}-id\")\n        # クラスタidごとの件数集計\n        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name=\"value\")\n\n        level_unique_val_df = df[\n            [cluster_columns.id, cluster_columns.label, cluster_columns.description]\n        ].drop_duplicates()\n        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how=\"left\")\n        level_unique_vals = [\n            {\n                \"level\": level,\n                \"id\": row[cluster_columns.id],\n                \"label\": row[cluster_columns.label],\n                \"description\": row[cluster_columns.description],\n                \"value\": row[\"value\"],\n            }\n            for _, row in level_unique_val_df.iterrows()\n        ]\n        all_rows.extend(level_unique_vals)\n    return pd.DataFrame(all_rows)\n\n\ndef merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:\n    \"\"\"階層的なクラスタのマージラベリングを実行する\n\n    Args:\n        clusters_df: クラスタリング結果のDataFrame\n        cluster_id_columns: クラスタIDのカラム名のリスト\n        config: 設定情報を含む辞書\n\n    Returns:\n        マージラベリング結果を含むDataFrame\n    \"\"\"\n    for idx in tqdm(range(len(cluster_id_columns) - 1)):\n        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])\n        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])\n\n        process_fn = partial(\n            process_merge_labelling,\n            result_df=clusters_df,\n            current_columns=current_columns,\n            previous_columns=previous_columns,\n            config=config,\n        )\n\n        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())\n        with ThreadPoolExecutor(max_workers=config[\"hierarchical_merge_labelling\"][\"workers\"]) as executor:\n            responses = list(\n                tqdm(\n                    executor.map(process_fn, current_cluster_ids),\n                    total=len(current_cluster_ids),\n                )\n            )\n\n        current_result_df = pd.DataFrame(responses)\n        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])\n    return clusters_df\n\n\ndef process_merge_labelling(\n    target_cluster_id: str,\n    result_df: pd.DataFrame,\n    current_columns: ClusterColumns,\n    previous_columns: ClusterColumns,\n    config,\n):\n    \"\"\"個別のクラスタに対してマージラベリングを実行する\n\n    Args:\n        target_cluster_id: 処理対象のクラスタID\n        result_df: クラスタリング結果のDataFrame\n        current_columns: 現在のレベルのカラム情報\n        previous_columns: 前のレベルのカラム情報\n        config: 設定情報を含む辞書\n\n    Returns:\n        マージラベリング結果を含む辞書\n    \"\"\"\n\n    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:\n        \"\"\"前のレベルのクラスタ情報を取得する\"\"\"\n        previous_records = df[df[current_columns.id] == target_cluster_id][\n            [previous_columns.label, previous_columns.description]\n        ].drop_duplicates()\n        previous_values = [\n            ClusterValues(\n                label=row[previous_columns.label],\n                description=row[previous_columns.description],\n            )\n            for _, row in previous_records.iterrows()\n        ]\n        return previous_values\n\n    previous_values = filter_previous_values(result_df, previous_columns)\n    if len(previous_values) == 1:\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: previous_values[0].label,\n            current_columns.description: previous_values[0].description,\n        }\n    elif len(previous_values) == 0:\n        raise ValueError(f\"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。\")\n\n    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]\n    sampling_num = min(\n        config[\"hierarchical_merge_labelling\"][\"sampling_num\"],\n        len(current_cluster_data),\n    )\n    sampled_data = current_cluster_data.sample(sampling_num)\n    sampled_argument_text = \"\\n\".join(sampled_data[\"argument\"].values)\n    cluster_text = \"\\n\".join([value.to_prompt_text() for value in previous_values])\n    messages = [\n        {\"role\": \"system\", \"content\": config[\"hierarchical_merge_labelling\"][\"prompt\"]},\n        {\n            \"role\": \"user\",\n            \"content\": \"クラスタラベル\\n\" + cluster_text + \"\\n\" + \"クラスタの意見\\n\" + sampled_argument_text,\n        },\n    ]\n    try:\n        response = request_to_chat_llm(\n            messages=messages,\n            model=config[\"hierarchical_merge_labelling\"][\"model\"],\n            is_json=True,\n        )\n        response_json = json.loads(response)\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: response_json.get(\"label\", \"エラーでラベル名が取得できませんでした\"),\n            current_columns.description: response_json.get(\"description\", \"エラーで解説が取得できませんでした\"),\n        }\n    except Exception as e:\n        print(f\"エラーが発生しました: {e}\")\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: \"エラーでラベル名が取得できませんでした\",\n            current_columns.description: \"エラーで解説が取得できませんでした\",\n        }\n\n\ndef calculate_cluster_density(melted_df: pd.DataFrame, config: dict):\n    \"\"\"クラスタ内の密度計算\"\"\"\n    hierarchical_cluster_df = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n\n    densities = []\n    for level, c_id in zip(melted_df[\"level\"], melted_df[\"id\"], strict=False):\n        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f\"cluster-level-{level}-id\"] == c_id][\n            [\"x\", \"y\"]\n        ].values\n        density = calculate_density(cluster_embeds)\n        densities.append(density)\n\n    # 密度のランクを計算\n    melted_df[\"density\"] = densities\n    melted_df[\"density_rank\"] = melted_df.groupby(\"level\")[\"density\"].rank(ascending=False, method=\"first\")\n    melted_df[\"density_rank_percentile\"] = melted_df.groupby(\"level\")[\"density_rank\"].transform(lambda x: x / len(x))\n    return melted_df\n\n\ndef calculate_density(embeds: np.ndarray):\n    \"\"\"平均距離に基づいて密度を計算\"\"\"\n    center = np.mean(embeds, axis=0)\n    distances = np.linalg.norm(embeds - center, axis=1)\n    avg_distance = np.mean(distances)\n    density = 1 / (avg_distance + 1e-10)\n    return density",
          "prompt": "分割されすぎたクラスタを統合する必要があるので、統合後の名称を考えて出力して。\n\n# 指示\n* 統合前のクラスタの名称・説明および統合後のクラスタに属するデータ点のサンプルを与えるので、これらに基づいて統合後のクラスタの名称を出力してください\n    * 統合後のクラスタ名において、統合前のクラスタ名をそのまま使うことは避けてください。\n* 出力例に記載したJSONのフォーマットに従って出力してください\n\n# サンプルの入出力\n## 入力例1（クラスタラベル:説明文）\n- 地域の災害対応への批判: このクラスタは、地域における災害対応策の実施や体制に対する批判的な意見を集約したものです。住民からは、迅速かつ効果的な支援が行われていない点や、情報提供・連携の不足などに対する強い不満が表明されています。\n- 災害対応への不満: このクラスタは、災害発生時の対応全般に対する不満を示す意見をまとめたものです。救援活動の遅れや支援策の実効性に疑問を持つ声が多く、より積極的で透明性のある対応を求める意見が特徴です。\n- 地域復興の遅れ: このクラスタは、災害後の地域復興プロセスが予定通りに進んでいない点に対する懸念や不満を反映しています。再建計画や支援策の実施の遅延、そしてそれに伴う住民の生活再建への影響が強調されています。\n\n\n## 出力例1\n{{\n    \"label\": \"地域再生と災害支援に対する期待と懸念\",\n    \"description\": \"このクラスタは、特定の地域における再生や災害支援策に対し、具体的な取り組みが不足しているとの意見を集約しています。市民は、選挙を通じた政策議論の中で、地域復興や被災者支援を最優先すべきだとの期待と、現行の支援策に対する改善要求を強く表明しており、より効果的な政府の対応を求める声が反映されています。\"\n}}\n\n\n## 入力例2（クラスタラベル:説明文）\n- 新薬の承認遅延に対する不満:このクラスタは、新薬の承認プロセスが煩雑かつ時間がかかりすぎるという不満の声をまとめたものです。特に、重篤な疾患に苦しむ患者やその家族からは、迅速なアクセスの欠如に対する苛立ちが表明されています。\n- 未治療疾患に対する対応不足:このクラスタは、現時点で有効な治療法が存在しない疾患や、研究開発の進展が遅れている分野に関する課題を指摘する意見を集めています。患者からは、自分たちのニーズが無視されているという感覚が共有されています。\n- 希少疾患への医療アクセスの不平等:このクラスタは、希少疾患を抱える患者が十分な医療資源や治療機会を得られないことに対する不満を中心としています。医療制度の網からこぼれ落ちているという訴えが多く寄せられています。\n\n## 出力例2\n{{\n  \"label\": \"治療機会の格差と革新的医療へのアクセス課題\",\n  \"description\": \"このクラスタは、重篤または希少な疾患に対して、十分な治療法が提供されていない状況や、医療制度における承認プロセスの遅延、アクセス格差に対する不満を集約しています。患者やその家族は、既存の制度が急を要する医療ニーズに対応できていないことに強い懸念を示しており、より迅速かつ公平な医療提供体制の確立を求めています。\"\n}}",
          "model": "gemini-2.0-flash"
        }
      },
      {
        "step": "hierarchical_overview",
        "completed": "2025-04-22T13:54:42.450573",
        "duration": 5.238224,
        "params": {
          "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_llm\n\n\ndef hierarchical_overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_overview.txt\"\n\n    hierarchical_label_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_merge_labels.csv\")\n\n    prompt = config[\"hierarchical_overview\"][\"prompt\"]\n    model = config[\"hierarchical_overview\"][\"model\"]\n\n    # TODO: level1で固定にしているが、設定で変えられるようにする\n    target_level = 1\n    target_records = hierarchical_label_df[hierarchical_label_df[\"level\"] == target_level]\n    ids = target_records[\"id\"].to_list()\n    labels = target_records[\"label\"].to_list()\n    descriptions = target_records[\"description\"].to_list()\n    target_records.set_index(\"id\", inplace=True)\n\n    input = \"\"\n    for i, _ in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels[i]}\\n\\n\"\n        input += descriptions[i] + \"\\n\\n\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_llm(messages=messages, model=model)\n\n    with open(path, \"w\", encoding='utf-8') as file:\n        file.write(response)",
          "prompt": "/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。",
          "model": "gemini-2.0-flash"
        }
      }
    ],
    "lock_until": "2025-04-22T13:59:42.467571",
    "current_job": "hierarchical_aggregation",
    "current_job_started": "2025-04-22T13:54:42.467571",
    "current_job_progress": null,
    "current_jop_tasks": null
  },
  "comment_num": 342
}