{
  "name": "Recursive Public, Agenda Setting",
  "question": "\u4eba\u985e\u304c\u4eba\u5de5\u77e5\u80fd\u3092\u958b\u767a\u30fb\u5c55\u958b\u3059\u308b\u4e0a\u3067\u3001\u6700\u512a\u5148\u3059\u3079\u304d\u8ab2\u984c\u306f\u4f55\u3067\u3057\u3087\u3046\u304b\uff1f",
  "input": "example-polis",
  "model": "gpt-4o",
  "extraction": {
    "workers": 3,
    "limit": 200,
    "properties": [],
    "categories": {},
    "category_batch_size": 5,
    "source_code": "import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.category_classification import classify_args\nfrom services.llm import request_to_chat_openai\nfrom services.parse_json_list import parse_response\nfrom hierarchical_utils import update_progress # \u524d\u307e\u3067\u306fbroadlistening.utils\u304b\u3089\u547c\u3073\u51fa\u3057\u3066\u3044\u305f\u3002\u3053\u308c\u3067\u30a8\u30e9\u30fc\u306b\u306a\u3063\u305f\u3089\u3082\u3068\u306b\u623b\u3059\u3002\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\ndef _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:\n    if not all(property in comments.columns for property in property_columns):\n        raise ValueError(f\"Properties {property_columns} not found in comments. Columns are {comments.columns}\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n    property_columns = config[\"extraction\"][\"properties\"]\n\n    # \u30ab\u30e9\u30e0\u540d\u3060\u3051\u3092\u8aad\u307f\u8fbc\u307f\u3001\u5fc5\u8981\u306a\u30ab\u30e9\u30e0\u304c\u542b\u307e\u308c\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\", nrows=0)\n    _validate_property_columns(property_columns, comments)\n    # \u30a8\u30e9\u30fc\u304c\u51fa\u306a\u304b\u3063\u305f\u5834\u5408\u3001\u3059\u3079\u3066\u306e\u884c\u3092\u8aad\u307f\u8fbc\u3080\n    comments = pd.read_csv(\n        f\"inputs/{config['input']}.csv\", usecols=[\"comment-id\", \"comment-body\"] + config[\"extraction\"][\"properties\"]\n    )\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    argument_map = {}\n    relation_rows = []\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n\n        for comment_id, extracted_args in zip(batch, batch_results, strict=False):\n            for j, arg in enumerate(extracted_args):\n                if arg not in argument_map:\n                    # argument\u30c6\u30fc\u30d6\u30eb\u306b\u8ffd\u52a0\n                    arg_id = f\"A{comment_id}_{j}\"\n                    argument_map[arg] = {\n                        \"arg-id\": arg_id,\n                        \"argument\": arg,\n                    }\n                else:\n                    arg_id = argument_map[arg][\"arg-id\"]\n\n                # relation\u30c6\u30fc\u30d6\u30eb\u306bcomment\u3068arg\u306e\u95a2\u4fc2\u3092\u8ffd\u52a0\n                relation_row = {\n                    \"arg-id\": arg_id,\n                    \"comment-id\": comment_id,\n                }\n                relation_rows.append(relation_row)\n\n        update_progress(config, incr=len(batch))\n\n    # DataFrame\u5316\n    results = pd.DataFrame(argument_map.values())\n    relation_df = pd.DataFrame(relation_rows)\n\n    if results.empty:\n        raise RuntimeError(\"result is empty, maybe bad prompt\")\n\n    classification_categories = config[\"extraction\"][\"categories\"]\n    if classification_categories:\n        results = classify_args(results, config, workers)\n\n    results.to_csv(path, index=False)\n    # comment-id\u3068arg-id\u306e\u95a2\u4fc2\u3092\u4fdd\u5b58\n    relation_df.to_csv(f\"outputs/{dataset}/relations.csv\", index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures_with_index = [\n            (i, executor.submit(extract_arguments, input, prompt, model)) for i, input in enumerate(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)\n        results = [[] for _ in range(len(batch))]\n\n        for _, future in futures_with_index:\n            if future in not_done and not future.cancelled():\n                future.cancel()\n\n        for i, future in futures_with_index:\n            if future in done:\n                try:\n                    result = future.result()\n                    results[i] = result\n                except Exception as e:\n                    logging.error(f\"Task {future} failed with error: {e}\")\n                    results[i] = []\n        return results\n\n\n# def extract_by_llm(input, prompt, model):\n#     messages = [\n#         {\"role\": \"system\", \"content\": prompt},\n#         {\"role\": \"user\", \"content\": input},\n#     ]\n#     response = request_to_chat_openai(messages=messages, model=model)\n#     return response\n\n\ndef extract_arguments(input, prompt, model, retries=1):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model, is_json=False)\n        items = parse_response(response)\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []",
    "prompt": "# server/broadlistening/pipeline/prompts/extraction/default.txt \u306e\u4fee\u6b63\n\n/system\n\u3042\u306a\u305f\u306f\u5c02\u9580\u7684\u306a\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3001\u4e0e\u3048\u3089\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u300c\u8981\u671b\u300d\u300c\u4e0d\u6e80\u300d\u300c\u4e0d\u5b89\u300d\u306b\u95a2\u9023\u3059\u308b\u610f\u898b\u3092\u62bd\u51fa\u3059\u308b\u5f79\u5272\u3067\u3059\u3002\n\u62bd\u51fa\u3057\u305f\u610f\u898b\u306f\u3001\u305d\u308c\u305e\u308c\u72ec\u7acb\u3057\u305f\u7c21\u6f54\u306a\u6587\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u610f\u898b\u304c\u8907\u6570\u3042\u308b\u5834\u5408\u306f\u3001\u3059\u3079\u3066\u62bd\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u7d50\u679c\u306f\u6574\u5f62\u3055\u308c\u305fJSON\u5f62\u5f0f\u306e\u6587\u5b57\u5217\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u62bd\u51fa\u3059\u308b\u610f\u898b\u306f\u5fc5\u305a\u65e5\u672c\u8a9e\u3067\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n/human\nAI\u6280\u8853\u306e\u9032\u5316\u306f\u76ee\u899a\u307e\u3057\u3044\u304c\u3001\u96c7\u7528\u304c\u596a\u308f\u308c\u308b\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u5fc3\u914d\u3060\u3057\u3001\u3082\u3063\u3068\u900f\u660e\u6027\u3092\u9ad8\u3081\u3066\u307b\u3057\u3044\u3002\u898f\u5236\u3082\u5fc5\u8981\u3060\u3068\u601d\u3046\u3002\n\n/ai\n[\n  \"AI\u306b\u3088\u3063\u3066\u96c7\u7528\u304c\u596a\u308f\u308c\u308b\u306e\u3067\u306f\u306a\u3044\u304b\u3068\u3044\u3046\u4e0d\u5b89\u304c\u3042\u308b\",\n  \"AI\u6280\u8853\u306e\u900f\u660e\u6027\u3092\u9ad8\u3081\u3066\u307b\u3057\u3044\u3068\u3044\u3046\u8981\u671b\u304c\u3042\u308b\",\n  \"AI\u6280\u8853\u306b\u5bfe\u3059\u308b\u898f\u5236\u304c\u5fc5\u8981\u3060\u3068\u3044\u3046\u610f\u898b\u304c\u3042\u308b\"\n]\n\n/human\n\u518d\u751f\u53ef\u80fd\u30a8\u30cd\u30eb\u30ae\u30fc\u3078\u306e\u6295\u8cc7\u306f\u826f\u3044\u304c\u3001\u30b3\u30b9\u30c8\u304c\u304b\u304b\u308a\u3059\u304e\u308b\u3002\u3082\u3063\u3068\u52b9\u7387\u7684\u306a\u65b9\u6cd5\u306f\u306a\u3044\u306e\u304b\u3002\n\n/ai\n[\n  \"\u518d\u751f\u53ef\u80fd\u30a8\u30cd\u30eb\u30ae\u30fc\u3078\u306e\u6295\u8cc7\u30b3\u30b9\u30c8\u304c\u9ad8\u3059\u304e\u308b\u3053\u3068\u3078\u306e\u4e0d\u6e80\u304c\u3042\u308b\",\n  \"\u518d\u751f\u53ef\u80fd\u30a8\u30cd\u30eb\u30ae\u30fc\u306e\u3088\u308a\u52b9\u7387\u7684\u306a\u65b9\u6cd5\u3092\u6c42\u3081\u308b\u8981\u671b\u304c\u3042\u308b\"\n]\n\n/human\n\u7279\u306b\u554f\u984c\u70b9\u306f\u611f\u3058\u3066\u3044\u306a\u3044\u3002\u73fe\u72b6\u7dad\u6301\u3067\u826f\u3044\u3002\n\n/ai\n[]\n\n/human\n\u65b0\u3057\u3044\u516c\u5712\u306e\u8a08\u753b\u306f\u7d20\u6674\u3089\u3057\u3044\u304c\u3001\u30a2\u30af\u30bb\u30b9\u9053\u8def\u304c\u72ed\u3044\u306e\u304c\u6c17\u306b\u306a\u308b\u3002\u5b50\u4f9b\u305f\u3061\u306e\u5b89\u5168\u306f\u5927\u4e08\u592b\u3060\u308d\u3046\u304b\u3002\u3082\u3063\u3068\u5e83\u3044\u9053\u8def\u3092\u6574\u5099\u3057\u3066\u307b\u3057\u3044\u3002\n\n/ai\n[\n  \"\u65b0\u3057\u3044\u516c\u5712\u3078\u306e\u30a2\u30af\u30bb\u30b9\u9053\u8def\u304c\u72ed\u3044\u3053\u3068\u3078\u306e\u4e0d\u6e80\u304c\u3042\u308b\",\n  \"\u30a2\u30af\u30bb\u30b9\u9053\u8def\u3067\u306e\u5b50\u4f9b\u305f\u3061\u306e\u5b89\u5168\u306b\u5bfe\u3059\u308b\u4e0d\u5b89\u304c\u3042\u308b\",\n  \"\u3088\u308a\u5e83\u3044\u30a2\u30af\u30bb\u30b9\u9053\u8def\u3092\u6574\u5099\u3057\u3066\u307b\u3057\u3044\u3068\u3044\u3046\u8981\u671b\u304c\u3042\u308b\"\n]\n\n/human\n\u30b5\u30dd\u30fc\u30c8\u4f53\u5236\u306b\u306f\u6e80\u8db3\u3057\u3066\u3044\u308b\u304c\u3001\u3082\u3046\u5c11\u3057\u8fc5\u901f\u306b\u5bfe\u5fdc\u3057\u3066\u3082\u3089\u3048\u308b\u3068\u52a9\u304b\u308b\u3002\n\n/ai\n[\n  \"\u30b5\u30dd\u30fc\u30c8\u306e\u5bfe\u5fdc\u901f\u5ea6\u3092\u3082\u3046\u5c11\u3057\u8fc5\u901f\u306b\u3057\u3066\u307b\u3057\u3044\u3068\u3044\u3046\u8981\u671b\u304c\u3042\u308b\"\n]",
    "model": "gpt-4o"
  },
  "hierarchical_clustering": {
    "cluster_nums": [
      3,
      6,
      12,
      24
    ],
    "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import KMeans\n\n\ndef hierarchical_clustering(config):\n    UMAP = import_module(\"umap\").UMAP\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    cluster_nums = config[\"hierarchical_clustering\"][\"cluster_nums\"]\n\n    n_samples = embeddings_array.shape[0]\n    # \u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u306f15\n    default_n_neighbors = 15\n\n    # \u30c6\u30b9\u30c8\u7b49\u30b5\u30f3\u30d7\u30eb\u304c\u5c11\u306a\u3059\u304e\u308b\u5834\u5408\u3001n_neighbors\u306e\u8a2d\u5b9a\u5024\u3092\u4e0b\u3052\u308b\n    if n_samples <= default_n_neighbors:\n        n_neighbors = max(2, n_samples - 1)  # \u6700\u4f4e2\u4ee5\u4e0a\n    else:\n        n_neighbors = default_n_neighbors\n\n    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)\n    # TODO \u8a73\u7d30\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u52a0\u3048\u308b\n    # \u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u306e\u5834\u5408\u3001\u304a\u305d\u3089\u304f\u5143\u306e\u610f\u898b\u4ef6\u6570\u304c\u5c11\u306a\u3059\u304e\u308b\u3053\u3068\u304c\u539f\u56e0\n    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.\n    umap_embeds = umap_model.fit_transform(embeddings_array)\n\n    cluster_results = hierarchical_clustering_embeddings(\n        umap_embeds=umap_embeds,\n        cluster_nums=cluster_nums,\n    )\n    result_df = pd.DataFrame(\n        {\n            \"arg-id\": arguments_df[\"arg-id\"],\n            \"argument\": arguments_df[\"argument\"],\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        }\n    )\n\n    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):\n        result_df[f\"cluster-level-{cluster_level}-id\"] = [f\"{cluster_level}_{label}\" for label in final_labels]\n\n    result_df.to_csv(path, index=False)\n\n\n# def generate_cluster_count_list(min_clusters: int, max_clusters: int):\n#     cluster_counts = []\n#     current = min_clusters\n#     cluster_counts.append(current)\n\n#     if min_clusters == max_clusters:\n#         return cluster_counts\n\n#     while True:\n#         next_double = current * 2\n#         next_triple = current * 3\n\n#         if next_double >= max_clusters:\n#             if cluster_counts[-1] != max_clusters:\n#                 cluster_counts.append(max_clusters)\n#             break\n\n#         # \u6b21\u306e\u500d\u306f\u307e\u3060 max_clusters \u306b\u53ce\u307e\u308b\u304c\u30013\u500d\u3060\u3068\u8d85\u3048\u308b\n#         # -> (\u6b21\u306e\u500d\u306f\u7d30\u304b\u3059\u304e\u308b\u306e\u3067)\u30b9\u30ad\u30c3\u30d7\u3057\u3066 max_clusters \u306b\u98db\u3076\n#         if next_triple > max_clusters:\n#             cluster_counts.append(max_clusters)\n#             break\n\n#         cluster_counts.append(next_double)\n#         current = next_double\n\n#     return cluster_counts\n\n\ndef merge_clusters_with_hierarchy(\n    cluster_centers: np.ndarray,\n    kmeans_labels: np.ndarray,\n    umap_array: np.ndarray,\n    n_cluster_cut: int,\n):\n    Z = sch.linkage(cluster_centers, method=\"ward\")\n    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion=\"maxclust\")\n\n    n_samples = umap_array.shape[0]\n    final_labels = np.zeros(n_samples, dtype=int)\n\n    for i in range(n_samples):\n        original_label = kmeans_labels[i]\n        final_labels[i] = cluster_labels_merged[original_label]\n\n    return final_labels\n\n\ndef hierarchical_clustering_embeddings(\n    umap_embeds,\n    cluster_nums,\n):\n    # \u6700\u5927\u5206\u5272\u6570\u3067\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u5b9f\u65bd\n    print(\"start initial clustering\")\n    initial_cluster_num = cluster_nums[-1]\n    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)\n    kmeans_model.fit(umap_embeds)\n    print(\"end initial clustering\")\n\n    results = {}\n    print(\"start hierarchical clustering\")\n    cluster_nums.sort()\n    print(cluster_nums)\n    for n_cluster_cut in cluster_nums[:-1]:\n        print(\"n_cluster_cut: \", n_cluster_cut)\n        final_labels = merge_clusters_with_hierarchy(\n            cluster_centers=kmeans_model.cluster_centers_,\n            kmeans_labels=kmeans_model.labels_,\n            umap_array=umap_embeds,\n            n_cluster_cut=n_cluster_cut,\n        )\n        results[n_cluster_cut] = final_labels\n\n    results[initial_cluster_num] = kmeans_model.labels_\n    print(\"end hierarchical clustering\")\n\n    return results"
  },
  "intro": "\u3053\u306eAI\u751f\u6210\u30ec\u30dd\u30fc\u30c8\u306f\u3001Recursive Public\u30c1\u30fc\u30e0\u304c\u5b9f\u65bd\u3057\u305fPolis\u5354\u8b70\u306e\u30c7\u30fc\u30bf\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002",
  "output_dir": "hierarchical-example-polis",
  "embedding": {
    "model": "text-embedding-3-small",
    "source_code": "import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model)\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)"
  },
  "hierarchical_initial_labelling": {
    "sampling_num": 3,
    "workers": 1,
    "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nfrom typing import TypedDict\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_openai\n\n\nclass LabellingResult(TypedDict):\n    \"\"\"\u5404\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u8868\u3059\u578b\"\"\"\n\n    cluster_id: str  # \u30af\u30e9\u30b9\u30bf\u306eID\n    label: str  # \u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u540d\n    description: str  # \u30af\u30e9\u30b9\u30bf\u306e\u8aac\u660e\u6587\n\n\ndef hierarchical_initial_labelling(config: dict) -> None:\n    \"\"\"\u968e\u5c64\u7684\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306e\u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n            - output_dir: \u51fa\u529b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\n            - hierarchical_initial_labelling: \u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u306e\u8a2d\u5b9a\n                - sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6570\n                - prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n                - model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n                - workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_initial_labels.csv\"\n    clusters_argument_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_clusters.csv\")\n\n    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_id_column = cluster_id_columns[-1]\n    sampling_num = config[\"hierarchical_initial_labelling\"][\"sampling_num\"]\n    initial_labelling_prompt = config[\"hierarchical_initial_labelling\"][\"prompt\"]\n    model = config[\"hierarchical_initial_labelling\"][\"model\"]\n    workers = config[\"hierarchical_initial_labelling\"][\"workers\"]\n\n    initial_label_df = initial_labelling(\n        initial_labelling_prompt,\n        clusters_argument_df,\n        sampling_num,\n        model,\n        workers,\n    )\n    print(\"start initial labelling\")\n    initial_clusters_argument_df = clusters_argument_df.merge(\n        initial_label_df,\n        left_on=initial_cluster_id_column,\n        right_on=\"cluster_id\",\n        how=\"left\",\n    ).rename(\n        columns={\n            \"label\": f\"{initial_cluster_id_column.replace('-id', '')}-label\",\n            \"description\": f\"{initial_cluster_id_column.replace('-id', '')}-description\",\n        }\n    )\n    print(\"end initial labelling\")\n    initial_clusters_argument_df.to_csv(path, index=False)\n\n\ndef initial_labelling(\n    prompt: str,\n    clusters_df: pd.DataFrame,\n    sampling_num: int,\n    model: str,\n    workers: int,\n) -> pd.DataFrame:\n    \"\"\"\u5404\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n        clusters_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        sampling_num: \u5404\u30af\u30e9\u30b9\u30bf\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u610f\u898b\u306e\u6570\n        model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n        workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n\n    Returns:\n        \u5404\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080DataFrame\n    \"\"\"\n    cluster_columns = [col for col in clusters_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_column = cluster_columns[-1]\n    cluster_ids = clusters_df[initial_cluster_column].unique()\n    process_func = partial(\n        process_initial_labelling,\n        df=clusters_df,\n        prompt=prompt,\n        sampling_num=sampling_num,\n        target_column=initial_cluster_column,\n        model=model,\n    )\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        results = list(executor.map(process_func, cluster_ids))\n    return pd.DataFrame(results)\n\n\ndef process_initial_labelling(\n    cluster_id: str,\n    df: pd.DataFrame,\n    prompt: str,\n    sampling_num: int,\n    target_column: str,\n    model: str,\n) -> LabellingResult:\n    \"\"\"\u500b\u5225\u306e\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        cluster_id: \u51e6\u7406\u5bfe\u8c61\u306e\u30af\u30e9\u30b9\u30bfID\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n        sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u610f\u898b\u306e\u6570\n        target_column: \u30af\u30e9\u30b9\u30bfID\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u5217\u540d\n        model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n\n    Returns:\n        \u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\n    \"\"\"\n    cluster_data = df[df[target_column] == cluster_id]\n    sampling_num = min(sampling_num, len(cluster_data))\n    cluster = cluster_data.sample(sampling_num)\n    input = \"\\n\".join(cluster[\"argument\"].values)\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model, is_json=True)\n        response_json = json.loads(response)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=response_json.get(\"label\", \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n            description=response_json.get(\"description\", \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n        )\n    except Exception as e:\n        print(e)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=\"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n            description=\"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n        )",
    "prompt": "\u3042\u306a\u305f\u306fKJ\u6cd5\u304c\u5f97\u610f\u306a\u30c7\u30fc\u30bf\u5206\u6790\u8005\u3067\u3059\u3002user\u306einput\u306f\u30b0\u30eb\u30fc\u30d7\u306b\u96c6\u307e\u3063\u305f\u30e9\u30d9\u30eb\u3067\u3059\u3002\u306a\u305c\u305d\u306e\u30e9\u30d9\u30eb\u304c\u4e00\u3064\u306e\u30b0\u30eb\u30fc\u30d7\u3067\u3042\u308b\u304b\u89e3\u8aac\u3057\u3066\u3001\u305d\u308c\u304b\u3089\u8868\u672d\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306fJSON\u3068\u3057\u3001\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306f\u4ee5\u4e0b\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\n# \u30b5\u30f3\u30d7\u30eb\u306e\u5165\u51fa\u529b\n## \u5165\u529b\u4f8b\n\u6700\u8fd1\u3001\u653f\u6cbb\u5bb6\u304c\u80fd\u767b\u306e\u5fa9\u8208\u306b\u5411\u3051\u305f\u5177\u4f53\u7684\u306a\u30d7\u30e9\u30f3\u3092\u767a\u8868\u3057\u3001\u5730\u57df\u306e\u672a\u6765\u306b\u660e\u308b\u3044\u5e0c\u671b\u304c\u898b\u3048\u3066\u304d\u307e\u3057\u305f\u3002\u5e02\u6c11\u3068\u3057\u3066\u3001\u771f\u646f\u306a\u53d6\u308a\u7d44\u307f\u306b\u611f\u8b1d\u3057\u3066\u3044\u307e\u3059\u3002\n\u707d\u5bb3\u5fa9\u8208\u652f\u63f4\u304c\u3001\u9078\u6319\u671f\u9593\u4e2d\u306b\u3057\u3063\u304b\u308a\u8b70\u8ad6\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u308a\u3001\u653f\u6cbb\u5bb6\u304c\u56fd\u6c11\u306e\u672c\u5f53\u306e\u30cb\u30fc\u30ba\u306b\u5fdc\u3048\u308b\u59ff\u52e2\u306b\u671f\u5f85\u3057\u3066\u3044\u307e\u3059\u3002\n\u9078\u6319\u3092\u901a\u3058\u3066\u3001\u653f\u6cbb\u5bb6\u304c\u5730\u57df\u632f\u8208\u306b\u5168\u529b\u3067\u53d6\u308a\u7d44\u3080\u59ff\u52e2\u304c\u4f1d\u308f\u3063\u3066\u304d\u307e\u3059\u3002\u5177\u4f53\u7684\u306a\u653f\u7b56\u63d0\u6848\u3092\u76ee\u306b\u3059\u308b\u305f\u3073\u3001\u672a\u6765\u3078\u306e\u5e0c\u671b\u304c\u81a8\u3089\u307f\u307e\u3059\u3002\n\n\n## \u51fa\u529b\u4f8b\n{{\n    \"label\": \"\u5e02\u6c11\u306e\u672a\u6765\u3092\u652f\u3048\u308b\u5177\u4f53\u7684\u653f\u7b56\u3078\u306e\u671f\u5f85\",\n    \"description\": \"\u3053\u306e\u30af\u30e9\u30b9\u30bf\u306b\u306f\u3001\u5730\u57df\u306e\u5fa9\u8208\u3084\u88ab\u5bb3\u8005\u652f\u63f4\u306a\u3069\u3001\u5b9f\u969b\u306e\u793e\u4f1a\u8ab2\u984c\u306b\u5bfe\u3057\u3066\u653f\u6cbb\u5bb6\u304c\u5177\u4f53\u7684\u304b\u3064\u7a4d\u6975\u7684\u306b\u53d6\u308a\u7d44\u3080\u59ff\u52e2\u3092\u652f\u6301\u3059\u308b\u524d\u5411\u304d\u306a\u610f\u898b\u304c\u96c6\u307e\u3063\u3066\u3044\u307e\u3059\u3002\u5e02\u6c11\u306f\u3001\u9078\u6319\u3084\u653f\u7b56\u8b70\u8ad6\u3092\u901a\u3058\u3066\u3001\u73fe\u5b9f\u306e\u554f\u984c\u306b\u5373\u3057\u305f\u652f\u63f4\u7b56\u3084\u5fa9\u8208\u8a08\u753b\u304c\u5b9f\u73fe\u3055\u308c\u308b\u3053\u3068\u3092\u671f\u5f85\u3057\u3001\u660e\u308b\u3044\u672a\u6765\u306e\u69cb\u7bc9\u306b\u5411\u3051\u305f\u653f\u6cbb\u306e\u5909\u9769\u3092\u5fdc\u63f4\u3057\u3066\u3044\u307e\u3059\u3002\"\n}}",
    "model": "gpt-4o"
  },
  "hierarchical_merge_labelling": {
    "sampling_num": 3,
    "workers": 1,
    "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\n\n\n@dataclass\nclass ClusterColumns:\n    \"\"\"\u540c\u4e00\u968e\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u95a2\u9023\u306e\u30ab\u30e9\u30e0\u540d\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9\"\"\"\n\n    id: str\n    label: str\n    description: str\n\n    @classmethod\n    def from_id_column(cls, id_column: str) -> \"ClusterColumns\":\n        \"\"\"ID\u5217\u540d\u304b\u3089\u95a2\u9023\u3059\u308b\u30ab\u30e9\u30e0\u540d\u3092\u751f\u6210\"\"\"\n        return cls(\n            id=id_column,\n            label=id_column.replace(\"-id\", \"-label\"),\n            description=id_column.replace(\"-id\", \"-description\"),\n        )\n\n\n@dataclass\nclass ClusterValues:\n    \"\"\"\u5bfe\u8c61\u30af\u30e9\u30b9\u30bf\u306elabel/description\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9\"\"\"\n\n    label: str\n    description: str\n\n    def to_prompt_text(self) -> str:\n        return f\"- {self.label}: {self.description}\"\n\n\ndef hierarchical_merge_labelling(config: dict) -> None:\n    \"\"\"\u968e\u5c64\u7684\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306e\u7d50\u679c\u306b\u5bfe\u3057\u3066\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n            - output_dir: \u51fa\u529b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\n            - hierarchical_merge_labelling: \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u306e\u8a2d\u5b9a\n                - sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6570\n                - prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n                - model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n                - workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    merge_path = f\"outputs/{dataset}/hierarchical_merge_labels.csv\"\n    clusters_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_initial_labels.csv\")\n\n    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)\n    # \u30dc\u30c8\u30e0\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u30fb\u8aac\u660e\u3068\u30af\u30e9\u30b9\u30bfid\u4ed8\u304d\u306e\u5404argument\u3092\u5165\u529b\u3057\u3001\u5404\u968e\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u30e9\u30d9\u30eb\u30fb\u8aac\u660e\u3092\u751f\u6210\u3057\u3001argument\u306b\u4ed8\u3051\u305fdf\u3092\u4f5c\u6210\n    merge_result_df = merge_labelling(\n        clusters_df=clusters_df,\n        cluster_id_columns=sorted(cluster_id_columns, reverse=True),\n        config=config,\n    )\n    # \u4e0a\u8a18\u306edf\u304b\u3089\u5404\u30af\u30e9\u30b9\u30bf\u306elevel, id, label, description, value\u3092\u53d6\u5f97\u3057\u3066df\u3092\u4f5c\u6210\n    melted_df = melt_cluster_data(merge_result_df)\n    # \u4e0a\u8a18\u306edf\u306b\u89aa\u5b50\u95a2\u4fc2\u3092\u8ffd\u52a0\n    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)\n    melted_df = melted_df.merge(parent_child_df, on=[\"level\", \"id\"], how=\"left\")\n    density_df = calculate_cluster_density(melted_df, config)\n    density_df.to_csv(merge_path, index=False)\n\n\ndef _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u9593\u306e\u89aa\u5b50\u95a2\u4fc2\u3092\u30de\u30c3\u30d4\u30f3\u30b0\u3059\u308b\n\n    Args:\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        cluster_id_columns: \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n\n    Returns:\n        \u89aa\u5b50\u95a2\u4fc2\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u60c5\u5831\u3092\u542b\u3080DataFrame\n    \"\"\"\n    results = []\n    top_cluster_column = cluster_id_columns[0]\n    top_cluster_values = df[top_cluster_column].unique()\n    for c in top_cluster_values:\n        results.append(\n            {\n                \"level\": 1,\n                \"id\": c,\n                \"parent\": \"0\",  # aggregation\u3067\u8ffd\u52a0\u3059\u308b\u5168\u4f53\u30af\u30e9\u30b9\u30bf\u306eid\n            }\n        )\n\n    for idx in range(len(cluster_id_columns) - 1):\n        current_column = cluster_id_columns[idx]\n        children_column = cluster_id_columns[idx + 1]\n        current_level = current_column.replace(\"-id\", \"\").replace(\"cluster-level-\", \"\")\n        # \u73fe\u5728\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bfid\n        current_cluster_values = df[current_column].unique()\n        for current_id in current_cluster_values:\n            children_ids = df.loc[df[current_column] == current_id, children_column].unique()\n            for child_id in children_ids:\n                results.append(\n                    {\n                        \"level\": int(current_level) + 1,\n                        \"id\": child_id,\n                        \"parent\": current_id,\n                    }\n                )\n    return pd.DataFrame(results)\n\n\ndef _filter_id_columns(columns: list[str]) -> list[str]:\n    \"\"\"\u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\n\n    Args:\n        columns: \u5168\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n\n    Returns:\n        \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    return [col for col in columns if col.startswith(\"cluster-level-\") and col.endswith(\"-id\")]\n\n\ndef melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u30c7\u30fc\u30bf\u3092\u884c\u5f62\u5f0f\u306b\u5909\u63db\u3059\u308b\n\n    cluster-level-n-(id|label|description) \u3092\u884c\u5f62\u5f0f (level, id, label, description, value) \u306b\u307e\u3068\u3081\u308b\u3002\n    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] \u3092 [level, id, label, description, value(\u4ef6\u6570)] \u306b\u5909\u63db\u3059\u308b\u3002\n\n    Args:\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n\n    Returns:\n        \u884c\u5f62\u5f0f\u306b\u5909\u63db\u3055\u308c\u305fDataFrame\n    \"\"\"\n    id_columns: list[str] = _filter_id_columns(df.columns)\n    levels: set[int] = {int(col.replace(\"cluster-level-\", \"\").replace(\"-id\", \"\")) for col in id_columns}\n    all_rows: list[dict] = []\n\n    # level\u3054\u3068\u306b\u5404\u30af\u30e9\u30b9\u30bf\u306e\u51fa\u73fe\u4ef6\u6570\u3092\u96c6\u8a08\u30fb\u7e26\u6301\u3061\u306b\u3059\u308b\n    for level in levels:\n        cluster_columns = ClusterColumns.from_id_column(f\"cluster-level-{level}-id\")\n        # \u30af\u30e9\u30b9\u30bfid\u3054\u3068\u306e\u4ef6\u6570\u96c6\u8a08\n        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name=\"value\")\n\n        level_unique_val_df = df[\n            [cluster_columns.id, cluster_columns.label, cluster_columns.description]\n        ].drop_duplicates()\n        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how=\"left\")\n        level_unique_vals = [\n            {\n                \"level\": level,\n                \"id\": row[cluster_columns.id],\n                \"label\": row[cluster_columns.label],\n                \"description\": row[cluster_columns.description],\n                \"value\": row[\"value\"],\n            }\n            for _, row in level_unique_val_df.iterrows()\n        ]\n        all_rows.extend(level_unique_vals)\n    return pd.DataFrame(all_rows)\n\n\ndef merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:\n    \"\"\"\u968e\u5c64\u7684\u306a\u30af\u30e9\u30b9\u30bf\u306e\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        clusters_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        cluster_id_columns: \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n\n    Returns:\n        \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080DataFrame\n    \"\"\"\n    for idx in tqdm(range(len(cluster_id_columns) - 1)):\n        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])\n        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])\n\n        process_fn = partial(\n            process_merge_labelling,\n            result_df=clusters_df,\n            current_columns=current_columns,\n            previous_columns=previous_columns,\n            config=config,\n        )\n\n        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())\n        with ThreadPoolExecutor(max_workers=config[\"hierarchical_merge_labelling\"][\"workers\"]) as executor:\n            responses = list(\n                tqdm(\n                    executor.map(process_fn, current_cluster_ids),\n                    total=len(current_cluster_ids),\n                )\n            )\n\n        current_result_df = pd.DataFrame(responses)\n        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])\n    return clusters_df\n\n\ndef process_merge_labelling(\n    target_cluster_id: str,\n    result_df: pd.DataFrame,\n    current_columns: ClusterColumns,\n    previous_columns: ClusterColumns,\n    config,\n):\n    \"\"\"\u500b\u5225\u306e\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        target_cluster_id: \u51e6\u7406\u5bfe\u8c61\u306e\u30af\u30e9\u30b9\u30bfID\n        result_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        current_columns: \u73fe\u5728\u306e\u30ec\u30d9\u30eb\u306e\u30ab\u30e9\u30e0\u60c5\u5831\n        previous_columns: \u524d\u306e\u30ec\u30d9\u30eb\u306e\u30ab\u30e9\u30e0\u60c5\u5831\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n\n    Returns:\n        \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080\u8f9e\u66f8\n    \"\"\"\n\n    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:\n        \"\"\"\u524d\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bf\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\"\"\"\n        previous_records = df[df[current_columns.id] == target_cluster_id][\n            [previous_columns.label, previous_columns.description]\n        ].drop_duplicates()\n        previous_values = [\n            ClusterValues(\n                label=row[previous_columns.label],\n                description=row[previous_columns.description],\n            )\n            for _, row in previous_records.iterrows()\n        ]\n        return previous_values\n\n    previous_values = filter_previous_values(result_df, previous_columns)\n    if len(previous_values) == 1:\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: previous_values[0].label,\n            current_columns.description: previous_values[0].description,\n        }\n    elif len(previous_values) == 0:\n        raise ValueError(f\"\u30af\u30e9\u30b9\u30bf {target_cluster_id} \u306b\u306f\u524d\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bf\u304c\u5b58\u5728\u3057\u307e\u305b\u3093\u3002\")\n\n    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]\n    sampling_num = min(\n        config[\"hierarchical_merge_labelling\"][\"sampling_num\"],\n        len(current_cluster_data),\n    )\n    sampled_data = current_cluster_data.sample(sampling_num)\n    sampled_argument_text = \"\\n\".join(sampled_data[\"argument\"].values)\n    cluster_text = \"\\n\".join([value.to_prompt_text() for value in previous_values])\n    messages = [\n        {\"role\": \"system\", \"content\": config[\"hierarchical_merge_labelling\"][\"prompt\"]},\n        {\n            \"role\": \"user\",\n            \"content\": \"\u30af\u30e9\u30b9\u30bf\u30e9\u30d9\u30eb\\n\" + cluster_text + \"\\n\" + \"\u30af\u30e9\u30b9\u30bf\u306e\u610f\u898b\\n\" + sampled_argument_text,\n        },\n    ]\n    try:\n        response = request_to_chat_openai(\n            messages=messages,\n            model=config[\"hierarchical_merge_labelling\"][\"model\"],\n            is_json=True,\n        )\n        response_json = json.loads(response)\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: response_json.get(\"label\", \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n            current_columns.description: response_json.get(\"description\", \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n        }\n    except Exception as e:\n        print(f\"\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3057\u305f: {e}\")\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n            current_columns.description: \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n        }\n\n\ndef calculate_cluster_density(melted_df: pd.DataFrame, config: dict):\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u5185\u306e\u5bc6\u5ea6\u8a08\u7b97\"\"\"\n    hierarchical_cluster_df = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n\n    densities = []\n    for level, c_id in zip(melted_df[\"level\"], melted_df[\"id\"], strict=False):\n        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f\"cluster-level-{level}-id\"] == c_id][\n            [\"x\", \"y\"]\n        ].values\n        density = calculate_density(cluster_embeds)\n        densities.append(density)\n\n    # \u5bc6\u5ea6\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\n    melted_df[\"density\"] = densities\n    melted_df[\"density_rank\"] = melted_df.groupby(\"level\")[\"density\"].rank(ascending=False, method=\"first\")\n    melted_df[\"density_rank_percentile\"] = melted_df.groupby(\"level\")[\"density_rank\"].transform(lambda x: x / len(x))\n    return melted_df\n\n\ndef calculate_density(embeds: np.ndarray):\n    \"\"\"\u5e73\u5747\u8ddd\u96e2\u306b\u57fa\u3065\u3044\u3066\u5bc6\u5ea6\u3092\u8a08\u7b97\"\"\"\n    center = np.mean(embeds, axis=0)\n    distances = np.linalg.norm(embeds - center, axis=1)\n    avg_distance = np.mean(distances)\n    density = 1 / (avg_distance + 1e-10)\n    return density",
    "prompt": "\u5206\u5272\u3055\u308c\u3059\u304e\u305f\u30af\u30e9\u30b9\u30bf\u3092\u7d71\u5408\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u3067\u3001\u7d71\u5408\u5f8c\u306e\u540d\u79f0\u3092\u8003\u3048\u3066\u51fa\u529b\u3057\u3066\u3002\n\n# \u6307\u793a\n* \u7d71\u5408\u524d\u306e\u30af\u30e9\u30b9\u30bf\u306e\u540d\u79f0\u30fb\u8aac\u660e\u304a\u3088\u3073\u7d71\u5408\u5f8c\u306e\u30af\u30e9\u30b9\u30bf\u306b\u5c5e\u3059\u308b\u30c7\u30fc\u30bf\u70b9\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u4e0e\u3048\u308b\u306e\u3067\u3001\u3053\u308c\u3089\u306b\u57fa\u3065\u3044\u3066\u7d71\u5408\u5f8c\u306e\u30af\u30e9\u30b9\u30bf\u306e\u540d\u79f0\u3092\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n    * \u7d71\u5408\u5f8c\u306e\u30af\u30e9\u30b9\u30bf\u540d\u306b\u304a\u3044\u3066\u3001\u7d71\u5408\u524d\u306e\u30af\u30e9\u30b9\u30bf\u540d\u3092\u305d\u306e\u307e\u307e\u4f7f\u3046\u3053\u3068\u306f\u907f\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n* \u51fa\u529b\u4f8b\u306b\u8a18\u8f09\u3057\u305fJSON\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u5f93\u3063\u3066\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n\n# \u30b5\u30f3\u30d7\u30eb\u306e\u5165\u51fa\u529b\n## \u5165\u529b\u4f8b\uff08\u30af\u30e9\u30b9\u30bf\u30e9\u30d9\u30eb:\u8aac\u660e\u6587\uff09\n- \u5730\u57df\u306e\u707d\u5bb3\u5bfe\u5fdc\u3078\u306e\u6279\u5224: \u3053\u306e\u30af\u30e9\u30b9\u30bf\u306f\u3001\u5730\u57df\u306b\u304a\u3051\u308b\u707d\u5bb3\u5bfe\u5fdc\u7b56\u306e\u5b9f\u65bd\u3084\u4f53\u5236\u306b\u5bfe\u3059\u308b\u6279\u5224\u7684\u306a\u610f\u898b\u3092\u96c6\u7d04\u3057\u305f\u3082\u306e\u3067\u3059\u3002\u4f4f\u6c11\u304b\u3089\u306f\u3001\u8fc5\u901f\u304b\u3064\u52b9\u679c\u7684\u306a\u652f\u63f4\u304c\u884c\u308f\u308c\u3066\u3044\u306a\u3044\u70b9\u3084\u3001\u60c5\u5831\u63d0\u4f9b\u30fb\u9023\u643a\u306e\u4e0d\u8db3\u306a\u3069\u306b\u5bfe\u3059\u308b\u5f37\u3044\u4e0d\u6e80\u304c\u8868\u660e\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n- \u707d\u5bb3\u5bfe\u5fdc\u3078\u306e\u4e0d\u6e80: \u3053\u306e\u30af\u30e9\u30b9\u30bf\u306f\u3001\u707d\u5bb3\u767a\u751f\u6642\u306e\u5bfe\u5fdc\u5168\u822c\u306b\u5bfe\u3059\u308b\u4e0d\u6e80\u3092\u793a\u3059\u610f\u898b\u3092\u307e\u3068\u3081\u305f\u3082\u306e\u3067\u3059\u3002\u6551\u63f4\u6d3b\u52d5\u306e\u9045\u308c\u3084\u652f\u63f4\u7b56\u306e\u5b9f\u52b9\u6027\u306b\u7591\u554f\u3092\u6301\u3064\u58f0\u304c\u591a\u304f\u3001\u3088\u308a\u7a4d\u6975\u7684\u3067\u900f\u660e\u6027\u306e\u3042\u308b\u5bfe\u5fdc\u3092\u6c42\u3081\u308b\u610f\u898b\u304c\u7279\u5fb4\u3067\u3059\u3002\n- \u5730\u57df\u5fa9\u8208\u306e\u9045\u308c: \u3053\u306e\u30af\u30e9\u30b9\u30bf\u306f\u3001\u707d\u5bb3\u5f8c\u306e\u5730\u57df\u5fa9\u8208\u30d7\u30ed\u30bb\u30b9\u304c\u4e88\u5b9a\u901a\u308a\u306b\u9032\u3093\u3067\u3044\u306a\u3044\u70b9\u306b\u5bfe\u3059\u308b\u61f8\u5ff5\u3084\u4e0d\u6e80\u3092\u53cd\u6620\u3057\u3066\u3044\u307e\u3059\u3002\u518d\u5efa\u8a08\u753b\u3084\u652f\u63f4\u7b56\u306e\u5b9f\u65bd\u306e\u9045\u5ef6\u3001\u305d\u3057\u3066\u305d\u308c\u306b\u4f34\u3046\u4f4f\u6c11\u306e\u751f\u6d3b\u518d\u5efa\u3078\u306e\u5f71\u97ff\u304c\u5f37\u8abf\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n\n## \u51fa\u529b\u4f8b\n{{\n    \"label\": \"\u5730\u57df\u518d\u751f\u3068\u707d\u5bb3\u652f\u63f4\u306b\u5bfe\u3059\u308b\u671f\u5f85\u3068\u61f8\u5ff5\",\n    \"description\": \"\u3053\u306e\u30af\u30e9\u30b9\u30bf\u306f\u3001\u7279\u5b9a\u306e\u5730\u57df\u306b\u304a\u3051\u308b\u518d\u751f\u3084\u707d\u5bb3\u652f\u63f4\u7b56\u306b\u5bfe\u3057\u3001\u5177\u4f53\u7684\u306a\u53d6\u308a\u7d44\u307f\u304c\u4e0d\u8db3\u3057\u3066\u3044\u308b\u3068\u306e\u610f\u898b\u3092\u96c6\u7d04\u3057\u3066\u3044\u307e\u3059\u3002\u5e02\u6c11\u306f\u3001\u9078\u6319\u3092\u901a\u3058\u305f\u653f\u7b56\u8b70\u8ad6\u306e\u4e2d\u3067\u3001\u5730\u57df\u5fa9\u8208\u3084\u88ab\u707d\u8005\u652f\u63f4\u3092\u6700\u512a\u5148\u3059\u3079\u304d\u3060\u3068\u306e\u671f\u5f85\u3068\u3001\u73fe\u884c\u306e\u652f\u63f4\u7b56\u306b\u5bfe\u3059\u308b\u6539\u5584\u8981\u6c42\u3092\u5f37\u304f\u8868\u660e\u3057\u3066\u304a\u308a\u3001\u3088\u308a\u52b9\u679c\u7684\u306a\u653f\u5e9c\u306e\u5bfe\u5fdc\u3092\u6c42\u3081\u308b\u58f0\u304c\u53cd\u6620\u3055\u308c\u3066\u3044\u307e\u3059\u3002\"\n}}",
    "model": "gpt-4o"
  },
  "hierarchical_overview": {
    "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_openai\n\n\ndef hierarchical_overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_overview.txt\"\n\n    hierarchical_label_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_merge_labels.csv\")\n\n    prompt = config[\"hierarchical_overview\"][\"prompt\"]\n    model = config[\"hierarchical_overview\"][\"model\"]\n\n    # TODO: level1\u3067\u56fa\u5b9a\u306b\u3057\u3066\u3044\u308b\u304c\u3001\u8a2d\u5b9a\u3067\u5909\u3048\u3089\u308c\u308b\u3088\u3046\u306b\u3059\u308b\n    target_level = 1\n    target_records = hierarchical_label_df[hierarchical_label_df[\"level\"] == target_level]\n    ids = target_records[\"id\"].to_list()\n    labels = target_records[\"label\"].to_list()\n    descriptions = target_records[\"description\"].to_list()\n    target_records.set_index(\"id\", inplace=True)\n\n    input = \"\"\n    for i, _ in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels[i]}\\n\\n\"\n        input += descriptions[i] + \"\\n\\n\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n\n    with open(path, \"w\") as file:\n        file.write(response)",
    "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30b7\u30f3\u30af\u30bf\u30f3\u30af\u3067\u50cd\u304f\u5c02\u9580\u306e\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\n\u30c1\u30fc\u30e0\u306f\u7279\u5b9a\u306e\u30c6\u30fc\u30de\u306b\u95a2\u3057\u3066\u30d1\u30d6\u30ea\u30c3\u30af\u30fb\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u5b9f\u65bd\u3057\u3001\u7570\u306a\u308b\u9078\u629e\u80a2\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u5206\u6790\u3057\u59cb\u3081\u3066\u3044\u307e\u3059\u3002\n\u3053\u308c\u304b\u3089\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u30ea\u30b9\u30c8\u3068\u305d\u306e\u7c21\u5358\u306a\u5206\u6790\u304c\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002\n\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u306f\u3001\u8abf\u67fb\u7d50\u679c\u306e\u7c21\u6f54\u306a\u8981\u7d04\u3092\u8fd4\u3059\u3053\u3068\u3067\u3059\u3002\u8981\u7d04\u306f\u975e\u5e38\u306b\u7c21\u6f54\u306b\uff08\u6700\u5927\u30671\u6bb5\u843d\u3001\u6700\u59274\u6587\uff09\u307e\u3068\u3081\u3001\u7121\u610f\u5473\u306a\u8a00\u8449\u3092\u907f\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306f\u65e5\u672c\u8a9e\u3067\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002",
    "model": "gpt-4o"
  },
  "hierarchical_aggregation": {
    "sampling_num": 5000,
    "hidden_properties": {},
    "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import TypedDict\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\n\n\nclass Argument(TypedDict):\n    arg_id: str\n    argument: str\n    comment_id: str\n    x: float\n    y: float\n    p: float\n    cluster_ids: list[str]\n\n\nclass Cluster(TypedDict):\n    level: int\n    id: str\n    label: str\n    takeaway: str\n    value: int\n    parent: str\n    density_rank_percentile: float | None\n\n\ndef hierarchical_aggregation(config):\n    path = f\"outputs/{config['output_dir']}/hierarchical_result.json\"\n    results = {\n        \"arguments\": [],\n        \"clusters\": [],\n        \"comments\": {},\n        \"propertyMap\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index(\"arg-id\", inplace=True)\n    arg_num = len(arguments)\n    relation_df = pd.read_csv(f\"outputs/{config['output_dir']}/relations.csv\")\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_merge_labels.csv\")\n\n    hidden_properties_map: dict[str, list[str]] = config[\"hierarchical_aggregation\"][\"hidden_properties\"]\n\n    results[\"arguments\"] = _build_arguments(clusters)\n    results[\"clusters\"] = _build_cluster_value(labels, arg_num)\n    # NOTE: \u5c5e\u6027\u306b\u5fdc\u3058\u305f\u30b3\u30e1\u30f3\u30c8\u30d5\u30a3\u30eb\u30bf\u6a5f\u80fd\u304c\u5b9f\u88c5\u3055\u308c\u3066\u304a\u3089\u305a\u3001\u5168\u3066\u306e\u30b3\u30e1\u30f3\u30c8\u304c\u542b\u307e\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\n    # results[\"comments\"] = _build_comments_value(\n    #     comments, arguments, hidden_properties_map\n    # )\n    results[\"comment_num\"] = len(comments)\n    results[\"translations\"] = _build_translations(config)\n    # \u5c5e\u6027\u60c5\u5831\u306e\u30ab\u30e9\u30e0\u306f\u3001\u5143\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u6307\u5b9a\u3057\u305f\u30ab\u30e9\u30e0\u3068classification\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u3092\u5408\u308f\u305b\u305f\u3082\u306e\n    results[\"propertyMap\"] = _build_property_map(arguments, hidden_properties_map, config)\n\n    with open(f\"outputs/{config['output_dir']}/hierarchical_overview.txt\") as f:\n        overview = f.read()\n    print(\"overview\")\n    print(overview)\n    results[\"overview\"] = overview\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2, ensure_ascii=False)\n    # TODO: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ed\u30b8\u30c3\u30af\u3092\u5b9f\u88c5\u3057\u305f\u3044\u304c\u3001\u73fe\u72b6\u306f\u5168\u4ef6\u62bd\u51fa\n    create_custom_intro(config)\n    if config[\"is_pubcom\"]:\n        add_original_comments(labels, arguments, relation_df, clusters, config)\n\n\ndef create_custom_intro(config):\n    dataset = config[\"output_dir\"]\n    args_path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    result_path = f\"outputs/{dataset}/hierarchical_result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n    processed_num = min(input_count, config[\"extraction\"][\"limit\"])\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n\u5206\u6790\u5bfe\u8c61\u3068\u306a\u3063\u305f\u30c7\u30fc\u30bf\u306e\u4ef6\u6570\u306f{processed_num}\u4ef6\u3067\u3001\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066OpenAI API\u3092\u7528\u3044\u3066{args_count}\u4ef6\u306e\u610f\u898b\uff08\u8b70\u8ad6\uff09\u3092\u62bd\u51fa\u3057\u3001\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u305f\u3002\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(intro=intro, processed_num=processed_num, args_count=args_count)\n\n    with open(result_path) as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\") as f:\n        json.dump(result, f, indent=2, ensure_ascii=False)\n\n\ndef add_original_comments(labels, arguments, relation_df, clusters, config):\n    # \u5927\u30ab\u30c6\u30b4\u30ea\uff08cluster-level-1\uff09\u306b\u8a72\u5f53\u3059\u308b\u30e9\u30d9\u30eb\u3060\u3051\u62bd\u51fa\n    labels_lv1 = labels[labels[\"level\"] == 1][[\"id\", \"label\"]].rename(\n        columns={\"id\": \"cluster-level-1-id\", \"label\": \"category_label\"}\n    )\n\n    # arguments \u3068 clusters \u3092\u30de\u30fc\u30b8\uff08\u30ab\u30c6\u30b4\u30ea\u60c5\u5831\u4ed8\u4e0e\uff09\n    merged = arguments.merge(clusters[[\"arg-id\", \"cluster-level-1-id\"]], on=\"arg-id\").merge(\n        labels_lv1, on=\"cluster-level-1-id\", how=\"left\"\n    )\n\n    # relation_df \u3068\u7d50\u5408\n    merged = merged.merge(relation_df, on=\"arg-id\", how=\"left\")\n\n    # \u5143\u30b3\u30e1\u30f3\u30c8\u53d6\u5f97\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    comments[\"comment-id\"] = comments[\"comment-id\"].astype(str)\n    merged[\"comment-id\"] = merged[\"comment-id\"].astype(str)\n\n    # \u5143\u30b3\u30e1\u30f3\u30c8\u672c\u6587\u306a\u3069\u3068\u30de\u30fc\u30b8\n    final_df = merged.merge(comments, on=\"comment-id\", how=\"left\")\n\n    # \u5fc5\u8981\u30ab\u30e9\u30e0\u306e\u307f\u6574\u5f62\n    final_cols = [\"comment-id\", \"comment-body\", \"arg-id\", \"argument\", \"cluster-level-1-id\", \"category_label\"]\n    for col in [\"source\", \"url\"]:\n        if col in comments.columns:\n            final_cols.append(col)\n\n    final_df = final_df[final_cols]\n    final_df = final_df.rename(\n        columns={\n            \"cluster-level-1-id\": \"category_id\",\n            \"category_label\": \"category\",\n            \"arg-id\": \"arg_id\",\n            \"argument\": \"argument\",\n            \"comment-body\": \"original-comment\",\n        }\n    )\n\n    # \u4fdd\u5b58\n    final_df.to_csv(f\"outputs/{config['output_dir']}/final_result_with_comments.csv\", index=False)\n\n\ndef _build_arguments(clusters: pd.DataFrame) -> list[Argument]:\n    cluster_columns = [col for col in clusters.columns if col.startswith(\"cluster-level-\") and \"id\" in col]\n\n    arguments: list[Argument] = []\n    for _, row in clusters.iterrows():\n        cluster_ids = [\"0\"]\n        for cluster_column in cluster_columns:\n            cluster_ids.append(row[cluster_column])\n        argument: Argument = {\n            \"arg_id\": row[\"arg-id\"],\n            \"argument\": row[\"argument\"],\n            \"x\": row[\"x\"],\n            \"y\": row[\"y\"],\n            \"p\": 0,  # NOTE: \u4e00\u65e6\u5168\u90e80\u3067\u3044\u308c\u308b\n            \"cluster_ids\": cluster_ids,\n        }\n        arguments.append(argument)\n    return arguments\n\n\ndef _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:\n    results: list[Cluster] = [\n        Cluster(\n            level=0,\n            id=\"0\",\n            label=\"\u5168\u4f53\",\n            takeaway=\"\",\n            value=total_num,\n            parent=\"\",\n            density_rank_percentile=0,\n        )\n    ]\n\n    for _, melted_label in melted_labels.iterrows():\n        cluster_value = Cluster(\n            level=melted_label[\"level\"],\n            id=melted_label[\"id\"],\n            label=melted_label[\"label\"],\n            takeaway=melted_label[\"description\"],\n            value=melted_label[\"value\"],\n            parent=melted_label.get(\"parent\", \"\u5168\u4f53\"),\n            density_rank_percentile=melted_label.get(\"density_rank_percentile\"),\n        )\n        results.append(cluster_value)\n    return results\n\n\n# def _build_comments_value(\n#     comments: pd.DataFrame,\n#     arguments: pd.DataFrame,\n#     hidden_properties_map: dict[str, list[str]],\n# ):\n#     comment_dict: dict[str, dict[str, str]] = {}\n#     useful_comment_ids = set(arguments[\"comment-id\"].values)\n#     for _, row in comments.iterrows():\n#         id = row[\"comment-id\"]\n#         if id in useful_comment_ids:\n#             res = {\"comment\": row[\"comment-body\"]}\n#             should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())\n#             if should_skip:\n#                 continue\n#             comment_dict[str(id)] = res\n\n#     return comment_dict\n\n\ndef _build_translations(config):\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        return json.loads(translations)\n    return {}\n\n\ndef _build_property_map(\n    arguments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict\n) -> dict[str, dict[str, str]]:\n    property_columns = list(hidden_properties_map.keys()) + list(config[\"extraction\"][\"categories\"].keys())\n    property_map = defaultdict(dict)\n\n    # \u6307\u5b9a\u3055\u308c\u305f property_columns \u304c arguments \u306b\u5b58\u5728\u3059\u308b\u304b\u30c1\u30a7\u30c3\u30af\n    missing_cols = [col for col in property_columns if col not in arguments.columns]\n    if missing_cols:\n        raise ValueError(\n            f\"\u6307\u5b9a\u3055\u308c\u305f\u30ab\u30e9\u30e0 {missing_cols} \u304c args.csv \u306b\u5b58\u5728\u3057\u307e\u305b\u3093\u3002\"\n            \"\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30ebaggregation / hidden_properties\u304b\u3089\u8a72\u5f53\u30ab\u30e9\u30e0\u3092\u53d6\u308a\u9664\u3044\u3066\u304f\u3060\u3055\u3044\u3002\"\n        )\n\n    for prop in property_columns:\n        for arg_id, row in arguments.iterrows():\n            # LLM\u306b\u3088\u308bcategory classification\u304c\u3046\u307e\u304f\u884c\u304b\u305a\u3001NaN\u306e\u5834\u5408\u306fNone\u306b\u3059\u308b\n            property_map[prop][arg_id] = row[prop] if not pd.isna(row[prop]) else None\n    return property_map"
  },
  "plan": [
    {
      "step": "extraction",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "embedding",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "hierarchical_clustering",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "hierarchical_initial_labelling",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "hierarchical_merge_labelling",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "hierarchical_overview",
      "run": true,
      "reason": "not trace of previous run"
    },
    {
      "step": "hierarchical_aggregation",
      "run": true,
      "reason": "not trace of previous run"
    }
  ],
  "status": "error",
  "start_time": "2025-04-13T16:01:40.138803",
  "completed_jobs": [],
  "lock_until": "2025-04-13T16:06:40.732316",
  "current_job": "extraction",
  "current_job_started": "2025-04-13T16:01:40.141840",
  "current_job_progress": 200,
  "current_jop_tasks": 200,
  "end_time": "2025-04-13T16:01:40.730641",
  "error": "RuntimeError: result is empty, maybe bad prompt",
  "error_stack_trace": "Traceback (most recent call last):\n  File \"C:\\Users\\U76724\\Documents\\Python\\uv\\broadlistening\\hierarchical_main.py\", line 61, in main\n    run_step(\"extraction\", extraction, config)\n  File \"C:\\Users\\U76724\\Documents\\Python\\uv\\broadlistening\\hierarchical_utils.py\", line 223, in run_step\n    func(config)\n  File \"C:\\Users\\U76724\\Documents\\Python\\uv\\broadlistening\\steps\\extraction.py\", line 77, in extraction\n    raise RuntimeError(\"result is empty, maybe bad prompt\")\nRuntimeError: result is empty, maybe bad prompt\n"
}